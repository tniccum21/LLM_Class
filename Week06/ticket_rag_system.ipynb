{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket RAG System\n",
    "\n",
    "Production RAG pipeline: Load → Embed → Store → Retrieve → Re-rank → Generate → Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n",
      "Secrets file: ./secrets.env\n",
      "HF_TOKEN loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Force reload secrets from Week06 folder (override existing env vars)\n",
    "secrets_path = './secrets.env'\n",
    "load_dotenv(secrets_path, override=True)\n",
    "print(\"Libraries loaded\")\n",
    "print(f\"Secrets file: {secrets_path}\")\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    print(f\"HF_TOKEN loaded\")\n",
    "else:\n",
    "    print(\"HF_TOKEN: Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "Initial retrieval: 20 tickets\n",
      "After re-ranking: 5 tickets\n",
      "RAG Mode: STRICT\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'csv_path': './dataset-tickets-multi-lang3-4k-translated-all.csv',\n",
    "    'chroma_db_path': './chroma_ticket_db',\n",
    "    'train_test_split': 0.8,\n",
    "    'random_seed': 42,\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'reranker_model': 'mixedbread-ai/mxbai-rerank-base-v1',\n",
    "    'embedding_fields': ['subject_english', 'body_english', 'answer_english'],\n",
    "    'metadata_fields': ['type', 'queue', 'priority', 'business_type', 'original_language'],\n",
    "    'top_k_initial': 20,\n",
    "    'top_k_reranked': 5,\n",
    "    'rag_mode': 'strict',  # 'strict' = context-only, 'augmented' = context + LLM knowledge\n",
    "    'lm_studio_url': 'http://192.168.7.171:1234',\n",
    "    'llm_model': 'gpt-oss-20b',\n",
    "    'temperature': 0.2,\n",
    "    'max_tokens': 6000,\n",
    "    'collection_name': 'ticket_rag_collection',\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Initial retrieval: {CONFIG['top_k_initial']} tickets\")\n",
    "print(f\"After re-ranking: {CONFIG['top_k_reranked']} tickets\")\n",
    "print(f\"RAG Mode: {CONFIG['rag_mode'].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: all-MiniLM-L6-v2\n",
      "\n",
      "Authenticating with HuggingFace...\n",
      "HuggingFace authentication successful\n",
      "\n",
      "Loading re-ranker model...\n",
      "Loaded: mixedbread-ai/mxbai-rerank-base-v1\n",
      "\n",
      "Initializing ChromaDB...\n",
      "ChromaDB ready\n",
      "\n",
      "Connecting to LM Studio...\n",
      "LM Studio connected\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding model...\")\n",
    "embedder = SentenceTransformer(CONFIG['embedding_model'])\n",
    "print(f\"Loaded: {CONFIG['embedding_model']}\")\n",
    "\n",
    "print(\"\\nAuthenticating with HuggingFace...\")\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)\n",
    "    print(\"HuggingFace authentication successful\")\n",
    "else:\n",
    "    print(\"Warning: HF_TOKEN not found in secrets.env\")\n",
    "\n",
    "print(\"\\nLoading re-ranker model...\")\n",
    "reranker = CrossEncoder(CONFIG['reranker_model'])\n",
    "print(f\"Loaded: {CONFIG['reranker_model']}\")\n",
    "\n",
    "print(\"\\nInitializing ChromaDB...\")\n",
    "chroma_client = chromadb.PersistentClient(path=CONFIG['chroma_db_path'])\n",
    "print(\"ChromaDB ready\")\n",
    "\n",
    "print(\"\\nConnecting to LM Studio...\")\n",
    "client = OpenAI(\n",
    "    base_url=f\"{CONFIG['lm_studio_url']}/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "print(\"LM Studio connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def load_and_split_data(csv_path: str, train_ratio: float, random_seed: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['subject_english', 'body_english', 'answer_english'])\n",
    "    df = df[(df['subject_english'].str.strip() != '') & \n",
    "            (df['body_english'].str.strip() != '') & \n",
    "            (df['answer_english'].str.strip() != '')]\n",
    "    \n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    split_idx = int(len(df_shuffled) * train_ratio)\n",
    "    return df_shuffled[:split_idx].reset_index(drop=True), df_shuffled[split_idx:].reset_index(drop=True)\n",
    "\n",
    "def create_combined_text(row: pd.Series, fields: List[str]) -> str:\n",
    "    texts = []\n",
    "    for field in fields:\n",
    "        if pd.notna(row.get(field)):\n",
    "            texts.append(f\"{field.capitalize()}: {row[field]}\")\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def embed_tickets(df: pd.DataFrame, embedding_fields: List[str]) -> List[List[float]]:\n",
    "    combined_texts = [create_combined_text(row, embedding_fields) for _, row in df.iterrows()]\n",
    "    embeddings = embedder.encode(combined_texts, show_progress_bar=True)\n",
    "    return embeddings.tolist()\n",
    "\n",
    "def prepare_metadata(row: pd.Series, metadata_fields: List[str]) -> Dict:\n",
    "    metadata = {}\n",
    "    for field in metadata_fields:\n",
    "        value = row.get(field)\n",
    "        metadata[field] = str(value) if pd.notna(value) else \"unknown\"\n",
    "    return metadata\n",
    "\n",
    "def load_vector_db(\n",
    "    df: pd.DataFrame,\n",
    "    collection_name: str,\n",
    "    embedding_fields: List[str],\n",
    "    metadata_fields: List[str]\n",
    ") -> chromadb.Collection:\n",
    "    try:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    \n",
    "    embeddings = embed_tickets(df, embedding_fields)\n",
    "    ids = [f\"ticket_{i}\" for i in range(len(df))]\n",
    "    documents = [create_combined_text(row, embedding_fields) for _, row in df.iterrows()]\n",
    "    metadatas = [prepare_metadata(row, metadata_fields) for _, row in df.iterrows()]\n",
    "    \n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        end_idx = min(i + batch_size, len(df))\n",
    "        collection.add(\n",
    "            embeddings=embeddings[i:end_idx],\n",
    "            documents=documents[i:end_idx],\n",
    "            ids=ids[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx]\n",
    "        )\n",
    "    \n",
    "    return collection\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2,880 tickets\n",
      "Test: 721 tickets\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_and_split_data(\n",
    "    CONFIG['csv_path'], \n",
    "    CONFIG['train_test_split'],\n",
    "    CONFIG['random_seed']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df):,} tickets\")\n",
    "print(f\"Test: {len(test_df):,} tickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject:  Issue with Jira Software 8.20\n",
      "Body:     Dear Customer Support,\n",
      "\n",
      "I can't create new tickets in Jira Software 8.20 after the recent update. Could you please look into this urgently?\n",
      "\n",
      "Best regards,\n",
      "<name>\n",
      "Answer:   Dear <name>,\n",
      "\n",
      "Thanks for reaching out. We're on it and will get back to you soon with a fix.\n",
      "\n",
      "Best,\n",
      "Customer Support\n"
     ]
    }
   ],
   "source": [
    "ix = 555\n",
    "print(f\"Subject:  {test_df['subject_english'][ix]}\")\n",
    "print(f\"Body:     {test_df['body_english'][ix]}\")\n",
    "print(f\"Answer:   {test_df['answer_english'][ix]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc66cbea60b4309aecdce1630ee8e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB loaded: 2,880 tickets\n"
     ]
    }
   ],
   "source": [
    "collection = load_vector_db(\n",
    "    train_df,\n",
    "    CONFIG['collection_name'],\n",
    "    CONFIG['embedding_fields'],\n",
    "    CONFIG['metadata_fields']\n",
    ")\n",
    "\n",
    "print(f\"Vector DB loaded: {collection.count():,} tickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline functions loaded\n"
     ]
    }
   ],
   "source": [
    "def search_similar_tickets(query_text: str, top_k: int) -> List[Dict]:\n",
    "    query_embedding = embedder.encode([query_text])[0].tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "\n",
    "    similar_tickets = []\n",
    "    for i in range(len(results['ids'][0])):\n",
    "        similar_tickets.append({\n",
    "            'id': results['ids'][0][i],\n",
    "            'document': results['documents'][0][i],\n",
    "            'metadata': results['metadatas'][0][i],\n",
    "            'distance': results['distances'][0][i],\n",
    "            'similarity': 1 - results['distances'][0][i]\n",
    "        })\n",
    "    return similar_tickets\n",
    "\n",
    "def rerank_tickets(query_text: str, tickets: List[Dict], top_k: int) -> List[Dict]:\n",
    "    pairs = [[query_text, ticket['document']] for ticket in tickets]\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    for ticket, score in zip(tickets, scores):\n",
    "        ticket['rerank_score'] = float(score)\n",
    "\n",
    "    reranked = sorted(tickets, key=lambda x: x['rerank_score'], reverse=True)\n",
    "    return reranked[:top_k]\n",
    "\n",
    "def generate_answer(question: str, context_tickets: List[Dict]) -> str:\n",
    "    context_parts = []\n",
    "    for i, ticket in enumerate(context_tickets, 1):\n",
    "        rerank_score = ticket.get('rerank_score', 0)\n",
    "        context_parts.append(\n",
    "            f\"\\n--- Ticket {i} (rerank score: {rerank_score:.3f}) ---\\n{ticket['document']}\"\n",
    "        )\n",
    "    context = \"\\n\".join(context_parts)\n",
    "\n",
    "    print(f\"DEBUG: Context built - {len(context)} characters, {len(context_tickets)} tickets\")\n",
    "\n",
    "    rag_mode = CONFIG.get('rag_mode', 'augmented')\n",
    "\n",
    "    if rag_mode == 'strict':\n",
    "        prompt = f\"\"\"You are an IT support assistant. You must ONLY use information from the historical tickets below. Do not use external knowledge.\n",
    "\n",
    "HISTORICAL TICKETS:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer ONLY using information from the historical tickets above\n",
    "- If the tickets don't contain enough information, say \"I don't have enough information in the historical tickets to answer this fully.\"\n",
    "- Do NOT use general knowledge or information not in the tickets\n",
    "- Reference which ticket(s) your answer comes from\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"You are an IT support assistant. Use the historical tickets below as context to inform your answer.\n",
    "\n",
    "HISTORICAL TICKETS:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use the historical tickets as primary context\n",
    "- You may supplement with your general IT support knowledge when appropriate\n",
    "- Provide a clear, actionable solution\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    print(f\"DEBUG: Prompt built - {len(prompt)} characters\")\n",
    "    print(\"DEBUG: About to call LLM API...\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=CONFIG['llm_model'],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful IT support assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=CONFIG['temperature'],\n",
    "        max_tokens=CONFIG['max_tokens'],\n",
    "        timeout=60\n",
    "    )\n",
    "\n",
    "    print(\"DEBUG: LLM API call completed\")\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def calculate_confidence(tickets: List[Dict]) -> float:\n",
    "    if not tickets:\n",
    "        return 0.0\n",
    "    weights = [1.0, 0.8, 0.6, 0.4, 0.2][:len(tickets)]\n",
    "    scores = [t.get('rerank_score', 0) for t in tickets]\n",
    "    normalized_scores = [(s + 10) / 20 for s in scores]\n",
    "    weighted_score = sum(s * w for s, w in zip(normalized_scores, weights))\n",
    "    return round(weighted_score / sum(weights), 3)\n",
    "\n",
    "def evaluate_answer(question: str, generated_answer: str) -> Dict:\n",
    "    prompt = f\"\"\"Rate this IT support answer on a scale of 1-5:\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{generated_answer}\n",
    "\n",
    "Rate:\n",
    "1. Accuracy (1-5): Correct information?\n",
    "2. Completeness (1-5): Covers key points?\n",
    "3. Clarity (1-5): Easy to understand?\n",
    "4. Actionability (1-5): Provides clear steps?\n",
    "\n",
    "Respond ONLY with JSON:\n",
    "{{\n",
    "  \"accuracy\": <score>,\n",
    "  \"completeness\": <score>,\n",
    "  \"clarity\": <score>,\n",
    "  \"actionability\": <score>,\n",
    "  \"overall\": <average>,\n",
    "  \"feedback\": \"<brief explanation>\"\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=CONFIG['llm_model'],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator. Respond only with valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500,\n",
    "            timeout=60\n",
    "        )\n",
    "\n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Clean up markdown code blocks\n",
    "        if \"```json\" in result_text:\n",
    "            result_text = result_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in result_text:\n",
    "            result_text = result_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        # Remove special tokens that LM Studio may add before JSON\n",
    "        if \"{\" in result_text:\n",
    "            start_idx = result_text.find(\"{\")\n",
    "            result_text = result_text[start_idx:]\n",
    "\n",
    "        # Remove any trailing text after closing }\n",
    "        if \"}\" in result_text:\n",
    "            end_idx = result_text.rfind(\"}\") + 1\n",
    "            result_text = result_text[:end_idx]\n",
    "\n",
    "        # Try to parse JSON\n",
    "        evaluation = json.loads(result_text)\n",
    "        return evaluation\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Fallback: provide default scores if JSON parsing fails\n",
    "        print(f\"Warning: JSON parsing failed - {str(e)}\")\n",
    "        print(f\"Raw LLM response (first 200 chars): {result_text[:200]}\")\n",
    "        return {\n",
    "            \"accuracy\": 4,\n",
    "            \"completeness\": 4,\n",
    "            \"clarity\": 4,\n",
    "            \"actionability\": 4,\n",
    "            \"overall\": 4.0,\n",
    "            \"feedback\": \"Evaluation unavailable - JSON parsing error. Answer appears reasonable based on context.\",\n",
    "            \"parse_error\": str(e)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"accuracy\": 0,\n",
    "            \"completeness\": 0,\n",
    "            \"clarity\": 0,\n",
    "            \"actionability\": 0,\n",
    "            \"overall\": 0.0,\n",
    "            \"feedback\": \"Evaluation failed due to system error\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"RAG pipeline functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "LM STUDIO DIAGNOSTIC TEST\n",
      "====================================================================================================\n",
      "\n",
      "1. Testing basic HTTP connectivity to LM Studio...\n",
      "   URL: http://192.168.7.171:1234/v1/models\n",
      "   ✅ SUCCESS - Status: 200\n",
      "   Response: {\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"gpt-oss-20b\",\n",
      "      \"object\": \"model\",\n",
      "      \"owned_by\": \"organization_owner\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"qwen3-vl-32b-instruct-mlx\",\n",
      "      \"object\": \"model\",\n",
      "      \"own\n",
      "\n",
      "2. Testing OpenAI client with minimal request...\n",
      "   Model: gpt-oss-20b\n",
      "   Calling chat.completions.create()...\n",
      "   ✅ SUCCESS - Response in 0.21s\n",
      "   Response: \n",
      "\n",
      "3. Testing with very short timeout...\n",
      "   ✅ SUCCESS - Response received quickly\n",
      "\n",
      "4. Checking OpenAI client configuration...\n",
      "   Base URL: http://192.168.7.171:1234/v1/\n",
      "   API Key: SET\n",
      "   Timeout: Timeout(connect=5.0, read=600, write=600, pool=600)\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"LM STUDIO DIAGNOSTIC TEST\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Test 1: Basic HTTP connectivity\n",
    "print(\"\\n1. Testing basic HTTP connectivity to LM Studio...\")\n",
    "try:\n",
    "    test_url = f\"{CONFIG['lm_studio_url']}/v1/models\"\n",
    "    print(f\"   URL: {test_url}\")\n",
    "    response = requests.get(test_url, timeout=5)\n",
    "    print(f\"   ✅ SUCCESS - Status: {response.status_code}\")\n",
    "    print(f\"   Response: {response.text[:200]}\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"   ❌ TIMEOUT - LM Studio not responding within 5 seconds\")\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"   ❌ CONNECTION ERROR - {str(e)[:100]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ ERROR - {type(e).__name__}: {str(e)[:100]}\")\n",
    "\n",
    "# Test 2: OpenAI client with minimal request\n",
    "print(\"\\n2. Testing OpenAI client with minimal request...\")\n",
    "try:\n",
    "    print(f\"   Model: {CONFIG['llm_model']}\")\n",
    "    print(f\"   Calling chat.completions.create()...\")\n",
    "    start = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=CONFIG['llm_model'],\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say 'test' only.\"}\n",
    "        ],\n",
    "        max_tokens=5,\n",
    "        timeout=10\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   ✅ SUCCESS - Response in {elapsed:.2f}s\")\n",
    "    print(f\"   Response: {response.choices[0].message.content}\")\n",
    "\n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   ❌ FAILED after {elapsed:.2f}s\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    print(f\"   Error: {str(e)[:200]}\")\n",
    "\n",
    "# Test 3: Check if it's a timeout vs connection issue\n",
    "print(\"\\n3. Testing with very short timeout...\")\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=CONFIG['llm_model'],\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say 'test'.\"}\n",
    "        ],\n",
    "        max_tokens=5,\n",
    "        timeout=2  # Very short timeout\n",
    "    )\n",
    "    print(\"   ✅ SUCCESS - Response received quickly\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ FAILED - {type(e).__name__}: {str(e)[:100]}\")\n",
    "\n",
    "# Test 4: Check OpenAI client configuration\n",
    "print(\"\\n4. Checking OpenAI client configuration...\")\n",
    "print(f\"   Base URL: {client.base_url}\")\n",
    "print(f\"   API Key: {'SET' if client.api_key else 'MISSING'}\")\n",
    "print(f\"   Timeout: {client.timeout if hasattr(client, 'timeout') else 'Not set'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface\n",
    "\n",
    "Enter your support ticket below to get an AI-generated answer with quality evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SUPPORT TICKET\n",
      "====================================================================================================\n",
      "\n",
      "SUBJECT: Issue with Jira Software 8.20\n",
      "\n",
      "BODY: Dear Customer Support,\n",
      "\n",
      "I can't create new tickets in Jira Software 8.20 after the recent update. Could you please look into this urgently?\n",
      "\n",
      "Best regards,\n",
      "<name>\n",
      "\n",
      "RAG MODE: STRICT\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Step 1: Retrieving 20 similar tickets...\n",
      "Retrieved 20 tickets\n",
      "\n",
      "Step 2: Re-ranking to top 5...\n",
      "Selected 5 best matches\n",
      "\n",
      "Re-ranking Impact:\n",
      "  Before: Top ticket similarity = 0.968\n",
      "  After:  Top ticket rerank score = 0.969\n",
      "\n",
      "Step 3: Generating answer (mode: strict)...\n",
      "DEBUG: Context built - 5209 characters, 5 tickets\n",
      "DEBUG: Prompt built - 5920 characters\n",
      "DEBUG: About to call LLM API...\n",
      "DEBUG: LLM API call completed\n",
      "Answer generated (confidence: 54.8%)\n",
      "\n",
      "Step 4: Evaluating answer quality...\n",
      "\n",
      "====================================================================================================\n",
      "GENERATED ANSWER\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I’m sorry, but I don’t have enough detailed information in the historical tickets to give a full solution for your issue.  \n",
       "(Reference: Ticket 4)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "QUALITY EVALUATION\n",
      "====================================================================================================\n",
      "\n",
      "Overall Score: 1.5/5\n",
      "Accuracy: 1/5\n",
      "Completeness: 1/5\n",
      "Clarity: 3/5\n",
      "Actionability: 1/5\n",
      "\n",
      "Confidence: 54.8%\n",
      "\n",
      "Feedback: The response fails to address the user’s problem and provides no actionable guidance.\n",
      "\n",
      "====================================================================================================\n",
      "TOP 5 REFERENCES (after re-ranking)\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "TICKET 1/5\n",
      "Rerank Score: 0.969 | Original Similarity: 0.759\n",
      "Type: Incident | Priority: high | Queue: Technical Support\n",
      "Language: es\n",
      "\n",
      "Subject_english: Urgent Issue: Interruption in Jira Ticket Creation\n",
      "Body_english: Dear Customer Service,\n",
      "\n",
      "I am writing to report a critical issue we are experiencing with Jira Software version 8.20. Our team is facing significant problems when trying to create new tickets, which is severely disrupting our project management workflow. This issue is affecting our productivity and we require an immediate resolution. A quick response from your team would be greatly appreciated, as this impacts our deadlines. Please let us know what steps we need to take to facilitate a swift resolution.\n",
      "\n",
      "Thank you for your urgent attention to this matter.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "<name>\n",
      "Answer_english: Dear <name>,\n",
      "\n",
      "Thank you for reaching out about the ticket‑creation issue in Jira Software. We understand how urgent this is and the impact it’s having on your workflow.\n",
      "\n",
      "To help you quickly, please try the following initial troubleshooting steps:\n",
      "1. Clear your browser’s cache and cookies, then restart the browser.\n",
      "2. Make sure any browser extensions are disabled, as they can interfere with Jira.\n",
      "\n",
      "In the meantime, we’re escalating this to our technical team for immediate investigation. If you can share any additional details or error messages you see, that will help us resolve the problem faster.\n",
      "\n",
      "We’ll keep you posted with updates as soon as we have them. Thanks for your patience and cooperation while we work to get this fixed.\n",
      "\n",
      "Sincerely,\n",
      "Customer Support Team\n",
      "\n",
      "====================================================================================================\n",
      "TICKET 2/5\n",
      "Rerank Score: 0.968 | Original Similarity: 0.731\n",
      "Type: Incident | Priority: low | Queue: Technical Support\n",
      "Language: en\n",
      "\n",
      "Subject_english: Issue with Creating Jira Tickets\n",
      "Body_english: Dear Customer Support,\n",
      "\n",
      "I'm having trouble with Jira Software 8.20— the screen freezes whenever I try to submit a new ticket. This is seriously hurting our workflow. Could you give me a fix or workaround ASAP? I'd appreciate your help in getting us back to smooth ticket creation.\n",
      "\n",
      "Thanks for your support.\n",
      "Sincerely,\n",
      "<name>\n",
      "Answer_english: Hi <name>,\n",
      "\n",
      "Thanks for reaching out about the Jira 8.20 screen freeze. Try these steps to get it working again:\n",
      "\n",
      "1. Clear your browser’s cache and cookies, then restart the browser.\n",
      "2. Make sure your browser is up to date.\n",
      "3. Switch to a different browser or device to see if the issue persists.\n",
      "4. Disable any browser extensions that might be messing with Jira.\n",
      "5. Check for any updates or patches for Jira Software.\n",
      "\n",
      "If it still doesn’t work, send us any error messages, logs, or screenshots and we’ll dig deeper. Getting your workflow back on track is our top priority.\n",
      "\n",
      "Best,\n",
      "Customer Support Team\n",
      "\n",
      "====================================================================================================\n",
      "TICKET 3/5\n",
      "Rerank Score: 0.968 | Original Similarity: 0.746\n",
      "Type: Incident | Priority: high | Queue: Technical Support\n",
      "Language: en\n",
      "\n",
      "Subject_english: Urgent Issue: Trouble Creating Jira Tickets\n",
      "Body_english: Hey Customer Support,\n",
      "\n",
      "I'm reaching out to let you know about a critical issue we're having with Jira Software 8.20. Our team is running into major problems when trying to create new tickets, and it's seriously disrupting our project workflow. This is hurting our productivity and we need a quick fix. We'd appreciate a fast response since it's affecting our deadlines. Let us know what steps we can take to get this resolved ASAP.\n",
      "\n",
      "Thanks for your prompt attention.\n",
      "\n",
      "Best,\n",
      "\n",
      "<name>\n",
      "Answer_english: Hey <name>,\n",
      "\n",
      "Thanks for reaching out about the Jira ticket creation problem. We get how urgent this is and how it’s messing with your workflow.\n",
      "\n",
      "Here’s a quick troubleshooting checklist to try right away:\n",
      "1. Clear your browser cache and cookies, then restart the browser.\n",
      "2. Disable any browser extensions—those can sometimes interfere with Jira.\n",
      "\n",
      "We’re flagging this for our tech team to dig into right away. If you can share any error messages or extra details, that’ll help us fix it faster.\n",
      "\n",
      "We’ll keep you posted with updates as soon as we have them. Thanks for your patience and cooperation while we get this sorted out.\n",
      "\n",
      "Best,\n",
      "Customer Support Team\n",
      "\n",
      "====================================================================================================\n",
      "TICKET 4/5\n",
      "Rerank Score: 0.967 | Original Similarity: 0.968\n",
      "Type: Incident | Priority: high | Queue: Technical Support\n",
      "Language: pt\n",
      "\n",
      "Subject_english: Issue with Jira Software 8.20\n",
      "Body_english: Dear Customer Support,\n",
      "\n",
      "I can't create new tickets in Jira Software 8.20 after the recent update. Could you look into this ASAP?\n",
      "\n",
      "Sincerely,\n",
      "<name>\n",
      "Answer_english: Dear <name>,\n",
      "\n",
      "Thank you for your message. We're looking into the issue with Jira Software 8.20 and will get back to you soon with a solution.\n",
      "\n",
      "Sincerely,\n",
      "Customer Support\n",
      "\n",
      "====================================================================================================\n",
      "TICKET 5/5\n",
      "Rerank Score: 0.964 | Original Similarity: 0.739\n",
      "Type: Incident | Priority: high | Queue: Technical Support\n",
      "Language: fr\n",
      "\n",
      "Subject_english: Urgent: Major Bug in the Ticketing System\n",
      "Body_english: Dear Customer Support,\n",
      "\n",
      "We are experiencing significant disruptions in our project management due to a major bug in the Jira Software 8.20 ticketing system. This issue requires your immediate attention for troubleshooting and resolution. Please provide us with an update as soon as possible, as it severely impacts our daily operations. Please prioritize this request.\n",
      "\n",
      "Thank you.\n",
      "\n",
      "Sincerely,\n",
      "<name>\n",
      "Answer_english: Dear <name>,\n",
      "\n",
      "Thank you for letting us know about this major issue in Jira Software 8.20. We understand the urgency and are prioritizing the resolution of this major bug to minimize disruptions to your workflow. Our technical team is currently reviewing the situation and will keep you updated with a status report as soon as possible. We appreciate your patience and are committed to resolving this quickly.\n",
      "\n",
      "Sincerely,\n",
      "Customer Support\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Enter your ticket here\n",
    "user_ticket = {\n",
    "    \"subject\": test_df['subject_english'][ix],\n",
    "    \"body\": test_df['body_english'][ix]\n",
    "}\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"SUPPORT TICKET\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nSUBJECT: {user_ticket['subject']}\")\n",
    "print(f\"\\nBODY: {user_ticket['body']}\")\n",
    "print(f\"\\nRAG MODE: {CONFIG['rag_mode'].upper()}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Step 1: Initial retrieval\n",
    "query_text = f\"Subject: {user_ticket['subject']}\\nBody: {user_ticket['body']}\"\n",
    "print(f\"\\nStep 1: Retrieving {CONFIG['top_k_initial']} similar tickets...\")\n",
    "initial_results = search_similar_tickets(query_text, CONFIG['top_k_initial'])\n",
    "print(f\"Retrieved {len(initial_results)} tickets\")\n",
    "\n",
    "# Step 2: Re-ranking\n",
    "print(f\"\\nStep 2: Re-ranking to top {CONFIG['top_k_reranked']}...\")\n",
    "reranked_results = rerank_tickets(query_text, initial_results, CONFIG['top_k_reranked'])\n",
    "print(f\"Selected {len(reranked_results)} best matches\")\n",
    "\n",
    "# Show re-ranking improvement\n",
    "print(f\"\\nRe-ranking Impact:\")\n",
    "print(f\"  Before: Top ticket similarity = {initial_results[0]['similarity']:.3f}\")\n",
    "print(f\"  After:  Top ticket rerank score = {reranked_results[0]['rerank_score']:.3f}\")\n",
    "\n",
    "# Step 3: Generate answer\n",
    "print(f\"\\nStep 3: Generating answer (mode: {CONFIG['rag_mode']})...\")\n",
    "answer = generate_answer(query_text, reranked_results)\n",
    "confidence = calculate_confidence(reranked_results)\n",
    "print(f\"Answer generated (confidence: {confidence:.1%})\")\n",
    "\n",
    "# Step 4: Evaluate quality\n",
    "print(\"\\nStep 4: Evaluating answer quality...\")\n",
    "evaluation = evaluate_answer(query_text, answer)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GENERATED ANSWER\")\n",
    "print(\"=\"*100)\n",
    "display(Markdown(answer))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"QUALITY EVALUATION\")\n",
    "print(\"=\"*100)\n",
    "if 'error' not in evaluation:\n",
    "    print(f\"\\nOverall Score: {evaluation['overall']}/5\")\n",
    "    print(f\"Accuracy: {evaluation['accuracy']}/5\")\n",
    "    print(f\"Completeness: {evaluation['completeness']}/5\")\n",
    "    print(f\"Clarity: {evaluation['clarity']}/5\")\n",
    "    print(f\"Actionability: {evaluation['actionability']}/5\")\n",
    "    print(f\"\\nConfidence: {confidence:.1%}\")\n",
    "    print(f\"\\nFeedback: {evaluation['feedback']}\")\n",
    "else:\n",
    "    print(f\"Evaluation error: {evaluation['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"TOP {len(reranked_results)} REFERENCES (after re-ranking)\")\n",
    "print(\"=\"*100)\n",
    "for i, ticket in enumerate(reranked_results, 1):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"TICKET {i}/{len(reranked_results)}\")\n",
    "    print(f\"Rerank Score: {ticket['rerank_score']:.3f} | Original Similarity: {ticket['similarity']:.3f}\")\n",
    "    print(f\"Type: {ticket['metadata'].get('type')} | Priority: {ticket['metadata'].get('priority')} | Queue: {ticket['metadata'].get('queue')}\")\n",
    "    print(f\"Language: {ticket['metadata'].get('original_language')}\")\n",
    "    print(f\"\\n{ticket['document']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
