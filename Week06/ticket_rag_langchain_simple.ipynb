{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket RAG System - LangChain Introduction\n",
    "\n",
    "**Simple introduction to LangChain and LCEL (LangChain Expression Language)**\n",
    "\n",
    "**What is LCEL?**: A way to chain components together using the pipe operator `|`\n",
    "\n",
    "**Example**: `input | step1 | step2 | output`\n",
    "\n",
    "**Pipeline**: Retrieve documents ‚Üí Format context ‚Üí Generate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "### What We're Installing\n",
    "\n",
    "This cell installs the LangChain ecosystem packages we need:\n",
    "\n",
    "- **`langchain`**: Core LangChain framework with base classes and utilities\n",
    "- **`langchain-community`**: Community integrations (HuggingFace embeddings, etc.)\n",
    "- **`langchain-openai`**: OpenAI-compatible LLM interface (works with LM Studio!)\n",
    "- **`langchain-chroma`**: ChromaDB vector store integration\n",
    "\n",
    "### Why These Packages?\n",
    "\n",
    "LangChain is modular - you only install what you need. We're using:\n",
    "- HuggingFace for **embeddings** (free, local)\n",
    "- ChromaDB for **vector storage** (fast, embedded database)\n",
    "- OpenAI API format for **LLM** (compatible with LM Studio, OpenAI, many others)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain packages\n",
    "!pip install -q langchain langchain-community langchain-openai langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "### Import Organization\n",
    "\n",
    "Good practice: organize imports by category for readability.\n",
    "\n",
    "#### Standard Library\n",
    "- **`os`**: File system operations\n",
    "- **`pandas`**: Data manipulation (DataFrames)\n",
    "- **`typing`**: Type hints for better code documentation\n",
    "\n",
    "#### LangChain Core Components\n",
    "- **`ChatPromptTemplate`**: Structures prompts with system/user messages\n",
    "- **`StrOutputParser`**: Extracts text from LLM responses\n",
    "- **`RunnableLambda`**: Wraps functions to use in LCEL chains\n",
    "- **`ChatOpenAI`**: LLM interface (OpenAI API compatible)\n",
    "\n",
    "#### LangChain Integrations\n",
    "- **`HuggingFaceEmbeddings`**: Free, local text embeddings\n",
    "- **`Chroma`**: Vector database for similarity search\n",
    "- **`Document`**: Standard document format in LangChain\n",
    "- **`BaseRetriever`**: Base class for custom retrievers\n",
    "\n",
    "#### Supporting Libraries\n",
    "- **`chromadb`**: Direct ChromaDB client access\n",
    "- **`CrossEncoder`**: Re-ranking model from sentence-transformers\n",
    "- **`dotenv`**: Load environment variables (API keys)\n",
    "- **`IPython.display`**: Pretty output in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "# ChromaDB\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Re-ranker\n",
    "from sentence_transformers import CrossEncoder\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load secrets\n",
    "load_dotenv('./secrets.env', override=True)\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Centralized Configuration Pattern\n",
    "\n",
    "**Best Practice**: Keep all configuration in one dictionary for:\n",
    "- Easy modification (change once, affects everywhere)\n",
    "- Clear documentation of parameters\n",
    "- Reproducibility (share config = share exact setup)\n",
    "\n",
    "### Configuration Breakdown\n",
    "\n",
    "#### Data Configuration\n",
    "- **`csv_path`**: Location of ticket dataset (4k tickets, translated)\n",
    "- **`train_test_split`**: 80% training, 20% testing\n",
    "- **`random_seed`**: Ensures reproducible splits\n",
    "\n",
    "#### Vector Store\n",
    "- **`chroma_db_path`**: Where to persist the vector database\n",
    "- **`embedding_model`**: `all-MiniLM-L6-v2` (384 dimensions, fast, good quality)\n",
    "\n",
    "#### Retrieval Configuration\n",
    "- **`reranker_model`**: `mxbai-rerank-base-v1` (cross-encoder for better ranking)\n",
    "- **`top_k_initial`**: First stage retrieves 20 candidates\n",
    "- **`top_k_reranked`**: Re-ranker picks best 5\n",
    "\n",
    "#### LLM Configuration\n",
    "- **`lm_studio_url`**: Local LM Studio server (can also use OpenAI)\n",
    "- **`llm_model`**: Model name in LM Studio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'csv_path': './dataset-tickets-multi-lang3-4k-translated-all.csv',\n",
    "    'chroma_db_path': './chroma_ticket_db_langchain',\n",
    "    'train_test_split': 0.8,\n",
    "    'random_seed': 42,\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'reranker_model': 'mixedbread-ai/mxbai-rerank-base-v1',\n",
    "    'top_k_initial': 20,\n",
    "    'top_k_reranked': 5,\n",
    "    'lm_studio_url': 'http://192.168.7.171:1234',\n",
    "    'llm_model': 'gpt-oss-20b',\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### Data Preparation Steps\n",
    "\n",
    "This cell performs several important data operations:\n",
    "\n",
    "#### 1. Load CSV\n",
    "```python\n",
    "df = pd.read_csv(CONFIG['csv_path'])\n",
    "```\n",
    "Loads the multi-language ticket dataset with translations.\n",
    "\n",
    "#### 2. Clean Data\n",
    "```python\n",
    "df = df.dropna(subset=['subject_english', 'body_english', 'answer_english'])\n",
    "```\n",
    "Removes tickets with missing English translations (our RAG system uses English).\n",
    "\n",
    "#### 3. Shuffle Data\n",
    "```python\n",
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "```\n",
    "- **`frac=1`**: Sample 100% of rows (shuffle all)\n",
    "- **`random_state=42`**: Reproducible shuffle (same order every run)\n",
    "\n",
    "#### 4. Train/Test Split\n",
    "```python\n",
    "split_idx = int(len(df) * 0.8)  # 80% mark\n",
    "train_df = df_shuffled[:split_idx]   # First 80%\n",
    "test_df = df_shuffled[split_idx:]    # Last 20%\n",
    "```\n",
    "\n",
    "### Why Split Data?\n",
    "\n",
    "**Training Set**: Build the vector database (knowledge base)\n",
    "\n",
    "**Test Set**: Evaluate RAG performance (unseen tickets)\n",
    "\n",
    "This prevents \"cheating\" - the system must generalize, not memorize!\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "You should see approximately:\n",
    "- **Train**: ~3,200 tickets (80%)\n",
    "- **Test**: ~800 tickets (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2,880 tickets\n",
      "Test: 721 tickets\n"
     ]
    }
   ],
   "source": [
    "# Load and split\n",
    "df = pd.read_csv(CONFIG['csv_path'])\n",
    "df = df.dropna(subset=['subject_english', 'body_english', 'answer_english'])\n",
    "df_shuffled = df.sample(frac=1, random_state=CONFIG['random_seed']).reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(df_shuffled) * CONFIG['train_test_split'])\n",
    "train_df = df_shuffled[:split_idx].reset_index(drop=True)\n",
    "test_df = df_shuffled[split_idx:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} tickets\")\n",
    "print(f\"Test: {len(test_df):,} tickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LangChain Components\n",
    "\n",
    "### The Three Building Blocks of RAG\n",
    "\n",
    "Our RAG system needs:\n",
    "\n",
    "1. **Embeddings** ‚Üí Convert text to vectors\n",
    "2. **LLM** ‚Üí Generate answers\n",
    "3. **Re-ranker** ‚Üí Improve retrieval quality\n",
    "\n",
    "### Component Details\n",
    "\n",
    "#### 1. Embeddings Model (`HuggingFaceEmbeddings`)\n",
    "\n",
    "**What it does**: Converts text ‚Üí 384-dimensional vectors\n",
    "\n",
    "**Key parameters**:\n",
    "- `model_name`: `all-MiniLM-L6-v2` (fast, good quality, popular choice)\n",
    "- `device='mps'`: Use Apple Silicon GPU (use `'cpu'` on other systems)\n",
    "- `normalize_embeddings=True`: Unit vectors for cosine similarity\n",
    "\n",
    "**Why this model?**: Balance of speed, quality, and size\n",
    "- Fast inference (~1000 docs/sec)\n",
    "- Small memory footprint (~80MB)\n",
    "- Good quality for most tasks\n",
    "\n",
    "#### 2. LLM (`ChatOpenAI`)\n",
    "\n",
    "**What it does**: Generates natural language answers\n",
    "\n",
    "**Key parameters**:\n",
    "- `base_url`: LM Studio endpoint (OpenAI-compatible API)\n",
    "- `api_key`: Not needed for local LM Studio\n",
    "- `temperature=0.2`: Low randomness (more focused answers)\n",
    "- `max_tokens=6000`: Maximum response length\n",
    "\n",
    "**Why LM Studio?**: Free, local, private, fast!\n",
    "\n",
    "#### 3. Re-ranker (`CrossEncoder`)\n",
    "\n",
    "**What it does**: Scores query-document pairs for better ranking\n",
    "\n",
    "**Model**: `mxbai-rerank-base-v1` (mixed bread AI)\n",
    "- More accurate than vector similarity alone\n",
    "- Slower (can't run on all documents)\n",
    "- Used in Stage 2 of retrieval\n",
    "\n",
    "### Performance Notes\n",
    "\n",
    "**First run**: Downloads models (~100MB total)\n",
    "\n",
    "**Subsequent runs**: Loads from cache (fast!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "‚úÖ Embeddings ready\n",
      "\n",
      "Connecting to LM Studio...\n",
      "‚úÖ LLM ready\n",
      "\n",
      "Loading re-ranker...\n",
      "‚úÖ Re-ranker ready\n"
     ]
    }
   ],
   "source": [
    "# 1. Embeddings\n",
    "print(\"Loading embeddings...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=CONFIG['embedding_model'],\n",
    "    model_kwargs={'device': 'mps'}, # use 'cpu' or 'cuda' on other systems\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(f\"‚úÖ Embeddings ready\")\n",
    "\n",
    "# 2. LLM\n",
    "print(\"\\nConnecting to LM Studio...\")\n",
    "llm = ChatOpenAI(\n",
    "    base_url=f\"{CONFIG['lm_studio_url']}/v1\",\n",
    "    api_key=\"not-needed\",\n",
    "    model=CONFIG['llm_model'],\n",
    "    temperature=0.2,\n",
    "    max_tokens=6000\n",
    ")\n",
    "print(f\"‚úÖ LLM ready\")\n",
    "\n",
    "# 3. Re-ranker\n",
    "print(\"\\nLoading re-ranker...\")\n",
    "reranker = CrossEncoder(CONFIG['reranker_model'])\n",
    "print(f\"‚úÖ Re-ranker ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "\n",
    "### üóÑÔ∏è Building the Knowledge Base\n",
    "\n",
    "This is where we create the **vector database** - the core of our RAG system.\n",
    "\n",
    "### Two-Step Process\n",
    "\n",
    "#### Step 1: Convert Tickets ‚Üí LangChain Documents\n",
    "\n",
    "**Helper Function**: `create_ticket_text(row)`\n",
    "```python\n",
    "def create_ticket_text(row):\n",
    "    return f\"\"\"Subject: {row['subject_english']}\n",
    "Body: {row['body_english']}\n",
    "Answer: {row['answer_english']}\"\"\"\n",
    "```\n",
    "\n",
    "Formats each ticket as structured text with clear sections.\n",
    "\n",
    "**Document Creation**:\n",
    "```python\n",
    "Document(\n",
    "    page_content=create_ticket_text(row),  # The text content\n",
    "    metadata={                              # Extra information\n",
    "        'ticket_id': f\"ticket_{idx}\",\n",
    "        'type': str(row.get('type', 'unknown')),\n",
    "        'priority': str(row.get('priority', 'unknown'))\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**LangChain `Document` Class**:\n",
    "- `page_content`: The actual text (searchable)\n",
    "- `metadata`: Tags/attributes (filterable, returnable)\n",
    "\n",
    "#### Step 2: Build Vector Store\n",
    "\n",
    "**ChromaDB** is an embedded vector database:\n",
    "- Fast similarity search\n",
    "- Persistent storage (saves to disk)\n",
    "- No separate server needed\n",
    "\n",
    "### Note About This Cell\n",
    "\n",
    "**SKIP IF ALREADY BUILT**: If you've run this before and have a persisted database, you can skip re-creating it. The next cell handles loading from disk.\n",
    "\n",
    "### Technical Details\n",
    "\n",
    "**Embedding Process**:\n",
    "1. Take each document's `page_content`\n",
    "2. Pass through embedding model ‚Üí 384D vector\n",
    "3. Store vector + metadata in ChromaDB\n",
    "\n",
    "**Time**: ~30-60 seconds for 3,200 tickets\n",
    "\n",
    "**Storage**: ~50MB on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating documents...\n",
      "Created 2,880 documents\n",
      "\n",
      "Building vector store...\n",
      "üìù Using in-memory ChromaDB (rebuilds each run, but always works!)\n",
      "Vector store ready with 5,760 documents\n"
     ]
    }
   ],
   "source": [
    "# Helper function\n",
    "def create_ticket_text(row):\n",
    "    return f\"\"\"Subject: {row['subject_english']}\n",
    "Body: {row['body_english']}\n",
    "Answer: {row['answer_english']}\"\"\"\n",
    "\n",
    "# Convert to LangChain Documents\n",
    "print(\"Creating documents...\")\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=create_ticket_text(row),\n",
    "        metadata={\n",
    "            'ticket_id': f\"ticket_{idx}\",\n",
    "            'type': str(row.get('type', 'unknown')),\n",
    "            'priority': str(row.get('priority', 'unknown'))\n",
    "        }\n",
    "    )\n",
    "    for idx, row in train_df.iterrows()\n",
    "]\n",
    "print(f\"Created {len(documents):,} documents\")\n",
    "\n",
    "# Build vector store (in-memory for simplicity)\n",
    "print(\"\\nBuilding vector store...\")\n",
    "print(\"üìù Using in-memory ChromaDB (rebuilds each run, but always works!)\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"tickets_simple\"\n",
    ")\n",
    "print(f\"Vector store ready with {vectorstore._collection.count():,} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Retriever with Re-ranking\n",
    "\n",
    "### Two-Stage Retrieval Strategy\n",
    "\n",
    "This custom retriever implements a **sophisticated retrieval pipeline**:\n",
    "\n",
    "```\n",
    "Query ‚Üí Vector Search (fast, ~20 docs) ‚Üí Re-rank (slow, accurate) ‚Üí Top 5 docs\n",
    "```\n",
    "\n",
    "### Why Custom Retriever?\n",
    "\n",
    "LangChain's `BaseRetriever` class lets us create specialized retrievers that fit into LCEL chains seamlessly.\n",
    "\n",
    "### Code Breakdown\n",
    "\n",
    "#### Class Definition\n",
    "```python\n",
    "class SimpleRerankRetriever(BaseRetriever):\n",
    "```\n",
    "Inherits from `BaseRetriever` ‚Üí compatible with all LangChain chain operations.\n",
    "\n",
    "#### Required Fields\n",
    "```python\n",
    "vectorstore: Chroma         # Where to search\n",
    "reranker: CrossEncoder      # How to re-rank\n",
    "k_initial: int = 20         # Stage 1: get 20 candidates\n",
    "k_final: int = 5            # Stage 2: return top 5\n",
    "```\n",
    "\n",
    "#### Core Method: `_get_relevant_documents(query)`\n",
    "\n",
    "**Stage 1: Vector Search**\n",
    "```python\n",
    "docs = self.vectorstore.similarity_search(query, k=self.k_initial)\n",
    "```\n",
    "- Uses cosine similarity on embeddings\n",
    "- Fast: ~10-20ms for 3k documents\n",
    "- Gets more candidates than needed\n",
    "\n",
    "**Stage 2: Re-ranking**\n",
    "```python\n",
    "pairs = [[query, doc.page_content] for doc in docs]\n",
    "scores = self.reranker.predict(pairs)\n",
    "```\n",
    "- Cross-encoder scores each query-document pair\n",
    "- Slower: ~100-200ms for 20 pairs\n",
    "- Much more accurate than vector similarity alone\n",
    "\n",
    "**Stage 3: Sort and Filter**\n",
    "```python\n",
    "docs_sorted = sorted(docs, key=lambda x: x.metadata['rerank_score'], reverse=True)\n",
    "return docs_sorted[:self.k_final]\n",
    "```\n",
    "- Sorts by re-rank score (highest first)\n",
    "- Returns only top 5\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "| Method | Speed | Accuracy | Scalability |\n",
    "|--------|-------|----------|-------------|\n",
    "| Vector search only | Fast | OK | Great |\n",
    "| Re-rank all docs | Slow | Great | Poor |\n",
    "| **Two-stage** | Fast | Great | Great |\n",
    "\n",
    "### Performance Numbers\n",
    "\n",
    "For 3,200 documents:\n",
    "- **Vector search** (k=20): ~15ms\n",
    "- **Re-ranking** (20 docs): ~150ms\n",
    "- **Total**: ~165ms per query\n",
    "\n",
    "Compare to re-ranking all 3,200 docs: ~24,000ms! ‚è±Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready\n"
     ]
    }
   ],
   "source": [
    "class SimpleRerankRetriever(BaseRetriever):\n",
    "    \"\"\"Two-stage retrieval: vector search ‚Üí re-rank.\"\"\"\n",
    "    \n",
    "    vectorstore: Chroma\n",
    "    reranker: CrossEncoder\n",
    "    k_initial: int = 20\n",
    "    k_final: int = 5\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Stage 1: Get initial results\n",
    "        docs = self.vectorstore.similarity_search(query, k=self.k_initial)\n",
    "        \n",
    "        # Stage 2: Re-rank\n",
    "        pairs = [[query, doc.page_content] for doc in docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Add scores and sort\n",
    "        for doc, score in zip(docs, scores):\n",
    "            doc.metadata['rerank_score'] = float(score)\n",
    "        \n",
    "        docs_sorted = sorted(docs, key=lambda x: x.metadata['rerank_score'], reverse=True)\n",
    "        return docs_sorted[:self.k_final]\n",
    "\n",
    "# Create retriever\n",
    "retriever = SimpleRerankRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    reranker=reranker,\n",
    "    k_initial=CONFIG['top_k_initial'],\n",
    "    k_final=CONFIG['top_k_reranked']\n",
    ")\n",
    "\n",
    "print(\"Retriever ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "### Context Formatting Function\n",
    "\n",
    "This simple but crucial function transforms retrieved documents into a formatted context string for the LLM.\n",
    "\n",
    "### Code Analysis\n",
    "\n",
    "```python\n",
    "def format_docs_to_context(docs: List[Document]) -> str:\n",
    "```\n",
    "\n",
    "**Input**: List of `Document` objects (from our retriever)\n",
    "\n",
    "**Output**: Single formatted string\n",
    "\n",
    "### Formatting Strategy\n",
    "\n",
    "For each document:\n",
    "1. Add a separator: `--- Ticket {i} ---`\n",
    "2. Include the re-rank score: `(score: 0.845)`\n",
    "3. Add the document content\n",
    "\n",
    "**Example Output**:\n",
    "```\n",
    "--- Ticket 1 (score: 0.892) ---\n",
    "Subject: Cannot access email\n",
    "Body: User reports login fails...\n",
    "Answer: Reset password via admin panel...\n",
    "\n",
    "--- Ticket 2 (score: 0.756) ---\n",
    "Subject: VPN connection issues\n",
    "Body: Remote worker cannot connect...\n",
    "Answer: Check firewall settings...\n",
    "```\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Clear Structure**: LLM can easily distinguish between tickets\n",
    "\n",
    "**Score Information**: LLM knows which tickets are most relevant\n",
    "\n",
    "**Numbered Tickets**: LLM can cite specific tickets in answer\n",
    "\n",
    "### LCEL Integration\n",
    "\n",
    "This function will be wrapped in `RunnableLambda` to use in our chain.\n",
    "\n",
    "### What is RunnableLambda?\n",
    "RunnableLambda is a special component from LangChain that acts as a \"wrapper\" for any function you want. Its purpose is to make a standard Python function compatible with the LCEL chain syntax.\n",
    "Think of an LCEL chain as a series of connected steps, like this:\n",
    "'''python\n",
    "retriever | another_step | llm\n",
    "'''\n",
    "Every component in this chain must \"speak the same language\"‚Äîthey must have standard invoke(), batch(), and stream() methods. A regular Python function like def my_func(x): doesn't have these.\n",
    "RunnableLambda takes your function and wraps it in a \"Runnable\" interface, giving it the necessary methods so it can be seamlessly plugged into a chain.\n",
    "```python\n",
    "retriever | RunnableLambda(format_docs_to_context) | prompt | llm\n",
    "```\n",
    "\n",
    "Simple functions ‚Üí Powerful chains!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions ready\n"
     ]
    }
   ],
   "source": [
    "def format_docs_to_context(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents into context string.\"\"\"\n",
    "    parts = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        score = doc.metadata.get('rerank_score', 0)\n",
    "        parts.append(f\"\\n--- Ticket {i} (score: {score:.3f}) ---\\n{doc.page_content}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "print(\"Helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LCEL Chain\n",
    "\n",
    "### Introduction to LCEL (LangChain Expression Language)\n",
    "\n",
    "**This is the magic of LangChain!** LCEL lets you chain components together using the **pipe operator `|`**.\n",
    "\n",
    "Think of it like Unix pipes: `cat file.txt | grep \"error\" | wc -l`\n",
    "\n",
    "### Our RAG Chain\n",
    "\n",
    "Here's our full RAG chain in one expression:\n",
    "\n",
    "```python\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs_to_context), \n",
    "     \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "### How It Works - Step by Step\n",
    "\n",
    "```\n",
    "Input Query\n",
    "    ‚Üì\n",
    "Step 1: Retrieve & Format\n",
    "    ‚îú‚îÄ retriever finds similar tickets\n",
    "    ‚îú‚îÄ format_docs_to_context creates context string\n",
    "    ‚îî‚îÄ question stays as-is\n",
    "    ‚Üì\n",
    "Step 2: Build Prompt\n",
    "    ‚îî‚îÄ Combines context + question into prompt\n",
    "    ‚Üì\n",
    "Step 3: Generate Answer\n",
    "    ‚îî‚îÄ LLM creates response\n",
    "    ‚Üì\n",
    "Step 4: Extract Text\n",
    "    ‚îî‚îÄ Parse string from LLM response\n",
    "    ‚Üì\n",
    "Final Answer\n",
    "```\n",
    "\n",
    "### Breaking Down the Code\n",
    "\n",
    "#### The Dictionary: Preparing Inputs\n",
    "\n",
    "```python\n",
    "{\"context\": retriever | RunnableLambda(format_docs_to_context), \n",
    " \"question\": lambda x: x}\n",
    "```\n",
    "\n",
    "This creates a dictionary with two values:\n",
    "\n",
    "- **`context`**: Retrieved and formatted tickets\n",
    "  1. `retriever` gets relevant tickets (with built-in re-ranking)\n",
    "  2. `format_docs_to_context` formats them into a string\n",
    "  \n",
    "- **`question`**: The user's query (unchanged)\n",
    "  - `lambda x: x` just passes the input through\n",
    "  - We need this because the prompt expects both `{context}` and `{question}`\n",
    "\n",
    "#### The Prompt Template\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an IT support assistant...\"),\n",
    "    (\"user\", \"HISTORICAL TICKETS:\\n{context}\\n\\nUSER QUESTION:\\n{question}...\")\n",
    "])\n",
    "```\n",
    "\n",
    "Takes the dictionary and fills in the template variables.\n",
    "\n",
    "#### The LLM\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(base_url=f\"{CONFIG['lm_studio_url']}/v1\", ...)\n",
    "```\n",
    "\n",
    "Sends the prompt to LM Studio and generates an answer.\n",
    "\n",
    "#### The Output Parser\n",
    "\n",
    "```python\n",
    "StrOutputParser()\n",
    "```\n",
    "\n",
    "Extracts just the text from the LLM's response.\n",
    "\n",
    "### LCEL Concepts\n",
    "\n",
    "#### 1. Pipe Operator `|`\n",
    "Chains components together - output of one becomes input of the next.\n",
    "\n",
    "#### 2. RunnableLambda\n",
    "```python\n",
    "RunnableLambda(format_docs_to_context)\n",
    "```\n",
    "Wraps regular Python functions so they work in chains.\n",
    "\n",
    "#### 3. Lambda Functions\n",
    "```python\n",
    "lambda x: x\n",
    "```\n",
    "A simple pass-through function.\n",
    "\n",
    "#### 4. Dictionary Inputs\n",
    "```python\n",
    "{\"key1\": value1, \"key2\": value2}\n",
    "```\n",
    "Used when the next step needs multiple named inputs (like prompt variables).\n",
    "\n",
    "### Why Use LCEL?\n",
    "\n",
    "**Readable**: Easy to see the flow at a glance\n",
    "\n",
    "**Composable**: Swap out components easily\n",
    "\n",
    "**Consistent**: Same pattern works for simple and complex chains\n",
    "\n",
    "### Using the Chain\n",
    "\n",
    "```python\n",
    "# Get an answer\n",
    "answer = rag_chain.invoke(\"How do I reset my password?\")\n",
    "\n",
    "# Stream the response word-by-word\n",
    "for chunk in rag_chain.stream(\"How do I reset my password?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LCEL chain built!\n",
      "   Flow: Query ‚Üí Retrieve ‚Üí Format ‚Üí Prompt ‚Üí LLM ‚Üí Answer\n"
     ]
    }
   ],
   "source": [
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an IT support assistant. Answer based on the historical tickets provided.\"),\n",
    "    (\"user\", \"\"\"HISTORICAL TICKETS:\n",
    "{context}\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "Provide a helpful answer based on the historical tickets above.\"\"\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "# Step 1: Retrieve ‚Üí Step 2: Format ‚Üí Step 3: Prompt ‚Üí Step 4: LLM ‚Üí Step 5: Parse\n",
    "chain = (\n",
    "    retriever                                    # Step 1: Get documents\n",
    "    | RunnableLambda(format_docs_to_context)     # Step 2: Format to context string\n",
    "    | RunnableLambda(lambda context: {           # Step 3: Prepare prompt inputs\n",
    "        \"context\": context,\n",
    "        \"question\": \"\"  # Will be filled during invoke\n",
    "    })\n",
    ")\n",
    "\n",
    "# Complete RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs_to_context), \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LCEL chain built!\")\n",
    "print(\"   Flow: Query ‚Üí Retrieve ‚Üí Format ‚Üí Prompt ‚Üí LLM ‚Üí Answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Chain\n",
    "\n",
    "### End-to-End RAG System Test\n",
    "\n",
    "Now we put everything together and see the RAG system in action!\n",
    "\n",
    "### Test Setup\n",
    "\n",
    "#### 1. Select Test Ticket\n",
    "```python\n",
    "test_ticket = test_df.iloc[555]\n",
    "```\n",
    "Picks ticket #555 from our **test set** (unseen by the vector database).\n",
    "\n",
    "#### 2. Format Query\n",
    "```python\n",
    "query = f\"\"\"Subject: {test_ticket['subject_english']}\n",
    "Body: {test_ticket['body_english']}\"\"\"\n",
    "```\n",
    "Creates query in same format as training data (but **without** the answer!).\n",
    "\n",
    "### What Happens When You Run This Cell\n",
    "\n",
    "#### Behind the Scenes:\n",
    "\n",
    "1. **Retrieval Stage**\n",
    "   - Query embedding: Convert query text ‚Üí 384D vector\n",
    "   - Vector search: Find 20 similar tickets (cosine similarity)\n",
    "   - Re-ranking: Score all 20 with cross-encoder\n",
    "   - Selection: Pick top 5 re-ranked results\n",
    "\n",
    "2. **Generation Stage**\n",
    "   - Format: Convert 5 documents ‚Üí context string\n",
    "   - Prompt: Combine context + query ‚Üí complete prompt\n",
    "   - LLM Call: Send to LM Studio\n",
    "   - Parse: Extract text from response\n",
    "\n",
    "#### Performance Expectations:\n",
    "- **Retrieval**: ~165ms (vector search + re-ranking)\n",
    "- **LLM Generation**: ~2-5 seconds (depends on model/hardware)\n",
    "- **Total**: ~2-6 seconds\n",
    "\n",
    "### Evaluating Results\n",
    "\n",
    "**Good signs**:\n",
    "- Answer addresses the specific question\n",
    "- Uses information from retrieved tickets\n",
    "- Formatted professionally\n",
    "- Includes troubleshooting steps\n",
    "\n",
    "**Red flags**:\n",
    "- Generic answer (not using retrieval context)\n",
    "- Hallucinated information\n",
    "- Doesn't match the ticket type\n",
    "- Overly brief or unclear\n",
    "\n",
    "### Interactive Experiment\n",
    "\n",
    "Try changing `test_df.iloc[555]` to different indices:\n",
    "- `test_df.iloc[0]` - First test ticket\n",
    "- `test_df.iloc[100]` - Another random ticket\n",
    "- `test_df.sample(1).iloc[0]` - Random ticket\n",
    "\n",
    "Compare:\n",
    "1. Original answer (in test set)\n",
    "2. RAG-generated answer\n",
    "3. Retrieved ticket answers\n",
    "\n",
    "This helps you understand when RAG works well vs. needs improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST TICKET\n",
      "================================================================================\n",
      "Subject: Issue with Jira Software 8.20\n",
      "Body: Dear Customer Support,\n",
      "\n",
      "I can't create new tickets in Jira Software 8.20 after the recent update. Could you please look into this urgently?\n",
      "\n",
      "Best regards,\n",
      "<name>\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîÑ Running LCEL chain...\n",
      "\n",
      "================================================================================\n",
      "GENERATED ANSWER\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Subject:** Re: Issue with Jira Software 8.20 ‚Äì Ticket Creation Problem  \n",
       "\n",
       "Hi <name>,\n",
       "\n",
       "Thank you for bringing this to our attention. I understand how disruptive it is when new tickets can‚Äôt be created, especially after an update. Below are some steps that have helped other customers resolve the same issue quickly. Please try them and let me know what happens.\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Browser‚ÄëRelated Checks  \n",
       "| Step | What to do | Why it matters |\n",
       "|------|------------|----------------|\n",
       "| **Clear cache & cookies** | `Settings ‚Üí Privacy ‚Üí Clear browsing data` (choose ‚ÄúCookies‚Äù and ‚ÄúCached images/files‚Äù) | Old cached data can interfere with the new update. |\n",
       "| **Disable extensions/add‚Äëons** | Turn off all browser extensions, especially ad‚Äëblockers or script blockers | Some extensions block Jira scripts. |\n",
       "| **Try a private/incognito window** | Open a new incognito tab and log in to Jira | This runs a clean session without stored data. |\n",
       "| **Switch browsers** | If you‚Äôre on Chrome, try Firefox (or vice‚Äëversa) | Helps isolate browser‚Äëspecific issues. |\n",
       "\n",
       "---\n",
       "\n",
       "### 2. Permissions & Project Settings  \n",
       "1. **Verify your role** ‚Äì Make sure you have ‚ÄúCreate Issues‚Äù permission in the project(s) where you‚Äôre trying to create tickets.  \n",
       "2. **Check issue type scheme** ‚Äì After an update, sometimes the default issue types get reset. Confirm that the issue type you‚Äôre selecting is still available for the project.\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Network & Console Diagnostics  \n",
       "1. **Open DevTools (F12)** ‚Üí **Console tab** while attempting to create a ticket. Look for any red error messages ‚Äì copy them here if possible.  \n",
       "2. **Check network requests** ‚Äì In DevTools ‚Üí **Network**, filter by ‚ÄúXHR‚Äù or ‚ÄúFetch‚Äù. When you click ‚ÄúCreate‚Äù, look for any failed requests (status 4xx/5xx).  \n",
       "\n",
       "---\n",
       "\n",
       "### 4. Jira Logs (if you have admin access)  \n",
       "1. Go to **Administration ‚Üí System ‚Üí Logging and Profiling**.  \n",
       "2. Enable **‚ÄúDebug‚Äù** level for the component `com.atlassian.jira.issue`.  \n",
       "3. Re‚Äëattempt ticket creation, then download the log file from the same page.  \n",
       "\n",
       "---\n",
       "\n",
       "### 5. Test on a Fresh Profile / Machine  \n",
       "If possible, try logging into Jira from another computer or user profile. This can confirm whether the issue is local to your machine.\n",
       "\n",
       "---\n",
       "\n",
       "## What to Send Back\n",
       "\n",
       "- A screenshot of any error message (or copy‚Äëpaste the console output).  \n",
       "- The browser you‚Äôre using and its version.  \n",
       "- Whether you‚Äôre on a corporate VPN or proxy ‚Äì sometimes those block certain ports.  \n",
       "\n",
       "With that information we can pinpoint whether it‚Äôs a client‚Äëside problem, a permission issue, or something deeper in Jira.\n",
       "\n",
       "---\n",
       "\n",
       "### Next Steps\n",
       "\n",
       "If the above steps don‚Äôt resolve the problem, I‚Äôll open an **internal ticket** and assign it to our Jira Engineering team for deeper investigation. We‚Äôll keep you posted on any progress.\n",
       "\n",
       "Thank you for your patience ‚Äì we‚Äôre committed to getting this fixed as quickly as possible.\n",
       "\n",
       "Best regards,\n",
       "\n",
       "<Your Name>  \n",
       "Customer Support Team\n",
       "\n",
       "--- \n",
       "\n",
       "*If you need immediate assistance, feel free to reply with the details above or call our 24/7 support line at <support‚Äëphone>.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get test ticket\n",
    "test_ticket = test_df.iloc[555]\n",
    "\n",
    "query = f\"\"\"Subject: {test_ticket['subject_english']}\n",
    "Body: {test_ticket['body_english']}\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST TICKET\")\n",
    "print(\"=\"*80)\n",
    "print(query)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run the chain\n",
    "print(\"\\nüîÑ Running LCEL chain...\\n\")\n",
    "answer = rag_chain.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATED ANSWER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display answer as Markdown for better formatting\n",
    "display(Markdown(answer))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Retrieved Documents\n",
    "\n",
    "### Examining the Retrieval Quality\n",
    "\n",
    "This cell lets us inspect **what the system actually retrieved** - crucial for debugging and understanding RAG performance!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "The quality of your RAG answer is **directly dependent** on retrieval quality:\n",
    "\n",
    "```\n",
    "Bad Retrieval ‚Üí Bad Answer (even with great LLM)\n",
    "Good Retrieval ‚Üí Good Answer (even with mediocre LLM)\n",
    "```\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "When examining retrieved documents:\n",
    "\n",
    "#### Good Retrieval Signs\n",
    "\n",
    "1. **Relevant Content**\n",
    "   - Documents address similar issues\n",
    "   - Terminology matches the query\n",
    "   - Solutions are applicable\n",
    "\n",
    "2. **High Re-rank Scores**\n",
    "   - Top document: >0.7 (strong match)\n",
    "   - Documents 2-5: >0.4 (relevant)\n",
    "   - Clear score separation (best doc significantly higher)\n",
    "\n",
    "3. **Diversity**\n",
    "   - Different tickets with same solution\n",
    "   - Multiple perspectives on same problem\n",
    "   - Various contexts (different users, setups)\n",
    "\n",
    "#### Poor Retrieval Signs\n",
    "\n",
    "1. **Irrelevant Content**\n",
    "   - Documents about different topics\n",
    "   - Wrong product/system\n",
    "   - Unrelated technical issues\n",
    "\n",
    "2. **Low Re-rank Scores**\n",
    "   - All scores <0.5 (weak matches)\n",
    "   - Scores very similar (no clear winner)\n",
    "   - Top score <0.3 (likely poor answer quality)\n",
    "\n",
    "3. **Duplicates**\n",
    "   - Same ticket multiple times\n",
    "   - Identical or near-identical solutions\n",
    "   - No information diversity\n",
    "\n",
    "### Analysis Workflow\n",
    "\n",
    "For each retrieved document:\n",
    "\n",
    "1. **Check relevance**: Does it actually relate to the query?\n",
    "2. **Evaluate score**: Is the re-rank score reasonable?\n",
    "3. **Assess quality**: Would this help answer the question?\n",
    "4. **Compare ranks**: Are better documents ranked higher?\n",
    "\n",
    "### Debugging Tips\n",
    "\n",
    "**If retrieval is poor**:\n",
    "- Try different embedding models\n",
    "- Adjust `k_initial` and `k_final` values\n",
    "- Check if query format matches training data\n",
    "- Verify documents are properly indexed\n",
    "\n",
    "**If scores seem wrong**:\n",
    "- Re-ranker might need different model\n",
    "- Query might be too short/vague\n",
    "- Documents might need better formatting\n",
    "\n",
    "### Learning Exercise\n",
    "\n",
    "Compare the retrieved documents to:\n",
    "1. The original test ticket answer\n",
    "2. The RAG-generated answer\n",
    "3. Other similar tickets in the test set\n",
    "\n",
    "This helps you understand:\n",
    "- What information the LLM had available\n",
    "- Why it generated its specific answer\n",
    "- How retrieval quality affects output quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 5 RETRIEVED TICKETS\n",
      "================================================================================\n",
      "\n",
      "--- Ticket 1 (rerank score: 0.974) ---\n",
      "Subject: Urgent Issue: Interruption in Jira Ticket Creation\n",
      "Body: Dear Customer Service,\n",
      "\n",
      "I am writing to report a critical issue we are experiencing with Jira Software version 8.20. Our team is facing significant problems when trying to create new tickets, which is severely disrupting our project m...\n",
      "\n",
      "--- Ticket 2 (rerank score: 0.974) ---\n",
      "Subject: Urgent Issue: Interruption in Jira Ticket Creation\n",
      "Body: Dear Customer Service,\n",
      "\n",
      "I am writing to report a critical issue we are experiencing with Jira Software version 8.20. Our team is facing significant problems when trying to create new tickets, which is severely disrupting our project m...\n",
      "\n",
      "--- Ticket 3 (rerank score: 0.973) ---\n",
      "Subject: Issue with Jira Software 8.20\n",
      "Body: Dear Customer Support,\n",
      "\n",
      "I can't create new tickets in Jira Software 8.20 after the recent update. Could you look into this ASAP?\n",
      "\n",
      "Sincerely,\n",
      "<name>\n",
      "Answer: Dear <name>,\n",
      "\n",
      "Thank you for your message. We're looking into the issue with Jira Software 8.20 and ...\n",
      "\n",
      "--- Ticket 4 (rerank score: 0.973) ---\n",
      "Subject: Issue with Jira Software 8.20\n",
      "Body: Dear Customer Support,\n",
      "\n",
      "I can't create new tickets in Jira Software 8.20 after the recent update. Could you look into this ASAP?\n",
      "\n",
      "Sincerely,\n",
      "<name>\n",
      "Answer: Dear <name>,\n",
      "\n",
      "Thank you for your message. We're looking into the issue with Jira Software 8.20 and ...\n",
      "\n",
      "--- Ticket 5 (rerank score: 0.973) ---\n",
      "Subject: Urgent Issue: Trouble Creating Jira Tickets\n",
      "Body: Hey Customer Support,\n",
      "\n",
      "I'm reaching out to let you know about a critical issue we're having with Jira Software 8.20. Our team is running into major problems when trying to create new tickets, and it's seriously disrupting our project workflo...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get the documents that were retrieved\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP {len(docs)} RETRIEVED TICKETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    score = doc.metadata.get('rerank_score', 0)\n",
    "    print(f\"\\n--- Ticket {i} (rerank score: {score:.3f}) ---\")\n",
    "    print(doc.page_content[:300] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding LCEL\n",
    "\n",
    "### What We Just Built\n",
    "\n",
    "```python\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "### Breaking It Down\n",
    "\n",
    "1. **`retriever`** - Gets relevant documents from vector store\n",
    "2. **`| format_docs`** - Formats documents into context string\n",
    "3. **`| prompt`** - Creates the prompt with context and question\n",
    "4. **`| llm`** - Sends to LLM and gets response\n",
    "5. **`| StrOutputParser()`** - Extracts text from LLM response\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "**1. Readable**: Chain flow is clear and easy to understand\n",
    "\n",
    "**2. Modular**: Each component can be tested independently\n",
    "```python\n",
    "# Test just the retriever\n",
    "docs = retriever.invoke(\"test query\")\n",
    "```\n",
    "\n",
    "**3. Reusable**: Components can be used in different chains\n",
    "```python\n",
    "# Use same retriever in different chain\n",
    "another_chain = retriever | different_prompt | llm\n",
    "```\n",
    "\n",
    "**4. Composable**: Easy to add or remove steps\n",
    "```python\n",
    "# Add a translation step\n",
    "chain_with_translation = retriever | format | prompt | llm | translator\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Original vs LCEL\n",
    "\n",
    "### Original Approach (Procedural)\n",
    "```python\n",
    "def rag_pipeline(query):\n",
    "    docs = retrieve(query)\n",
    "    context = format(docs)\n",
    "    prompt_text = build_prompt(context, query)\n",
    "    answer = llm.generate(prompt_text)\n",
    "    return answer\n",
    "```\n",
    "\n",
    "### LCEL Approach (Declarative)\n",
    "```python\n",
    "rag_chain = retriever | format_docs | prompt | llm | parser\n",
    "answer = rag_chain.invoke(query)\n",
    "```\n",
    "\n",
    "### Why LCEL?\n",
    "\n",
    "| Feature | Original | LCEL |\n",
    "|---------|----------|------|\n",
    "| Code style | Procedural | Declarative |\n",
    "| Testing | Full pipeline | Per component |\n",
    "| Reusability | Copy-paste | Compose |\n",
    "| Streaming | Manual | Built-in |\n",
    "| Modification | Edit function | Swap components |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand basic LCEL chains, you can:\n",
    "\n",
    "1. **Add streaming**: Get answers word-by-word\n",
    "```python\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "2. **Add error handling**: Catch and handle errors gracefully\n",
    "```python\n",
    "chain_with_fallback = rag_chain.with_fallbacks([backup_chain])\n",
    "```\n",
    "\n",
    "3. **Add evaluation**: Chain an evaluation step\n",
    "```python\n",
    "full_chain = rag_chain | evaluation_chain\n",
    "```\n",
    "\n",
    "4. **Batch processing**: Process multiple queries\n",
    "```python\n",
    "answers = rag_chain.batch([query1, query2, query3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What We Learned**:\n",
    "1. LangChain components (embeddings, LLM, retriever, prompts)\n",
    "2. LCEL syntax: chaining with `|` operator\n",
    "3. Sequential data flow through components\n",
    "4. Benefits: modularity, testability, reusability\n",
    "\n",
    "**What We Built**:\n",
    "- Simple RAG chain: Retrieve ‚Üí Format ‚Üí Generate\n",
    "- Two-stage retrieval with re-ranking\n",
    "- Clean, composable architecture\n",
    "\n",
    "**This is the foundation** - from here we can build more complex chains with parallel execution, conditional logic, and advanced patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Homework Assignments\n",
    "\n",
    "Complete these exercises to deepen your understanding of LangChain and RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Add Streaming Output\n",
    "\n",
    "**Goal**: Modify the RAG chain to stream the LLM response word-by-word instead of waiting for the complete answer.\n",
    "\n",
    "**Tasks**:\n",
    "1. Research the `.stream()` method in LangChain\n",
    "2. Create a new cell that uses `rag_chain.stream(query)` instead of `rag_chain.invoke(query)`\n",
    "3. Print each chunk as it arrives with `end=\"\"` and `flush=True`\n",
    "4. Compare the user experience between streaming vs. non-streaming\n",
    "\n",
    "**Starter Code**:\n",
    "```python\n",
    "# TODO: Implement streaming\n",
    "query = \"How do I reset my password?\"\n",
    "\n",
    "print(\"Streaming answer: \", end=\"\", flush=True)\n",
    "# Your code here\n",
    "```\n",
    "\n",
    "**Bonus**: Add a timer to measure and compare response times for first token vs. complete response.\n",
    "\n",
    "**Hint**: replace THIS chunk of code with the stream() object:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "answer = rag_chain.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATED ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: Experiment with Retrieval Parameters (Intermediate)\n",
    "\n",
    "**Goal**: Understand how retrieval parameters affect answer quality by testing different configurations.\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a new retriever with different `k_initial` and `k_final` values\n",
    "2. Test at least 3 different configurations:\n",
    "   - Configuration A: k_initial=10, k_final=3\n",
    "   - Configuration B: k_initial=30, k_final=10\n",
    "   - Configuration C: k_initial=20, k_final=5 (baseline)\n",
    "3. Run the same query through each configuration\n",
    "4. Compare the retrieved documents and generated answers\n",
    "5. Analyze trade-offs between:\n",
    "   - Retrieval quality (are the right documents found?)\n",
    "   - Answer accuracy (is the LLM generating better answers?)\n",
    "   - Performance (speed and resource usage)\n",
    "\n",
    "**Starter Code**:\n",
    "```python\n",
    "# TODO: Create experimental configurations\n",
    "configs_to_test = [\n",
    "    {\"name\": \"Config A\", \"k_initial\": 10, \"k_final\": 3},\n",
    "    {\"name\": \"Config B\", \"k_initial\": 30, \"k_final\": 10},\n",
    "    {\"name\": \"Config C\", \"k_initial\": 20, \"k_final\": 5},\n",
    "]\n",
    "\n",
    "test_query = test_df.iloc[100]  # Pick a different test ticket\n",
    "\n",
    "for config in configs_to_test:\n",
    "    # Create retriever with config\n",
    "    # Run query\n",
    "    # Compare results\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Questions**:\n",
    "- Which configuration provided the best answer quality?\n",
    "- What's the performance impact of larger k_initial values?\n",
    "- Is there a point where more documents hurt answer quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: Build a Confidence Scoring Chain (Advanced)\n",
    "\n",
    "**Goal**: Extend the RAG chain to include a confidence score based on document relevance and answer quality.\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a confidence calculation function that:\n",
    "   - Analyzes rerank scores from retrieved documents\n",
    "   - Calculates metrics like: average score, score variance, top score\n",
    "   - Returns a confidence level: \"High\" (>0.7), \"Medium\" (0.4-0.7), \"Low\" (<0.4)\n",
    "2. Wrap the function in a `RunnableLambda`\n",
    "3. Add it to the chain using the pipe operator\n",
    "4. Test with multiple queries and verify confidence scores make sense\n",
    "\n",
    "**Starter Code**:\n",
    "```python\n",
    "def calculate_confidence(chain_output: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate confidence score from retrieval and answer.\n",
    "    \n",
    "    Args:\n",
    "        chain_output: Dict with 'answer', 'docs', and other chain data\n",
    "        \n",
    "    Returns:\n",
    "        Dict with original data plus 'confidence' and 'confidence_level'\n",
    "    \"\"\"\n",
    "    # TODO: Extract rerank scores from documents\n",
    "    # TODO: Calculate statistics (mean, max, variance)\n",
    "    # TODO: Determine confidence level\n",
    "    # TODO: Return enriched output\n",
    "    pass\n",
    "\n",
    "# TODO: Create enhanced chain\n",
    "enhanced_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs_to_context), \n",
    "     \"question\": lambda x: x,\n",
    "     \"docs\": retriever}  # Pass docs separately for confidence calculation\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(calculate_confidence)\n",
    ")\n",
    "\n",
    "# Test with various queries\n",
    "test_queries = [\n",
    "    \"How do I reset my password?\",  # Should have high confidence\n",
    "    \"What is quantum computing?\",     # Should have low confidence (off-topic)\n",
    "]\n",
    "```\n",
    "\n",
    "**Challenge**: Create a fallback mechanism that triggers when confidence is low, asking the user to rephrase or providing a disclaimer that the system is uncertain.\n",
    "\n",
    "**Bonus**: Log confidence scores and correlate them with actual answer quality by manual review."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
