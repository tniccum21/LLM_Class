{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket RAG System - LangChain Introduction\n",
    "\n",
    "**Simple introduction to LangChain and LCEL (LangChain Expression Language)**\n",
    "\n",
    "**What is LCEL?**: A way to chain components together using the pipe operator `|`\n",
    "\n",
    "**Example**: `input | step1 | step2 | output`\n",
    "\n",
    "**Pipeline**: Retrieve documents ‚Üí Format context ‚Üí Generate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Install Dependencies\n\n### üì¶ What We're Installing\n\nThis cell installs the LangChain ecosystem packages we need:\n\n- **`langchain`**: Core LangChain framework with base classes and utilities\n- **`langchain-community`**: Community integrations (HuggingFace embeddings, etc.)\n- **`langchain-openai`**: OpenAI-compatible LLM interface (works with LM Studio!)\n- **`langchain-chroma`**: ChromaDB vector store integration\n\n### üí° Why These Packages?\n\nLangChain is modular - you only install what you need. We're using:\n- HuggingFace for **embeddings** (free, local)\n- ChromaDB for **vector storage** (fast, embedded database)\n- OpenAI API format for **LLM** (compatible with LM Studio, OpenAI, many others)\n\n### ‚ö° The `-q` Flag\n\n`-q` means \"quiet\" - suppresses verbose installation output to keep notebook clean."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain packages\n",
    "!pip install -q langchain langchain-community langchain-openai langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Imports\n\n### üìö Import Organization\n\nGood practice: organize imports by category for readability.\n\n#### Standard Library\n- **`os`**: File system operations\n- **`pandas`**: Data manipulation (DataFrames)\n- **`typing`**: Type hints for better code documentation\n\n#### LangChain Core Components\n- **`ChatPromptTemplate`**: Structures prompts with system/user messages\n- **`StrOutputParser`**: Extracts text from LLM responses\n- **`RunnableLambda`**: Wraps functions to use in LCEL chains\n- **`ChatOpenAI`**: LLM interface (OpenAI API compatible)\n\n#### LangChain Integrations\n- **`HuggingFaceEmbeddings`**: Free, local text embeddings\n- **`Chroma`**: Vector database for similarity search\n- **`Document`**: Standard document format in LangChain\n- **`BaseRetriever`**: Base class for custom retrievers\n\n#### Supporting Libraries\n- **`chromadb`**: Direct ChromaDB client access\n- **`CrossEncoder`**: Re-ranking model from sentence-transformers\n- **`dotenv`**: Load environment variables (API keys)\n- **`IPython.display`**: Pretty output in Jupyter\n\n### üéØ Key Concept: LangChain Components\n\nLangChain uses **composable building blocks**:\n1. **Retrievers** ‚Üí Find relevant documents\n2. **Prompts** ‚Üí Structure LLM input\n3. **LLMs** ‚Üí Generate responses\n4. **Output Parsers** ‚Üí Format results\n\nWe chain these together with the **`|` operator**!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "# ChromaDB\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Re-ranker\n",
    "from sentence_transformers import CrossEncoder\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load secrets\n",
    "load_dotenv('./secrets.env', override=True)\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\n### ‚öôÔ∏è Centralized Configuration Pattern\n\n**Best Practice**: Keep all configuration in one dictionary for:\n- Easy modification (change once, affects everywhere)\n- Clear documentation of parameters\n- Reproducibility (share config = share exact setup)\n\n### üîß Configuration Breakdown\n\n#### Data Configuration\n- **`csv_path`**: Location of ticket dataset (4k tickets, translated)\n- **`train_test_split`**: 80% training, 20% testing\n- **`random_seed`**: Ensures reproducible splits\n\n#### Vector Store\n- **`chroma_db_path`**: Where to persist the vector database\n- **`embedding_model`**: `all-MiniLM-L6-v2` (384 dimensions, fast, good quality)\n\n#### Retrieval Configuration\n- **`reranker_model`**: `mxbai-rerank-base-v1` (cross-encoder for better ranking)\n- **`top_k_initial`**: First stage retrieves 20 candidates\n- **`top_k_reranked`**: Re-ranker picks best 5\n\n#### LLM Configuration\n- **`lm_studio_url`**: Local LM Studio server (can also use OpenAI)\n- **`llm_model`**: Model name in LM Studio\n\n### üí° Why Two-Stage Retrieval?\n\n**Stage 1 (Vector Search)**: Fast but approximate ‚Üí Get 20 candidates\n\n**Stage 2 (Re-ranking)**: Slow but accurate ‚Üí Pick best 5\n\nThis balances **speed** (don't re-rank everything) and **quality** (accurate final selection)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'csv_path': './dataset-tickets-multi-lang3-4k-translated-all.csv',\n",
    "    'chroma_db_path': './chroma_ticket_db_langchain',\n",
    "    'train_test_split': 0.8,\n",
    "    'random_seed': 42,\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'reranker_model': 'mixedbread-ai/mxbai-rerank-base-v1',\n",
    "    'top_k_initial': 20,\n",
    "    'top_k_reranked': 5,\n",
    "    'lm_studio_url': 'http://192.168.7.171:1234',\n",
    "    'llm_model': 'gpt-oss-20b',\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Data\n\n### üìä Data Preparation Steps\n\nThis cell performs several important data operations:\n\n#### 1. Load CSV\n```python\ndf = pd.read_csv(CONFIG['csv_path'])\n```\nLoads the multi-language ticket dataset with translations.\n\n#### 2. Clean Data\n```python\ndf = df.dropna(subset=['subject_english', 'body_english', 'answer_english'])\n```\nRemoves tickets with missing English translations (our RAG system uses English).\n\n#### 3. Shuffle Data\n```python\ndf_shuffled = df.sample(frac=1, random_state=42)\n```\n- **`frac=1`**: Sample 100% of rows (shuffle all)\n- **`random_state=42`**: Reproducible shuffle (same order every run)\n\n#### 4. Train/Test Split\n```python\nsplit_idx = int(len(df) * 0.8)  # 80% mark\ntrain_df = df_shuffled[:split_idx]   # First 80%\ntest_df = df_shuffled[split_idx:]    # Last 20%\n```\n\n### üéØ Why Split Data?\n\n**Training Set**: Build the vector database (knowledge base)\n\n**Test Set**: Evaluate RAG performance (unseen tickets)\n\nThis prevents \"cheating\" - the system must generalize, not memorize!\n\n### üìà Expected Output\n\nYou should see approximately:\n- **Train**: ~3,200 tickets (80%)\n- **Test**: ~800 tickets (20%)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split\n",
    "df = pd.read_csv(CONFIG['csv_path'])\n",
    "df = df.dropna(subset=['subject_english', 'body_english', 'answer_english'])\n",
    "df_shuffled = df.sample(frac=1, random_state=CONFIG['random_seed']).reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(df_shuffled) * CONFIG['train_test_split'])\n",
    "train_df = df_shuffled[:split_idx].reset_index(drop=True)\n",
    "test_df = df_shuffled[split_idx:].reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Train: {len(train_df):,} tickets\")\n",
    "print(f\"üìä Test: {len(test_df):,} tickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initialize LangChain Components\n\n### üß± The Three Building Blocks of RAG\n\nEvery RAG system needs:\n\n1. **Embeddings** ‚Üí Convert text to vectors\n2. **LLM** ‚Üí Generate answers\n3. **Re-ranker** ‚Üí Improve retrieval quality\n\n### üéØ Component Details\n\n#### 1. Embeddings Model (`HuggingFaceEmbeddings`)\n\n**What it does**: Converts text ‚Üí 384-dimensional vectors\n\n**Key parameters**:\n- `model_name`: `all-MiniLM-L6-v2` (fast, good quality, popular choice)\n- `device='mps'`: Use Apple Silicon GPU (use `'cpu'` on other systems)\n- `normalize_embeddings=True`: Unit vectors for cosine similarity\n\n**Why this model?**: Balance of speed, quality, and size\n- ‚úÖ Fast inference (~1000 docs/sec)\n- ‚úÖ Small memory footprint (~80MB)\n- ‚úÖ Good quality for most tasks\n\n#### 2. LLM (`ChatOpenAI`)\n\n**What it does**: Generates natural language answers\n\n**Key parameters**:\n- `base_url`: LM Studio endpoint (OpenAI-compatible API)\n- `api_key`: Not needed for local LM Studio\n- `temperature=0.2`: Low randomness (more focused answers)\n- `max_tokens=6000`: Maximum response length\n\n**Why LM Studio?**: Free, local, private, fast!\n\n#### 3. Re-ranker (`CrossEncoder`)\n\n**What it does**: Scores query-document pairs for better ranking\n\n**Model**: `mxbai-rerank-base-v1` (mixed bread AI)\n- More accurate than vector similarity alone\n- Slower (can't run on all documents)\n- Used in Stage 2 of retrieval\n\n### üìä Performance Notes\n\n**First run**: Downloads models (~100MB total)\n\n**Subsequent runs**: Loads from cache (fast!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embeddings\n",
    "print(\"Loading embeddings...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=CONFIG['embedding_model'],\n",
    "    model_kwargs={'device': 'mps'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(f\"‚úÖ Embeddings ready\")\n",
    "\n",
    "# 2. LLM\n",
    "print(\"\\nConnecting to LM Studio...\")\n",
    "llm = ChatOpenAI(\n",
    "    base_url=f\"{CONFIG['lm_studio_url']}/v1\",\n",
    "    api_key=\"not-needed\",\n",
    "    model=CONFIG['llm_model'],\n",
    "    temperature=0.2,\n",
    "    max_tokens=6000\n",
    ")\n",
    "print(f\"‚úÖ LLM ready\")\n",
    "\n",
    "# 3. Re-ranker\n",
    "print(\"\\nLoading re-ranker...\")\n",
    "reranker = CrossEncoder(CONFIG['reranker_model'])\n",
    "print(f\"‚úÖ Re-ranker ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Create Vector Store\n\n### üóÑÔ∏è Building the Knowledge Base\n\nThis is where we create the **vector database** - the core of our RAG system.\n\n### üìù Two-Step Process\n\n#### Step 1: Convert Tickets ‚Üí LangChain Documents\n\n**Helper Function**: `create_ticket_text(row)`\n```python\ndef create_ticket_text(row):\n    return f\"\"\"Subject: {row['subject_english']}\nBody: {row['body_english']}\nAnswer: {row['answer_english']}\"\"\"\n```\n\nFormats each ticket as structured text with clear sections.\n\n**Document Creation**:\n```python\nDocument(\n    page_content=create_ticket_text(row),  # The text content\n    metadata={                              # Extra information\n        'ticket_id': f\"ticket_{idx}\",\n        'type': str(row.get('type', 'unknown')),\n        'priority': str(row.get('priority', 'unknown'))\n    }\n)\n```\n\n**LangChain `Document` Class**:\n- `page_content`: The actual text (searchable)\n- `metadata`: Tags/attributes (filterable, returnable)\n\n#### Step 2: Build Vector Store\n\n**ChromaDB** is an embedded vector database:\n- ‚úÖ Fast similarity search\n- ‚úÖ Persistent storage (saves to disk)\n- ‚úÖ No separate server needed\n\n### ‚ö†Ô∏è Note About This Cell\n\n**SKIP IF ALREADY BUILT**: If you've run this before and have a persisted database, you can skip re-creating it. The next cell handles loading from disk.\n\n### üîß Technical Details\n\n**Embedding Process**:\n1. Take each document's `page_content`\n2. Pass through embedding model ‚Üí 384D vector\n3. Store vector + metadata in ChromaDB\n\n**Time**: ~30-60 seconds for 3,200 tickets\n\n**Storage**: ~50MB on disk"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SKIP IF ALREADY BUILT ####\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def create_ticket_text(row):\n",
    "    return f\"\"\"Subject: {row['subject_english']}\n",
    "Body: {row['body_english']}\n",
    "Answer: {row['answer_english']}\"\"\"\n",
    "\n",
    "# Convert to LangChain Documents\n",
    "print(\"Creating documents...\")\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=create_ticket_text(row),\n",
    "        metadata={\n",
    "            'ticket_id': f\"ticket_{idx}\",\n",
    "            'type': str(row.get('type', 'unknown')),\n",
    "            'priority': str(row.get('priority', 'unknown'))\n",
    "        }\n",
    "    )\n",
    "    for idx, row in train_df.iterrows()\n",
    "]\n",
    "print(f\"‚úÖ Created {len(documents):,} documents\")\n",
    "\n",
    "# Build vector store\n",
    "print(\"\\nBuilding vector store...\")\n",
    "print(\"üìù Using in-memory mode (fast, but rebuilds each run)\")\n",
    "\n",
    "# Simple in-memory approach - always works!\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"tickets_simple\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"Adding documents to vector store...\")\n",
    "vectorstore.add_documents(documents)\n",
    "print(f\"‚úÖ Vector store ready with {vectorstore._collection.count():,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def create_ticket_text(row):\n",
    "    return f\"\"\"Subject: {row['subject_english']}\n",
    "Body: {row['body_english']}\n",
    "Answer: {row['answer_english']}\"\"\"\n",
    "\n",
    "# Convert to LangChain Documents\n",
    "print(\"Creating documents...\")\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=create_ticket_text(row),\n",
    "        metadata={\n",
    "            'ticket_id': f\"ticket_{idx}\",\n",
    "            'type': str(row.get('type', 'unknown')),\n",
    "            'priority': str(row.get('priority', 'unknown'))\n",
    "        }\n",
    "    )\n",
    "    for idx, row in train_df.iterrows()\n",
    "]\n",
    "print(f\"‚úÖ Created {len(documents):,} documents\")\n",
    "\n",
    "# Build vector store\n",
    "print(\"\\nBuilding vector store...\")\n",
    "\n",
    "# Fix for ChromaDB readonly database error\n",
    "import shutil\n",
    "if os.path.exists(CONFIG['chroma_db_path']):\n",
    "    print(f\"‚ö†Ô∏è  Removing existing ChromaDB at {CONFIG['chroma_db_path']}\")\n",
    "    shutil.rmtree(CONFIG['chroma_db_path'])\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"tickets_simple\",\n",
    "    persist_directory=CONFIG['chroma_db_path']\n",
    ")\n",
    "print(f\"‚úÖ Vector store ready with {vectorstore._collection.count():,} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Custom Retriever with Re-ranking\n\n### üéØ Two-Stage Retrieval Strategy\n\nThis custom retriever implements a **sophisticated retrieval pipeline**:\n\n```\nQuery ‚Üí Vector Search (fast, ~20 docs) ‚Üí Re-rank (slow, accurate) ‚Üí Top 5 docs\n```\n\n### üìä Why Custom Retriever?\n\nLangChain's `BaseRetriever` class lets us create specialized retrievers that fit into LCEL chains seamlessly.\n\n### üîß Code Breakdown\n\n#### Class Definition\n```python\nclass SimpleRerankRetriever(BaseRetriever):\n```\nInherits from `BaseRetriever` ‚Üí compatible with all LangChain chain operations.\n\n#### Required Fields\n```python\nvectorstore: Chroma         # Where to search\nreranker: CrossEncoder      # How to re-rank\nk_initial: int = 20         # Stage 1: get 20 candidates\nk_final: int = 5            # Stage 2: return top 5\n```\n\n#### Core Method: `_get_relevant_documents(query)`\n\n**Stage 1: Vector Search**\n```python\ndocs = self.vectorstore.similarity_search(query, k=self.k_initial)\n```\n- Uses cosine similarity on embeddings\n- Fast: ~10-20ms for 3k documents\n- Gets more candidates than needed\n\n**Stage 2: Re-ranking**\n```python\npairs = [[query, doc.page_content] for doc in docs]\nscores = self.reranker.predict(pairs)\n```\n- Cross-encoder scores each query-document pair\n- Slower: ~100-200ms for 20 pairs\n- Much more accurate than vector similarity alone\n\n**Stage 3: Sort and Filter**\n```python\ndocs_sorted = sorted(docs, key=lambda x: x.metadata['rerank_score'], reverse=True)\nreturn docs_sorted[:self.k_final]\n```\n- Sorts by re-rank score (highest first)\n- Returns only top 5\n\n### üí° Key Insight: Trade-offs\n\n| Method | Speed | Accuracy | Scalability |\n|--------|-------|----------|-------------|\n| Vector search only | ‚ö° Fast | üòê OK | ‚úÖ Great |\n| Re-rank all docs | üêå Slow | ‚úÖ Great | ‚ùå Poor |\n| **Two-stage** | ‚úÖ Fast | ‚úÖ Great | ‚úÖ Great |\n\n### üß™ Performance Numbers\n\nFor 3,200 documents:\n- **Vector search** (k=20): ~15ms\n- **Re-ranking** (20 docs): ~150ms\n- **Total**: ~165ms per query\n\nCompare to re-ranking all 3,200 docs: ~24,000ms! ‚è±Ô∏è"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRerankRetriever(BaseRetriever):\n",
    "    \"\"\"Two-stage retrieval: vector search ‚Üí re-rank.\"\"\"\n",
    "    \n",
    "    vectorstore: Chroma\n",
    "    reranker: CrossEncoder\n",
    "    k_initial: int = 20\n",
    "    k_final: int = 5\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Stage 1: Get initial results\n",
    "        docs = self.vectorstore.similarity_search(query, k=self.k_initial)\n",
    "        \n",
    "        # Stage 2: Re-rank\n",
    "        pairs = [[query, doc.page_content] for doc in docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Add scores and sort\n",
    "        for doc, score in zip(docs, scores):\n",
    "            doc.metadata['rerank_score'] = float(score)\n",
    "        \n",
    "        docs_sorted = sorted(docs, key=lambda x: x.metadata['rerank_score'], reverse=True)\n",
    "        return docs_sorted[:self.k_final]\n",
    "\n",
    "# Create retriever\n",
    "retriever = SimpleRerankRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    reranker=reranker,\n",
    "    k_initial=CONFIG['top_k_initial'],\n",
    "    k_final=CONFIG['top_k_reranked']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Define Helper Functions\n\n### üîß Context Formatting Function\n\nThis simple but crucial function transforms retrieved documents into a formatted context string for the LLM.\n\n### üìù Code Analysis\n\n```python\ndef format_docs_to_context(docs: List[Document]) -> str:\n```\n\n**Input**: List of `Document` objects (from our retriever)\n\n**Output**: Single formatted string\n\n### üé® Formatting Strategy\n\nFor each document:\n1. Add a separator: `--- Ticket {i} ---`\n2. Include the re-rank score: `(score: 0.845)`\n3. Add the document content\n\n**Example Output**:\n```\n--- Ticket 1 (score: 0.892) ---\nSubject: Cannot access email\nBody: User reports login fails...\nAnswer: Reset password via admin panel...\n\n--- Ticket 2 (score: 0.756) ---\nSubject: VPN connection issues\nBody: Remote worker cannot connect...\nAnswer: Check firewall settings...\n```\n\n### üí° Why This Matters\n\n**Clear Structure**: LLM can easily distinguish between tickets\n\n**Score Information**: LLM knows which tickets are most relevant\n\n**Numbered Tickets**: LLM can cite specific tickets in answer\n\n### üîó LCEL Integration\n\nThis function will be wrapped in `RunnableLambda` to use in our chain:\n\n```python\nretriever | RunnableLambda(format_docs_to_context) | prompt | llm\n```\n\nSimple functions ‚Üí Powerful chains! üöÄ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs_to_context(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents into context string.\"\"\"\n",
    "    parts = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        score = doc.metadata.get('rerank_score', 0)\n",
    "        parts.append(f\"\\n--- Ticket {i} (score: {score:.3f}) ---\\n{doc.page_content}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "print(\"‚úÖ Helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Build LCEL Chain\n\n### üîó Introduction to LCEL (LangChain Expression Language)\n\n**This is the magic of LangChain!** LCEL lets you chain components together using the **pipe operator `|`**.\n\nThink of it like Unix pipes: `cat file.txt | grep \"error\" | wc -l`\n\n### üéØ Building Blocks\n\n#### 1. Define the Prompt Template\n\n```python\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an IT support assistant...\"),\n    (\"user\", \"HISTORICAL TICKETS:\\n{context}\\n\\nUSER QUESTION:\\n{question}\\n...\")\n])\n```\n\n**Prompt variables**: `{context}` and `{question}` will be filled by our chain.\n\n**Message types**:\n- `system`: Sets assistant behavior/role\n- `user`: The actual query\n\n### üîÑ Understanding the Chain\n\nLet's build it step by step:\n\n#### Simple Chain (Conceptual)\n```python\nretriever | format | {\"context\": ..., \"question\": ...}\n```\n\n#### Complete RAG Chain\n```python\nrag_chain = (\n    {\"context\": retriever | RunnableLambda(format_docs_to_context), \n     \"question\": lambda x: x}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n```\n\n### üìä Flow Visualization\n\n```\nInput Query\n    ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Step 1: Parallel Execution            ‚îÇ\n‚îÇ  ‚îú‚îÄ retriever ‚Üí format ‚Üí context      ‚îÇ\n‚îÇ  ‚îî‚îÄ lambda x: x ‚Üí question            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Step 2: Prompt Template               ‚îÇ\n‚îÇ  Combines {context} + {question}      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Step 3: LLM Generation                ‚îÇ\n‚îÇ  Sends prompt to LM Studio            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Step 4: Output Parsing                ‚îÇ\n‚îÇ  Extracts string from LLM response    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚Üì\nFinal Answer\n```\n\n### üí° Key LCEL Concepts\n\n#### 1. Dictionary Runnables\n```python\n{\"context\": retriever | format, \"question\": lambda x: x}\n```\nCreates a dict with computed values - runs both branches in parallel!\n\n#### 2. RunnableLambda\n```python\nRunnableLambda(format_docs_to_context)\n```\nWraps any function to work in LCEL chains.\n\n#### 3. Lambda Functions\n```python\nlambda x: x\n```\nPass-through function (returns input unchanged).\n\n#### 4. Output Parsers\n```python\nStrOutputParser()\n```\nExtracts the text string from LLM response object.\n\n### üöÄ Why LCEL is Powerful\n\n‚úÖ **Readable**: Flow is clear and linear\n\n‚úÖ **Composable**: Swap components easily\n\n‚úÖ **Streaming**: Built-in support for streaming responses\n\n‚úÖ **Parallel**: Automatic parallel execution where possible\n\n‚úÖ **Type-safe**: Type hints flow through chain"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an IT support assistant. Answer based on the historical tickets provided.\"),\n",
    "    (\"user\", \"\"\"HISTORICAL TICKETS:\n",
    "{context}\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "Provide a helpful answer based on the historical tickets above.\"\"\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "# Step 1: Retrieve ‚Üí Step 2: Format ‚Üí Step 3: Prompt ‚Üí Step 4: LLM ‚Üí Step 5: Parse\n",
    "chain = (\n",
    "    retriever                                    # Step 1: Get documents\n",
    "    | RunnableLambda(format_docs_to_context)     # Step 2: Format to context string\n",
    "    | RunnableLambda(lambda context: {           # Step 3: Prepare prompt inputs\n",
    "        \"context\": context,\n",
    "        \"question\": \"\"  # Will be filled during invoke\n",
    "    })\n",
    ")\n",
    "\n",
    "# Complete RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs_to_context), \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LCEL chain built!\")\n",
    "print(\"   Flow: Query ‚Üí Retrieve ‚Üí Format ‚Üí Prompt ‚Üí LLM ‚Üí Answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test the Chain\n\n### üß™ End-to-End RAG System Test\n\nNow we put everything together and see the RAG system in action!\n\n### üìù Test Setup\n\n#### 1. Select Test Ticket\n```python\ntest_ticket = test_df.iloc[555]\n```\nPicks ticket #555 from our **test set** (unseen by the vector database).\n\n#### 2. Format Query\n```python\nquery = f\"\"\"Subject: {test_ticket['subject_english']}\nBody: {test_ticket['body_english']}\"\"\"\n```\nCreates query in same format as training data (but **without** the answer!).\n\n### üîÑ What Happens When You Run This Cell\n\n#### Behind the Scenes:\n\n1. **Retrieval Stage**\n   - Query embedding: Convert query text ‚Üí 384D vector\n   - Vector search: Find 20 similar tickets (cosine similarity)\n   - Re-ranking: Score all 20 with cross-encoder\n   - Selection: Pick top 5 re-ranked results\n\n2. **Generation Stage**\n   - Format: Convert 5 documents ‚Üí context string\n   - Prompt: Combine context + query ‚Üí complete prompt\n   - LLM Call: Send to LM Studio\n   - Parse: Extract text from response\n\n#### Performance Expectations:\n- **Retrieval**: ~165ms (vector search + re-ranking)\n- **LLM Generation**: ~2-5 seconds (depends on model/hardware)\n- **Total**: ~2-6 seconds\n\n### üìä Evaluating Results\n\n**Good signs**:\n- ‚úÖ Answer addresses the specific question\n- ‚úÖ Uses information from retrieved tickets\n- ‚úÖ Formatted professionally\n- ‚úÖ Includes troubleshooting steps\n\n**Red flags**:\n- ‚ùå Generic answer (not using retrieval context)\n- ‚ùå Hallucinated information\n- ‚ùå Doesn't match the ticket type\n- ‚ùå Overly brief or unclear\n\n### üí° Interactive Experiment\n\nTry changing `test_df.iloc[555]` to different indices:\n- `test_df.iloc[0]` - First test ticket\n- `test_df.iloc[100]` - Another random ticket\n- `test_df.sample(1).iloc[0]` - Random ticket\n\nCompare:\n1. Original answer (in test set)\n2. RAG-generated answer\n3. Retrieved ticket answers\n\nThis helps you understand when RAG works well vs. needs improvement!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test ticket\n",
    "test_ticket = test_df.iloc[555]\n",
    "\n",
    "query = f\"\"\"Subject: {test_ticket['subject_english']}\n",
    "Body: {test_ticket['body_english']}\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST TICKET\")\n",
    "print(\"=\"*80)\n",
    "print(query)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run the chain\n",
    "print(\"\\nüîÑ Running LCEL chain...\\n\")\n",
    "answer = rag_chain.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATED ANSWER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display answer as Markdown for better formatting\n",
    "display(Markdown(answer))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## View Retrieved Documents\n\n### üîç Examining the Retrieval Quality\n\nThis cell lets us inspect **what the system actually retrieved** - crucial for debugging and understanding RAG performance!\n\n### üéØ Why This Matters\n\nThe quality of your RAG answer is **directly dependent** on retrieval quality:\n\n```\nBad Retrieval ‚Üí Bad Answer (even with great LLM)\nGood Retrieval ‚Üí Good Answer (even with mediocre LLM)\n```\n\n### üìä What to Look For\n\nWhen examining retrieved documents:\n\n#### ‚úÖ Good Retrieval Signs\n\n1. **Relevant Content**\n   - Documents address similar issues\n   - Terminology matches the query\n   - Solutions are applicable\n\n2. **High Re-rank Scores**\n   - Top document: >0.7 (strong match)\n   - Documents 2-5: >0.4 (relevant)\n   - Clear score separation (best doc significantly higher)\n\n3. **Diversity**\n   - Different tickets with same solution\n   - Multiple perspectives on same problem\n   - Various contexts (different users, setups)\n\n#### ‚ùå Poor Retrieval Signs\n\n1. **Irrelevant Content**\n   - Documents about different topics\n   - Wrong product/system\n   - Unrelated technical issues\n\n2. **Low Re-rank Scores**\n   - All scores <0.5 (weak matches)\n   - Scores very similar (no clear winner)\n   - Top score <0.3 (likely poor answer quality)\n\n3. **Duplicates**\n   - Same ticket multiple times\n   - Identical or near-identical solutions\n   - No information diversity\n\n### üî¨ Analysis Workflow\n\nFor each retrieved document:\n\n1. **Check relevance**: Does it actually relate to the query?\n2. **Evaluate score**: Is the re-rank score reasonable?\n3. **Assess quality**: Would this help answer the question?\n4. **Compare ranks**: Are better documents ranked higher?\n\n### üí° Debugging Tips\n\n**If retrieval is poor**:\n- Try different embedding models\n- Adjust `k_initial` and `k_final` values\n- Check if query format matches training data\n- Verify documents are properly indexed\n\n**If scores seem wrong**:\n- Re-ranker might need different model\n- Query might be too short/vague\n- Documents might need better formatting\n\n### üéì Learning Exercise\n\nCompare the retrieved documents to:\n1. The original test ticket answer\n2. The RAG-generated answer\n3. Other similar tickets in the test set\n\nThis helps you understand:\n- What information the LLM had available\n- Why it generated its specific answer\n- How retrieval quality affects output quality"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the documents that were retrieved\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP {len(docs)} RETRIEVED TICKETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    score = doc.metadata.get('rerank_score', 0)\n",
    "    print(f\"\\n--- Ticket {i} (rerank score: {score:.3f}) ---\")\n",
    "    print(doc.page_content[:300] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding LCEL\n",
    "\n",
    "### What We Just Built\n",
    "\n",
    "```python\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "### Breaking It Down\n",
    "\n",
    "1. **`retriever`** - Gets relevant documents from vector store\n",
    "2. **`| format_docs`** - Formats documents into context string\n",
    "3. **`| prompt`** - Creates the prompt with context and question\n",
    "4. **`| llm`** - Sends to LLM and gets response\n",
    "5. **`| StrOutputParser()`** - Extracts text from LLM response\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "**1. Readable**: Chain flow is clear and easy to understand\n",
    "\n",
    "**2. Modular**: Each component can be tested independently\n",
    "```python\n",
    "# Test just the retriever\n",
    "docs = retriever.invoke(\"test query\")\n",
    "```\n",
    "\n",
    "**3. Reusable**: Components can be used in different chains\n",
    "```python\n",
    "# Use same retriever in different chain\n",
    "another_chain = retriever | different_prompt | llm\n",
    "```\n",
    "\n",
    "**4. Composable**: Easy to add or remove steps\n",
    "```python\n",
    "# Add a translation step\n",
    "chain_with_translation = retriever | format | prompt | llm | translator\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Original vs LCEL\n",
    "\n",
    "### Original Approach (Procedural)\n",
    "```python\n",
    "def rag_pipeline(query):\n",
    "    docs = retrieve(query)\n",
    "    context = format(docs)\n",
    "    prompt_text = build_prompt(context, query)\n",
    "    answer = llm.generate(prompt_text)\n",
    "    return answer\n",
    "```\n",
    "\n",
    "### LCEL Approach (Declarative)\n",
    "```python\n",
    "rag_chain = retriever | format_docs | prompt | llm | parser\n",
    "answer = rag_chain.invoke(query)\n",
    "```\n",
    "\n",
    "### Why LCEL?\n",
    "\n",
    "| Feature | Original | LCEL |\n",
    "|---------|----------|------|\n",
    "| Code style | Procedural | Declarative |\n",
    "| Testing | Full pipeline | Per component |\n",
    "| Reusability | Copy-paste | Compose |\n",
    "| Streaming | Manual | Built-in |\n",
    "| Modification | Edit function | Swap components |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand basic LCEL chains, you can:\n",
    "\n",
    "1. **Add streaming**: Get answers word-by-word\n",
    "```python\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "2. **Add error handling**: Catch and handle errors gracefully\n",
    "```python\n",
    "chain_with_fallback = rag_chain.with_fallbacks([backup_chain])\n",
    "```\n",
    "\n",
    "3. **Add evaluation**: Chain an evaluation step\n",
    "```python\n",
    "full_chain = rag_chain | evaluation_chain\n",
    "```\n",
    "\n",
    "4. **Batch processing**: Process multiple queries\n",
    "```python\n",
    "answers = rag_chain.batch([query1, query2, query3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What We Learned**:\n",
    "1. LangChain components (embeddings, LLM, retriever, prompts)\n",
    "2. LCEL syntax: chaining with `|` operator\n",
    "3. Sequential data flow through components\n",
    "4. Benefits: modularity, testability, reusability\n",
    "\n",
    "**What We Built**:\n",
    "- Simple RAG chain: Retrieve ‚Üí Format ‚Üí Generate\n",
    "- Two-stage retrieval with re-ranking\n",
    "- Clean, composable architecture\n",
    "\n",
    "**This is the foundation** - from here we can build more complex chains with parallel execution, conditional logic, and advanced patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Homework Assignments\n",
    "\n",
    "Complete these exercises to deepen your understanding of LangChain and RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Add Streaming Output\n",
    "\n",
    "**Goal**: Modify the RAG chain to stream the LLM response word-by-word instead of waiting for the complete answer.\n",
    "\n",
    "**Tasks**:\n",
    "1. Research the `.stream()` method in LangChain\n",
    "2. Create a new cell that uses `rag_chain.stream(query)` instead of `rag_chain.invoke(query)`\n",
    "3. Print each chunk as it arrives with `end=\"\"` and `flush=True`\n",
    "4. Compare the user experience between streaming vs. non-streaming\n",
    "\n",
    "**Starter Code**:\n",
    "```python\n",
    "# TODO: Implement streaming\n",
    "query = \"How do I reset my password?\"\n",
    "\n",
    "print(\"Streaming answer: \", end=\"\", flush=True)\n",
    "# Your code here\n",
    "```\n",
    "\n",
    "**Bonus**: Add a timer to measure and compare response times for first token vs. complete response.\n",
    "\n",
    "**Hint**: replace THIS chunk of code with the stream() object:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "answer = rag_chain.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATED ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: Experiment with Retrieval Parameters (Intermediate)\n",
    "\n",
    "**Goal**: Understand how retrieval parameters affect answer quality by testing different configurations.\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a new retriever with different `k_initial` and `k_final` values\n",
    "2. Test at least 3 different configurations:\n",
    "   - Configuration A: k_initial=10, k_final=3\n",
    "   - Configuration B: k_initial=30, k_final=10\n",
    "   - Configuration C: k_initial=20, k_final=5 (baseline)\n",
    "3. Run the same query through each configuration\n",
    "4. Compare the retrieved documents and generated answers\n",
    "5. Analyze trade-offs between:\n",
    "   - Retrieval quality (are the right documents found?)\n",
    "   - Answer accuracy (is the LLM generating better answers?)\n",
    "   - Performance (speed and resource usage)\n",
    "\n",
    "**Starter Code**:\n",
    "```python\n",
    "# TODO: Create experimental configurations\n",
    "configs_to_test = [\n",
    "    {\"name\": \"Config A\", \"k_initial\": 10, \"k_final\": 3},\n",
    "    {\"name\": \"Config B\", \"k_initial\": 30, \"k_final\": 10},\n",
    "    {\"name\": \"Config C\", \"k_initial\": 20, \"k_final\": 5},\n",
    "]\n",
    "\n",
    "test_query = test_df.iloc[100]  # Pick a different test ticket\n",
    "\n",
    "for config in configs_to_test:\n",
    "    # Create retriever with config\n",
    "    # Run query\n",
    "    # Compare results\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Questions**:\n",
    "- Which configuration provided the best answer quality?\n",
    "- What's the performance impact of larger k_initial values?\n",
    "- Is there a point where more documents hurt answer quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: Build a Confidence Scoring Chain (Advanced)\n",
    "\n",
    "**Goal**: Extend the RAG chain to include a confidence score based on document relevance and answer quality.\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a confidence calculation function that:\n",
    "   - Analyzes rerank scores from retrieved documents\n",
    "   - Calculates metrics like: average score, score variance, top score\n",
    "   - Returns a confidence level: \"High\" (>0.7), \"Medium\" (0.4-0.7), \"Low\" (<0.4)\n",
    "2. Wrap the function in a `RunnableLambda`\n",
    "3. Add it to the chain using the pipe operator\n",
    "4. Test with multiple queries and verify confidence scores make sense\n",
    "\n",
    "**Starter Code**:\n",
    "```python\n",
    "def calculate_confidence(chain_output: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate confidence score from retrieval and answer.\n",
    "    \n",
    "    Args:\n",
    "        chain_output: Dict with 'answer', 'docs', and other chain data\n",
    "        \n",
    "    Returns:\n",
    "        Dict with original data plus 'confidence' and 'confidence_level'\n",
    "    \"\"\"\n",
    "    # TODO: Extract rerank scores from documents\n",
    "    # TODO: Calculate statistics (mean, max, variance)\n",
    "    # TODO: Determine confidence level\n",
    "    # TODO: Return enriched output\n",
    "    pass\n",
    "\n",
    "# TODO: Create enhanced chain\n",
    "enhanced_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs_to_context), \n",
    "     \"question\": lambda x: x,\n",
    "     \"docs\": retriever}  # Pass docs separately for confidence calculation\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(calculate_confidence)\n",
    ")\n",
    "\n",
    "# Test with various queries\n",
    "test_queries = [\n",
    "    \"How do I reset my password?\",  # Should have high confidence\n",
    "    \"What is quantum computing?\",     # Should have low confidence (off-topic)\n",
    "]\n",
    "```\n",
    "\n",
    "**Challenge**: Create a fallback mechanism that triggers when confidence is low, asking the user to rephrase or providing a disclaimer that the system is uncertain.\n",
    "\n",
    "**Bonus**: Log confidence scores and correlate them with actual answer quality by manual review."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}