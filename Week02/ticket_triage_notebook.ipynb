{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442c0e7d",
   "metadata": {},
   "source": [
    "# Ticket Triage Exploration Notebook\n",
    "\n",
    "The goal is to understand the data, and do a few experiments testing the use of LLMs for our eventual app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import pandas as pd # for dataframe handling, CSV reading, etc.\n",
    "import requests     # for forming HTTP requests\n",
    "import random       # random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebc4ca",
   "metadata": {},
   "source": [
    "## Set up helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e51eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Helper functions\n",
    "\"\"\"\n",
    "\n",
    "def show_error(err_string: str):\n",
    "    \"\"\"\n",
    "    Print an error message and stop execution\n",
    "    \"\"\"\n",
    "    print(f\"Error: {err_string}\")\n",
    "    SystemExit()\n",
    "\n",
    "\n",
    "def load_data(csv_path: str):\n",
    "    \"\"\"\n",
    "    Load support ticket data from a CSV file.\n",
    "    \n",
    "    This function reads a CSV file containing support tickets and returns it as a \n",
    "    pandas DataFrame (think of it as a table/spreadsheet in Python).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A table containing all the support tickets with columns like\n",
    "                     subject, body, priority, language, etc.\n",
    "        None: If the file can't be found or loaded\n",
    "    \n",
    "    Example CSV structure:\n",
    "        subject        | body            | \n",
    "        ---------------|-----------------|\n",
    "        Login issue    | Can't log in... | \n",
    "    \"\"\"\n",
    "    # Define the path to our CSV file (relative to where the script is run)\n",
    "    \n",
    "    try:\n",
    "        # Try to read the CSV file into a DataFrame (table)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        # If the file doesn't exist, show an error message to the user\n",
    "        show_error(f\"CSV file not found at {csv_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # If any other error occurs, show the error details\n",
    "        show_error(f\"Error loading CSV: {str(e)}\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959dc833",
   "metadata": {},
   "source": [
    "# Two ways to call the LLM - requests/http vs. SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call LM Studio LLM\n",
    "def call_lm_studio_requests(system_content, user_content):\n",
    "    \"\"\"\n",
    "    Send a request to the LM Studio AI server for text analysis/generation.\n",
    "    \n",
    "    LM Studio is a local AI server that runs language models on your computer.\n",
    "    This function sends prompts to it and gets AI-generated responses back.\n",
    "    \n",
    "    Args:\n",
    "        system_content (str): Instructions for how the AI should behave\n",
    "                             Example: \"You are a helpful assistant analyzing support tickets.\"\n",
    "        user_content (str): The actual content/question for the AI to process\n",
    "                           Example: \"Translate this text: Bonjour\"\n",
    "    \n",
    "    Returns:\n",
    "        str: The AI's response text, or an error message if something goes wrong\n",
    "    \n",
    "    How it works:\n",
    "        1. Prepares a request with the AI model name and messages\n",
    "        2. Sends the request to LM Studio running on localhost:1234\n",
    "        3. Extracts and returns the AI's response from the JSON reply\n",
    "    \n",
    "    Note: LM Studio must be running locally on port 1234 for this to work\n",
    "    Change this to whatever LLM provider you decided to use (see slides from class week 1 for ideas)\n",
    "    \"\"\"\n",
    "    # URL where LM Studio's API is listening (localhost = your computer)\n",
    "    url = \"http://localhost:1234/v1/chat/completions\"\n",
    "    \n",
    "    # Prepare the data to send to the AI\n",
    "    payload = {\n",
    "        \"model\": \"openai/gpt-oss-20b\",  # The AI model to use\n",
    "        \"messages\": [\n",
    "            # System message: Sets the AI's behavior/role\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            # User message: The actual task/question\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        \"max_tokens\": 1200,    # Maximum length of the response\n",
    "        \"temperature\": 0.3     # Controls randomness (0=deterministic, 1=creative)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the request to LM Studio and get the response\n",
    "        resp = requests.post(url, json=payload, headers={\"Content-Type\": \"application/json\"})\n",
    "        \n",
    "        # Extract the AI's message from the JSON response\n",
    "        # The response structure is: {\"choices\": [{\"message\": {\"content\": \"AI response here\"}}]}\n",
    "        return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        # If anything goes wrong (LM Studio not running, network error, etc.)\n",
    "        show_error(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44737c2",
   "metadata": {},
   "source": [
    "# When the SDK is nicer\n",
    "\n",
    "- Less boilerplate. One call returns parsed objects; streaming is trivial.\n",
    "\n",
    "- Built-in retries/backoff & connection pooling. Fewer flaky calls on busy GPUs.\n",
    "\n",
    "- Async support. AsyncOpenAI saves you from wiring aiohttp/httpx yourself.\n",
    "\n",
    "- Uniform interface across providers. Just swap the base_url to move between LM Studio, OpenAI, Groq, vLLM, etc.\n",
    "\n",
    "- Newer features surface sooner. (e.g., tool/function calling, JSON schema responses) without hand-crafting payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bd029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 of llm provider call using SDK rather than requests:\n",
    "\n",
    "# remenmber to pip install openai>=1.35 if you want to use the SDK version\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()  # reads .env in the current working directory to get the groq api key\n",
    "\n",
    "\n",
    "# Point the SDK at LLM provider\n",
    "\n",
    "# If we're using LM Studio locally:\n",
    "#_client = OpenAI(\n",
    "#    base_url=\"http://localhost:1234/v1\",\n",
    "#    api_key=\"lm-studio\"  # any non-empty string works for LM Studio\n",
    "#)\n",
    "\n",
    "# or use this in to call a model on groq:\n",
    "\n",
    "\n",
    "_client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],) # must have a GROQ_API_KEY in the .env file\n",
    "\n",
    "def call_llm_sdk(      system_content: str,                   \n",
    "                       user_content: str,                   \n",
    "                       model: str = \"openai/gpt-oss-20b\",\n",
    "                       max_tokens: int = 2000,\n",
    "                       temperature: float = 0.1,\n",
    "                       reasoning_effort: str=\"low\",\n",
    "                       ) -> str:\n",
    "    \"\"\"\n",
    "    Call LLM Provider via the OpenAI SDK (function call) instead of manual HTTP.\n",
    "\n",
    "    Args:\n",
    "        system_content: System prompt/instructions.\n",
    "        user_content: User message.\n",
    "        model: Model name exposed by provider - defaults to 'openai/gpt-oss-20b', make sure the provider actually has this model or you'll get an error.\n",
    "        max_tokens: Max tokens to generate - defaults to 1200 tokens; generation will stop when it hits this limit, so be sure to specify sufficent size to allow a response.\n",
    "        temperature: Sampling temperature - defaults to 0.1 which is meant to be more 'deterministic' and less 'creative'.\n",
    "\n",
    "    Returns:\n",
    "        The assistant's reply text. Stops on errors. More sophisticated error handling would be smart!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = _client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        # keep your existing helper if you have it; otherwise raise\n",
    "        try:\n",
    "            show_error(f\"Error calling LLM: {e}\")\n",
    "        except NameError:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12606e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load the csv file into a pandas data frame\n",
    "\"\"\"\n",
    "\n",
    "csv_file = \"IT_Tickets/dataset-tickets-multi-lang_cleaned.csv\"\n",
    "df = load_data(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd9aa0",
   "metadata": {},
   "source": [
    "# Now for calling the LLMs\n",
    "\n",
    "We have our helper functions set up, we've sort of connected to the LLMs, now it's time to put them to use.\n",
    "\n",
    "Our first attempt will be to simply determine the language of the the Email.  Let's grab a random email from the data frame and display it; then we can use an LLM \n",
    "with a good prompt to get the language it is written in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61342f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random ticket\n",
    "ticket_number = random.randint(0, len(df))\n",
    "print(df.iloc[ticket_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be58f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DETECT LANGUAGE ==========\n",
    "# Prepare prompts for language detection\n",
    "system_prompt = \"You are a helpful assistant analyzing support tickets.\"\n",
    "\n",
    "# Create a detailed prompt asking the AI to detect the language\n",
    "# The triple quotes (###) help the AI understand boundaries\n",
    "user_prompt = \\\n",
    "f\"\"\"Analyze the following support request email and return ONLY the name of the language it is written in, \n",
    "return one of the following languages:\n",
    "German; English; French; Portuguese; Spanish; Unknown;\n",
    "\n",
    "###\n",
    "Subject: {df.iloc[ticket_number]['subject']}\n",
    "\n",
    "Body: {df.iloc[ticket_number]['body']}\n",
    "###\n",
    "\n",
    "Your response should be simply one of [German, English, French, Portuguese, Spanish or Unknown], with no additional commentary or charachters; \n",
    "If there is more than one language present, choose the predominent one.\n",
    "\"\"\"\n",
    "\n",
    "# Call the AI to detect language\n",
    "# ai_response = call_lm_studio_requests(system_prompt, user_prompt)\n",
    "\n",
    "ai_response = call_llm_sdk(system_prompt, user_prompt, model=\"openai/gpt-oss-20b\", max_tokens=1200, temperature=0.1)\n",
    "\n",
    "# Clean up the response and extract just the language code\n",
    "\n",
    "language_code = ai_response.strip().lower() if ai_response else \"NO RESPONSE\"\n",
    "\n",
    "print(user_prompt)\n",
    "print(\"=\"*50)\n",
    "print(f\"Detected: {language_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ticket number from the previous cell so we can work through the ticket fields\n",
    "# we will use language_code from the prior cell in this prompt to help the LLM do a nice translation.\n",
    "\n",
    "# ========== Translate the Subject ==========\n",
    "# Prepare prompts for Subject translation\n",
    "system_prompt = \"You are a helpful assistant whose assigned the job of translating support tickets from the original language to English.\"\n",
    "\n",
    "# Create a detailed prompt asking the AI to detect the language\n",
    "# The triple quotes (###) help the AI understand boundaries\n",
    "user_prompt = f\"\"\"Translate the following email subject line from {language_code} to English, while adapting it to American idioms and phrasing - \n",
    "                    your translation should faithfully match the meaning of the original: \n",
    "\n",
    "                ###\n",
    "\n",
    "                {df.iloc[ticket_number]['subject']}\n",
    "\n",
    "                ###\n",
    "\n",
    "                your response should be simply be the English translation with no other information.\"\"\"\n",
    "                    \n",
    " \n",
    "# Call the AI to detect language\n",
    "ai_response = call_llm_sdk(system_prompt, user_prompt, model=\"openai/gpt-oss-20b\", reasoning_effort='medium')\n",
    "\n",
    "# Clean up the response and extract just the language code\n",
    "subject = ai_response.strip() if ai_response else \"\"\n",
    "\n",
    "print(user_prompt)\n",
    "print(\"=\"*50)\n",
    "print(f\"Subject Line: {subject}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ticket number from the previous cell so we can work through the ticket fields\n",
    "# we will use language_code from the prior cell in this prompt to help the LLM do a nice translation.\n",
    "\n",
    "# ========== Translate the Email Body ==========\n",
    "# Prepare prompts for Email Body translation\n",
    "system_prompt = \"You are a helpful assistant whose assigned the job of translating support tickets from the original language to English.\"\n",
    "\n",
    "# Create a detailed prompt asking the AI to detect the language\n",
    "# The triple quotes (###) help the AI understand boundaries\n",
    "user_prompt = \\\n",
    "f\"\"\"Translate the following email body from {language_code} to English, while adapting it to American idioms and phrasing - \n",
    "your translation should faithfully match the meaning of the original: \n",
    "\n",
    "###\n",
    "{df.iloc[ticket_number]['body']}\n",
    "###\n",
    "\n",
    "Your response should be simply be the English translation with no other information.\n",
    "\"\"\"\n",
    "                    \n",
    "# Call the AI to detect language\n",
    "ai_response = call_llm_sdk(system_prompt, user_prompt, model=\"openai/gpt-oss-20b\", reasoning_effort='medium')\n",
    "\n",
    "# Clean up the response and extract just the language code\n",
    "body = ai_response.strip() if ai_response else \"\"\n",
    "\n",
    "print(user_prompt)\n",
    "print(\"=\"*50)\n",
    "print(f\"Email Body: {body}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef0c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOMEWORK 1\n",
    "# Add a cell to classify tickets into 4 Types: Incident, Request, Change, Problem - you'll have to craft a prompt that explains to the LLM what thse mean (or maybe not - you can try just asking first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 2\n",
    "# Add a cell to determine what queue the ticket should be routed to: Billing & Payments, Customer Service, General Inquiry, Human Resources, \n",
    "# IT Support, Product Support, Returns & Exchanges, Sales and Pre-Sales, Service Outages, Technical Support\n",
    "# Remember the concept of one-shot/multi-shot: You could craft a long prompt that gives an example of an email and assignment for each of these categories... \n",
    "# Examples are a GREAT way to show an LLM what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 3\n",
    "# Add a cell to determine the priority of the ticket: P1, P2, P3 – read the rules for the priorities and use those rules in your \n",
    "# prompt to have the LLM assign the correct priority.\n",
    "# Again, you can explain the priority levels to the LLM and/or you can provide examples in the prompt. \n",
    "# I sketched in the prioritization rules below from the slides\n",
    "\n",
    "\n",
    "\"\"\"Rules for Priority Assignment\n",
    "\n",
    "P1 - Critical\n",
    "Security or privacy incident: data breach, ransomware, malware outbreak, phishing success, compromised account, stolen device with sensitive data.\n",
    "Payment/revenue blocking: checkout/payment API down, failed customer transactions at scale.\n",
    "Company-wide or regional outage: SSO, VPN, email, network, authentication unavailable.\n",
    "Safety/legal risk: regulatory or compliance breach, urgent legal exposure.\n",
    "High impact blocking: department/region/companywide/external customers cannot work, and urgency is blocking or deadline today.\n",
    "\n",
    "P2 - Major\n",
    "Medium impact blocking or deadline: a team is blocked, or a single user is blocked with a hard deadline.\n",
    "Degraded shared service: email delays, slow VPN, partial outage affecting multiple users.\n",
    "Single user blocking: cannot log in, device will not boot, locked account (no explicit deadline).\n",
    "\n",
    "P3 - Minor\n",
    "Informational requests: “how do I…”, “please provide access,” feature requests.\n",
    "Cosmetic issues, minor bugs, or general inquiries.\n",
    "Non-blocking tickets with no deadline.\n",
    "\n",
    "Tie-Breakers\n",
    "If multiple rules match, select the highest priority (P1 > P2 > P3).\n",
    "If signals conflict, assign the lower (safer) priority and reduce confidence.\n",
    "\n",
    "###\n",
    "Classify the following support request:\n",
    "\n",
    "Subject: {subject}\n",
    "Body: {body}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 4\n",
    "# Add a ”final output” that wraps all of the fields (translated subject, translated body, incident type, queue and priority) \n",
    "# and readies it to send on to the appropriate queue... \n",
    "# If your primary language is not English, try having the LLM translate this final bundle into your language.  \n",
    "# How was the quality of the translation?  \n",
    "# If the model isn’t doing a great job, search for a model that’s more tuned for your language (it would also have to know the source languages of course).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 5\n",
    "# Success criteria:  \n",
    "# How long does it take YOU to read a ticket and assign these items?  How long does it take the LLM?  \n",
    "# What’s the speedup?  \n",
    "# How is the accuracy? \n",
    "# Can you find an example where you feel the LLM did a poor job of assigning an Incident type, Queue or Priority?  \n",
    "# Can you fix it by adjusting your prompt?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
