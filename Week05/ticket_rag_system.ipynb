{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket RAG System - Week 05\n",
    "\n",
    "##  Building on Previous Work\n",
    "\n",
    "### Week 02 Recap: Ticket Triage\n",
    "In Week 02, we built a ticket triage system that:\n",
    "- **Detected language** of support tickets (German, English, French, Portuguese, Spanish)\n",
    "- **Translated** subject and body to English using LLM\n",
    "- **Classified tickets** into type, queue, and priority categories\n",
    "- Used **individual LLM calls** for each task (language detection → translation → classification)\n",
    "\n",
    "**Key Learning**: We can process tickets one-at-a-time, but each ticket is handled independently.\n",
    "\n",
    "### Week 03 Recap: Optimization & Consolidation\n",
    "In Week 03, we improved our approach:\n",
    "- **Consolidated LLM calls**: Combined 6 separate calls into 1 comprehensive call\n",
    "- **JSON structured output**: Added schema validation for consistent responses\n",
    "- **Token tracking**: Measured cost/efficiency of LLM operations\n",
    "- **Batch processing**: Handled multiple tickets efficiently\n",
    "\n",
    "**Key Learning**: Efficiency matters - consolidate operations and track resources.\n",
    "\n",
    "### Week 04 Recap: RAG with PDFs\n",
    "In Week 04, we learned Retrieval-Augmented Generation (RAG):\n",
    "- **PDF → Chunks**: Split documents into manageable pieces\n",
    "- **Embeddings**: Convert text to vector representations\n",
    "- **ChromaDB**: Store vectors for fast similarity search\n",
    "- **LLM + Context**: Generate answers using retrieved relevant chunks\n",
    "\n",
    "**Pattern**: `Document → Embed → Store → Query → Retrieve Similar → LLM Answer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## This Week: RAG for Ticket Support\n",
    "\n",
    "### The Challenge\n",
    "We can triage individual tickets (Week 02/03), but can we leverage **historical tickets** to provide better answers?\n",
    "\n",
    "**Questions to answer:**\n",
    "- How do we find similar past tickets?\n",
    "- Can historical solutions help with new problems?\n",
    "- How do we measure answer quality?\n",
    "\n",
    "### The Solution: Apply RAG to Tickets\n",
    "Combine all previous concepts:\n",
    "- Use **pre-translated tickets** from Week 02/03 preprocessing (CSV with `_english` columns)\n",
    "- Apply **RAG pattern from Week 04** (but with structured CSV instead of PDF)\n",
    "- **Search** for similar historical tickets using vector similarity\n",
    "- **Generate** context-aware answers using LLM with relevant examples\n",
    "\n",
    "### New This Week\n",
    "1. **Train/Test Split**: Proper evaluation methodology (80/20)\n",
    "2. **Confidence Scoring**: How certain is the system about its answer?\n",
    "3. **LLM Evaluation**: Let LLM judge answer quality vs ground truth\n",
    "4. **Batch Testing**: Systematic quality assessment\n",
    "\n",
    "## Architecture Overview\n",
    "```\n",
    "Pre-translated CSV (from Week 02/03 work)\n",
    "    ↓\n",
    "Extract English columns (subject_english, body_english, answer_english)\n",
    "    ↓\n",
    "Train/Test Split (80/20)\n",
    "    ↓\n",
    "Generate Embeddings (Week 04 pattern)\n",
    "    ↓\n",
    "Store in ChromaDB (Week 04 pattern)\n",
    "    ↓\n",
    "New Ticket → Embed → Vector Search → Retrieve Top-K Similar Tickets\n",
    "    ↓\n",
    "Context + Question → LM Studio LLM → Answer + Confidence + References\n",
    "```\n",
    "\n",
    "## CSV Data Structure\n",
    "\n",
    "**Original columns** (preserved from Week 02 work):\n",
    "- `subject`, `body`, `answer` (in original languages: es/fr/de/pt/en)\n",
    "\n",
    "**Translation columns** (created by `translate_tickets.ipynb`):\n",
    "- `subject_english` - English translation of subject\n",
    "- `body_english` - English translation of body\n",
    "- `answer_english` - English translation of answer\n",
    "- `original_language` - Detected language code (en/es/fr/de/pt)\n",
    "\n",
    "**Why separate columns?**\n",
    "- Original content preserved for reference\n",
    "- Fast loading (no translation overhead)\n",
    "- Consistent translations across experiments\n",
    "- Separation of concerns (translate once, use many times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports\n",
    "\n",
    "### Why Centralized Configuration?\n",
    "\n",
    "**Evolution of our approach:**\n",
    "- **Week 02**: Hardcoded values like `max_tokens=1200` directly in functions\n",
    "- **Week 03**: Started grouping related values (model selection at top)\n",
    "- **Week 04**: Configuration-first approach for PDF processing\n",
    "- **Week 05**: All parameters in one `CONFIG` dictionary\n",
    "\n",
    "**Benefits of CONFIG dictionary:**\n",
    "-  **Easy Experimentation**: Change `top_k` once instead of in multiple functions\n",
    "-  **Track Changes**: See all parameters at a glance\n",
    "-  **Reproducibility**: Same config = same results\n",
    "\n",
    "**Common Parameters You'll Adjust:**\n",
    "- `top_k`: Number of similar tickets to retrieve (try 3, 5, 10)\n",
    "- `temperature`: LLM creativity (0.1 = factual, 0.9 = creative)\n",
    "- `embedding_model`: Trade speed vs quality\n",
    "- `train_test_split`: More test data = better evaluation\n",
    "- `test_mode`: Set to `False` to process all ~4K tickets (currently limited to 100 for speed)\n",
    "\n",
    "**Test Mode Note**: The system is currently in TEST MODE, loading only the first 100 tickets for quick experimentation. To use the full dataset:\n",
    "1. Find the line: `'test_mode': True` \n",
    "2. Change to: `'test_mode': False`\n",
    "3. Re-run the configuration and data loading cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install chromadb sentence-transformers pandas requests python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n",
      "✅ Train/Test Split: 80%/20%\n",
      "✅ Top-K Retrieval: 5\n",
      "✅ Embedding Model: all-MiniLM-L6-v2\n",
      "✅ Using English translation columns: subject_english, body_english, answer_english\n",
      "✅ RAG Mode: STRICT (Context-only (traceable))\n",
      "✅ CSV: ./dataset-tickets-multi-lang3-4k-translated-all.csv\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# CONFIGURATION - All parameters in one place\n",
    "# =====================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # File paths\n",
    "    'csv_path': './dataset-tickets-multi-lang3-4k-translated-all.csv',  # Use pre-translated CSV\n",
    "    'chroma_db_path': './chroma_ticket_db',\n",
    "    \n",
    "    # Data split\n",
    "    'train_test_split': 0.8,  # 80% train, 20% test\n",
    "    'random_seed': 42,\n",
    "    \n",
    "    # TEST MODE - Limit data loading for quick testing\n",
    "    'test_mode': False,  # Set to False to process all tickets\n",
    "    'test_limit': 100,  # Only load first N tickets in test mode\n",
    "    \n",
    "    # Embedding configuration\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'embedding_fields': ['subject_english', 'body_english', 'answer_english'],  # Use English translation columns\n",
    "    \n",
    "    # Metadata fields\n",
    "    'metadata_fields': ['type', 'queue', 'priority', 'business_type', 'original_language'],\n",
    "    \n",
    "    # Vector search configuration\n",
    "    'top_k': 5,  # Number of similar tickets to retrieve\n",
    "    'similarity_threshold': 0.3,  # Minimum similarity score (cosine distance)\n",
    "    \n",
    "    # RAG Mode Configuration\n",
    "    # 'strict': LLM uses ONLY historical tickets (context-only, traceable)\n",
    "    # 'augmented': LLM uses historical tickets + general knowledge (helpful, flexible)\n",
    "    'rag_mode': 'strict',  \n",
    "    \n",
    "    # LLM configuration\n",
    "    'lm_studio_url': 'http://169.254.233.17:1234',\n",
    "    'llm_model': 'gpt-oss-20b',\n",
    "    'temperature': 0.2,\n",
    "    'max_tokens': 2000,\n",
    "    \n",
    "    # ChromaDB collection\n",
    "    'collection_name': 'ticket_rag_collection',\n",
    "    \n",
    "    # Future extensibility\n",
    "    'enable_reranking': False,  # Placeholder for future lessons\n",
    "    'reranking_model': None,\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"✅ Train/Test Split: {CONFIG['train_test_split']*100:.0f}%/{(1-CONFIG['train_test_split'])*100:.0f}%\")\n",
    "print(f\"✅ Top-K Retrieval: {CONFIG['top_k']}\")\n",
    "print(f\"✅ Embedding Model: {CONFIG['embedding_model']}\")\n",
    "print(f\"✅ Using English translation columns: {', '.join(CONFIG['embedding_fields'])}\")\n",
    "\n",
    "# Display RAG mode with explanation\n",
    "rag_mode = CONFIG['rag_mode']\n",
    "rag_mode_desc = 'Context-only (traceable)' if rag_mode == 'strict' else 'Context + LLM knowledge (flexible)'\n",
    "print(f\"✅ RAG Mode: {rag_mode.upper()} ({rag_mode_desc})\")\n",
    "\n",
    "if CONFIG['test_mode']:\n",
    "    print(f\"⚠️  TEST MODE: Will load only first {CONFIG['test_limit']} tickets\")\n",
    "print(f\"✅ CSV: {CONFIG['csv_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components \n",
    "\n",
    "Just like Week 04, we initialize our core components:\n",
    "- **Embedding Model**: Converts text to vectors (sentence-transformers)\n",
    "- **ChromaDB**: Vector database for similarity search\n",
    "- **LM Studio Client**: OpenAI-compatible API for local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading embedding model...\n",
      "✅ Embedding model loaded: all-MiniLM-L6-v2\n",
      "   Embedding dimensions: 384\n",
      "\n",
      "✅ Initializing ChromaDB...\n",
      "✅ ChromaDB initialized at: ./chroma_ticket_db\n",
      "\n",
      "✅ Connecting to LM Studio...\n",
      "✅ LM Studio client initialized\n",
      "   URL: http://169.254.233.17:1234\n",
      "   Model: gpt-oss-20b\n",
      "\n",
      "✅ All components initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# INITIALIZE COMPONENTS\n",
    "# =====================================================\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"✅ Loading embedding model...\")\n",
    "embedder = SentenceTransformer(CONFIG['embedding_model'])\n",
    "print(f\"✅ Embedding model loaded: {CONFIG['embedding_model']}\")\n",
    "print(f\"   Embedding dimensions: {len(embedder.encode(['test'])[0])}\")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "print(\"\\n✅ Initializing ChromaDB...\")\n",
    "chroma_client = chromadb.PersistentClient(path=CONFIG['chroma_db_path'])\n",
    "print(f\"✅ ChromaDB initialized at: {CONFIG['chroma_db_path']}\")\n",
    "\n",
    "# Initialize LM Studio client\n",
    "print(\"\\n✅ Connecting to LM Studio...\")\n",
    "client = OpenAI(\n",
    "    base_url=f\"{CONFIG['lm_studio_url']}/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "print(f\"✅ LM Studio client initialized\")\n",
    "print(f\"   URL: {CONFIG['lm_studio_url']}\")\n",
    "print(f\"   Model: {CONFIG['llm_model']}\")\n",
    "\n",
    "print(\"\\n✅ All components initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions \n",
    "Following the pattern from Week 02 and Week 03, we define reusable helper functions at the top of our notebook. These make our code more modular and easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions loaded (Week 02/03 pattern restored)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# HELPER FUNCTIONS (Matches Week 02/03 Pattern)\n",
    "# =====================================================\n",
    "\n",
    "def show_error(err_string: str):\n",
    "    \"\"\"\n",
    "    Print an error message and stop execution.\n",
    "    (Consistent with Week 02/03 pattern)\n",
    "    \n",
    "    Args:\n",
    "        err_string: Error message to display\n",
    "    \"\"\"\n",
    "    print(f\"❌ Error: {err_string}\")\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "def load_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ticket data from CSV.\n",
    "    Enhanced version of Week 02 function - now handles translated columns.\n",
    "    \n",
    "    In Week 02, we loaded `dataset-tickets-multi-lang_cleaned.csv` from IT_Tickets/.\n",
    "    Now we load pre-translated data with separate English columns.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file with translations\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Ticket data with both original and *_english columns\n",
    "        None: If file can't be found or loaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Verify required English translation columns exist\n",
    "        required_cols = ['subject_english', 'body_english', 'answer_english']\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        \n",
    "        if missing:\n",
    "            show_error(\n",
    "                f\"Missing translation columns: {missing}.\\n\"\n",
    "                f\"Please run translate_tickets.ipynb first to generate English translations!\"\n",
    "            )\n",
    "        \n",
    "        print(f\"✅ Loaded {len(df):,} tickets from CSV\")\n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        show_error(f\"CSV file not found at {csv_path}\")\n",
    "    except Exception as e:\n",
    "        show_error(f\"Error loading CSV: {str(e)}\")\n",
    "\n",
    "\n",
    "def call_llm_sdk(\n",
    "    system_content: str,\n",
    "    user_content: str,\n",
    "    model: str = \"gpt-oss-20b\",\n",
    "    max_tokens: int = 2000,\n",
    "    temperature: float = 0.1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call LLM via OpenAI SDK.\n",
    "    Matches Week 03 signature for consistency.\n",
    "    \n",
    "    This is a wrapper around the OpenAI client that we initialized earlier.\n",
    "    Same interface as Week 03, but now used for RAG answer generation.\n",
    "    \n",
    "    Args:\n",
    "        system_content: System prompt/instructions\n",
    "        user_content: User message/question\n",
    "        model: Model name (default from CONFIG)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0=deterministic, 1=creative)\n",
    "    \n",
    "    Returns:\n",
    "        str: LLM response text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            timeout=60\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"LLM call failed: {str(e)}\")\n",
    "\n",
    "\n",
    "print(\"✅ Helper functions loaded (Week 02/03 pattern restored)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Our Data \n",
    "\n",
    "### Why Pre-Translated Data?\n",
    "\n",
    "Remember in Week 02/03, we translated tickets one-at-a-time using LLM calls. For this RAG system, we need to process **thousands** of tickets efficiently.\n",
    "\n",
    "**Solution**: Pre-translate once, reuse many times\n",
    "- Run `translate_tickets.py` script (one-time, ~1-6 hours for 4K tickets depending on model and provider - took 75 min using gpt-oss-20b on GROQ - cost  $1.24  - on local Mac - it took over 4 hours) \n",
    "- Creates CSV with separate `_english` columns\n",
    "- This notebook uses the pre-translated data\n",
    "\n",
    "### Why Train/Test Split?\n",
    "\n",
    "In Week 02/03, we processed tickets individually without evaluation. Now we need to **test** our RAG system properly:\n",
    "\n",
    "- **Training Set (80%)**: Build our knowledge base from these tickets (technically not \"training\"!)\n",
    "- **Test Set (20%)**: Evaluate how well we answer *new, unseen* tickets\n",
    "\n",
    "This is standard ML practice for unbiased evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LM Studio is connected and ready\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test LM Studio connection\n",
    "def test_lm_studio():\n",
    "    \"\"\"Test connection to LM Studio\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=CONFIG['llm_model'],\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'connected'\"}],\n",
    "            temperature=CONFIG['temperature'],\n",
    "            max_tokens=10,\n",
    "            timeout=30\n",
    "        )\n",
    "        print(\"✅ LM Studio is connected and ready\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LM Studio connection failed: {e}\")\n",
    "        print(\"Please ensure LM Studio is running with a model loaded on port 1234\")\n",
    "        return False\n",
    "\n",
    "test_lm_studio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding & Vector DB Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading CSV from: ./dataset-tickets-multi-lang3-4k-translated-all.csv\n",
      " Total tickets: 3,998\n",
      " After cleaning: 3,601 (removed 397 with missing/empty English translations)\n",
      "\n",
      " Split complete:\n",
      "    Train set: 2,880 tickets (80%)\n",
      "    Test set: 721 tickets (20%)\n",
      "\n",
      " Sample train ticket:\n",
      "\n",
      "Original (multilingual):\n",
      "                                         subject      type priority  \\\n",
      "0                                            NaN   Request      low   \n",
      "1  Probleme mit Outlook bei Microsoft Office 365  Incident     high   \n",
      "2             Touchscreen Issue on Surface Pro 7   Problem     high   \n",
      "\n",
      "  original_language  \n",
      "0                es  \n",
      "1                de  \n",
      "2                en  \n",
      "\n",
      "English translations:\n",
      "                                     subject_english      type priority\n",
      "0  Dear Support, I’m requesting a return for my H...   Request      low\n",
      "1           Outlook Issues with Microsoft Office 365  Incident     high\n",
      "2                 Touchscreen Issue on Surface Pro 7   Problem     high\n"
     ]
    }
   ],
   "source": [
    "def load_and_split_data(csv_path: str, train_ratio: float = 0.8, random_seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load CSV and split into train/test sets.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file\n",
    "        train_ratio: Ratio for training set (default 0.8 = 80%)\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_df, test_df)\n",
    "    \"\"\"\n",
    "    print(f\" Loading CSV from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # TEST MODE - Limit data for quick testing\n",
    "    if CONFIG.get('test_mode', False):\n",
    "        test_limit = CONFIG.get('test_limit', 10)\n",
    "        print(f\"⚠️  TEST MODE: Loading only first {test_limit} tickets\")\n",
    "        df = df.head(test_limit)\n",
    "    \n",
    "    # Check if English translation columns exist\n",
    "    required_cols = ['subject_english', 'body_english', 'answer_english']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n  Missing translation columns: {', '.join(missing_cols)}\")\n",
    "        print(f\"   Please run translate_tickets.ipynb first to generate English translations!\")\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Clean data: drop rows with missing English translations\n",
    "    initial_count = len(df)\n",
    "    df = df.dropna(subset=['subject_english', 'body_english', 'answer_english'])\n",
    "    \n",
    "    # Also filter out empty strings\n",
    "    df = df[(df['subject_english'].str.strip() != '') & \n",
    "            (df['body_english'].str.strip() != '') & \n",
    "            (df['answer_english'].str.strip() != '')]\n",
    "    \n",
    "    cleaned_count = len(df)\n",
    "    \n",
    "    print(f\" Total tickets: {initial_count:,}\")\n",
    "    print(f\" After cleaning: {cleaned_count:,} (removed {initial_count - cleaned_count:,} with missing/empty English translations)\")\n",
    "    \n",
    "    # Check if we have any data\n",
    "    if cleaned_count == 0:\n",
    "        print(f\"\\n❌ No tickets with English translations found!\")\n",
    "        print(f\"   Run translate_tickets.ipynb to generate translations first.\")\n",
    "        raise ValueError(\"No valid English translations in CSV\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    split_idx = int(len(df_shuffled) * train_ratio)\n",
    "    \n",
    "    train_df = df_shuffled[:split_idx].reset_index(drop=True)\n",
    "    test_df = df_shuffled[split_idx:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n Split complete:\")\n",
    "    print(f\"    Train set: {len(train_df):,} tickets ({train_ratio*100:.0f}%)\")\n",
    "    print(f\"    Test set: {len(test_df):,} tickets ({(1-train_ratio)*100:.0f}%)\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Load and split data\n",
    "train_df, test_df = load_and_split_data(\n",
    "    CONFIG['csv_path'], \n",
    "    CONFIG['train_test_split'],\n",
    "    CONFIG['random_seed']\n",
    ")\n",
    "\n",
    "# Display sample with both original and English columns\n",
    "print(\"\\n Sample train ticket:\")\n",
    "if 'subject' in train_df.columns:\n",
    "    print(\"\\nOriginal (multilingual):\")\n",
    "    print(train_df[['subject', 'type', 'priority', 'original_language']].head(3))\n",
    "    print(\"\\nEnglish translations:\")\n",
    "    print(train_df[['subject_english', 'type', 'priority']].head(3))\n",
    "else:\n",
    "    print(train_df[['subject_english', 'type', 'priority', 'original_language']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating Embeddings (Week 04 Concept → Tickets)\n",
    "\n",
    "### Embeddings: From PDF Chunks to Ticket Fields\n",
    "\n",
    "**Week 04 Recap**: We embedded PDF chunks (arbitrary text segments)  \n",
    "**Week 05**: We embed structured ticket fields (subject + body + answer)\n",
    "\n",
    "**Key Difference:**\n",
    "- PDF had arbitrary text → needed chunking strategy\n",
    "- Tickets are pre-structured → use fields directly\n",
    "\n",
    "### Why Combine Subject + Body + Answer?\n",
    "\n",
    "Each field contributes different information:\n",
    "- **Subject**: Main issue keywords (\"VPN connection failed\")\n",
    "- **Body**: Detailed context (\"tried restarting router...\")  \n",
    "- **Answer**: Solution keywords (\"check firewall settings...\")\n",
    "\n",
    "→ **Embedding captures the full \"problem→solution\" pattern**\n",
    "\n",
    "Let's see this in action with a single ticket first, then scale to all tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_text(row: pd.Series, fields: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Combine multiple fields into single text for embedding.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row\n",
    "        fields: List of field names to combine\n",
    "    \n",
    "    Returns:\n",
    "        Combined text string\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for field in fields:\n",
    "        if pd.notna(row.get(field)):\n",
    "            texts.append(f\"{field.capitalize()}: {row[field]}\")\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "\n",
    "def embed_tickets(df: pd.DataFrame, embedding_fields: List[str]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for ticket data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with ticket data (already translated to English)\n",
    "        embedding_fields: Fields to include in embeddings\n",
    "    \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Generating embeddings for {len(df):,} tickets...\")\n",
    "    print(f\"   Fields: {', '.join(embedding_fields)}\")\n",
    "    \n",
    "    # Combine fields into single text per ticket\n",
    "    combined_texts = [create_combined_text(row, embedding_fields) for _, row in df.iterrows()]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embedder.encode(combined_texts, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"✅ Generated {len(embeddings):,} embeddings (dim: {embeddings.shape[1]})\")\n",
    "    return embeddings.tolist()\n",
    "\n",
    "\n",
    "def prepare_metadata(row: pd.Series, metadata_fields: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from DataFrame row.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row\n",
    "        metadata_fields: Fields to extract as metadata\n",
    "    \n",
    "    Returns:\n",
    "        Metadata dictionary\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    for field in metadata_fields:\n",
    "        value = row.get(field)\n",
    "        # Convert to string, handle NaN\n",
    "        metadata[field] = str(value) if pd.notna(value) else \"unknown\"\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_vector_db(\n",
    "    df: pd.DataFrame,\n",
    "    collection_name: str,\n",
    "    embedding_fields: List[str],\n",
    "    metadata_fields: List[str],\n",
    "    clear_existing: bool = True\n",
    ") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Load tickets into ChromaDB vector database.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with ticket data (already translated to English)\n",
    "        collection_name: Name for ChromaDB collection\n",
    "        embedding_fields: Fields to embed\n",
    "        metadata_fields: Fields to store as metadata\n",
    "        clear_existing: Whether to clear existing collection\n",
    "    \n",
    "    Returns:\n",
    "        ChromaDB collection object\n",
    "    \"\"\"\n",
    "    print(f\"\\n Loading vector database: {collection_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create or get collection\n",
    "    if clear_existing:\n",
    "        try:\n",
    "            chroma_client.delete_collection(name=collection_name)\n",
    "            print(\"  Cleared existing collection\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(f\"✅ Created collection: {collection_name}\")\n",
    "    \n",
    "    # Generate embeddings (data already in English)\n",
    "    embeddings = embed_tickets(df, embedding_fields)\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    ids = [f\"ticket_{i}\" for i in range(len(df))]\n",
    "    documents = [create_combined_text(row, embedding_fields) for _, row in df.iterrows()]\n",
    "    metadatas = [prepare_metadata(row, metadata_fields) for _, row in df.iterrows()]\n",
    "    \n",
    "    # Add to collection in batches (ChromaDB has batch size limits)\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        end_idx = min(i + batch_size, len(df))\n",
    "        collection.add(\n",
    "            embeddings=embeddings[i:end_idx],\n",
    "            documents=documents[i:end_idx],\n",
    "            ids=ids[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx]\n",
    "        )\n",
    "        print(f\"   Loaded batch {i//batch_size + 1}: {end_idx:,}/{len(df):,} tickets\")\n",
    "    \n",
    "    print(f\"\\n✅ Vector database loaded successfully\")\n",
    "    print(f\"   Total tickets: {collection.count():,}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Embedding & Vector DB Functions - Single Ticket Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample ticket text to embed:\n",
      "================================================================================\n",
      "Subject_english: Dear Support, I’m requesting a return for my HP DeskJet 3755. It’s not working properly and the wireless connectivity is spotty.\n",
      "Body_english: Hi there, to start your return, just head over to our returns page or give our Customer Service a call at <tel_num>. Thanks!\n",
      "Answer_english:...\n",
      "================================================================================\n",
      "\n",
      " Generated embedding vector:\n",
      "    Dimensions: 384\n",
      "    First 10 values: [-0.1047  0.0166  0.025  -0.0016 -0.0491 -0.0374 -0.0098 -0.0488 -0.1002\n",
      " -0.0509]\n",
      "    These numbers capture the semantic meaning of the ticket\n",
      "\n",
      "What does this embedding represent?\n",
      "   - Tickets with similar problems will have similar embedding vectors\n",
      "   - Vector similarity (cosine distance) = semantic similarity\n",
      "   - This is how we'll find relevant historical tickets!\n",
      "\n",
      "Next step: Scale this to all 2,880 training tickets...\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# STEP 1: Single Ticket Embedding (Simple Example)\n",
    "# =====================================================\n",
    "\n",
    "# Get a sample ticket from our training data\n",
    "sample_ticket = train_df.iloc[0]\n",
    "\n",
    "# Combine the English fields into one text block\n",
    "sample_text = f\"\"\"Subject_english: {sample_ticket['subject_english']}\n",
    "Body_english: {sample_ticket['body_english']}\n",
    "Answer_english: {sample_ticket['answer_english']}\"\"\"\n",
    "\n",
    "print(\" Sample ticket text to embed:\")\n",
    "print(\"=\" * 80)\n",
    "print(sample_text[:300] + \"...\" if len(sample_text) > 300 else sample_text)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate embedding for this single ticket\n",
    "sample_embedding = embedder.encode([sample_text])[0]\n",
    "\n",
    "print(f\"\\n Generated embedding vector:\")\n",
    "print(f\"    Dimensions: {len(sample_embedding)}\")\n",
    "print(f\"    First 10 values: {sample_embedding[:10].round(4)}\")\n",
    "print(f\"    These numbers capture the semantic meaning of the ticket\")\n",
    "\n",
    "print(f\"\\nWhat does this embedding represent?\")\n",
    "print(f\"   - Tickets with similar problems will have similar embedding vectors\")\n",
    "print(f\"   - Vector similarity (cosine distance) = semantic similarity\")\n",
    "print(f\"   - This is how we'll find relevant historical tickets!\")\n",
    "\n",
    "print(f\"\\nNext step: Scale this to all {len(train_df):,} training tickets...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Embedding & Vector DB Functions (Scale to All Tickets)\n",
    "\n",
    "Now that we understand how to embed a single ticket, let's scale this to all training tickets and store them in ChromaDB for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading vector database: ticket_rag_collection\n",
      "============================================================\n",
      "  Cleared existing collection\n",
      "✅ Created collection: ticket_rag_collection\n",
      "🔄 Generating embeddings for 2,880 tickets...\n",
      "   Fields: subject_english, body_english, answer_english\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe775a19db94c52953f0fe21b4b9048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 2,880 embeddings (dim: 384)\n",
      "   Loaded batch 1: 1,000/2,880 tickets\n",
      "   Loaded batch 2: 2,000/2,880 tickets\n",
      "   Loaded batch 3: 2,880/2,880 tickets\n",
      "\n",
      "✅ Vector database loaded successfully\n",
      "   Total tickets: 2,880\n",
      "\n",
      "Database Statistics:\n",
      "   Collection: ticket_rag_collection\n",
      "   Total tickets: 2,880\n",
      "   Embedding dim: 384\n",
      "   Metadata fields: type, queue, priority, business_type, original_language\n"
     ]
    }
   ],
   "source": [
    "# Load training data into vector database\n",
    "collection = load_vector_db(\n",
    "    train_df,\n",
    "    CONFIG['collection_name'],\n",
    "    CONFIG['embedding_fields'],\n",
    "    CONFIG['metadata_fields'],\n",
    "    clear_existing=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDatabase Statistics:\")\n",
    "print(f\"   Collection: {CONFIG['collection_name']}\")\n",
    "print(f\"   Total tickets: {collection.count():,}\")\n",
    "print(f\"   Embedding dim: {len(embedder.encode(['test'])[0])}\")\n",
    "print(f\"   Metadata fields: {', '.join(CONFIG['metadata_fields'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query & Response Functions\n",
    "\n",
    "###  Understanding RAG Modes\n",
    "\n",
    "Our system supports two different RAG (Retrieval-Augmented Generation) modes:\n",
    "\n",
    "####  Strict RAG Mode (`rag_mode: 'strict'`)\n",
    "**What it does:**\n",
    "- LLM is **constrained** to ONLY use information from retrieved historical tickets\n",
    "- Cannot supplement with its general knowledge\n",
    "- Will explicitly say \"I don't have enough information...\" if context is insufficient\n",
    "\n",
    "**When to use:**\n",
    "- Compliance-critical environments where answers must be traceable to documented sources\n",
    "- Quality assurance testing to verify RAG retrieval quality\n",
    "- When you want to test if your knowledge base has sufficient coverage\n",
    "\n",
    "**Example behavior:**\n",
    "- Question: \"How do I reset my VPN password?\"\n",
    "- If similar tickets exist → Answers based on those tickets\n",
    "- If no similar tickets → \"I don't have enough information in the historical tickets to answer this question fully.\"\n",
    "\n",
    "---\n",
    "\n",
    "####  Augmented RAG Mode (`rag_mode: 'augmented'`)  \n",
    "**What it does:**\n",
    "- LLM uses retrieved historical tickets as **context** to inform answers\n",
    "- Can **supplement** with its general IT support knowledge\n",
    "- Provides helpful answers even when historical tickets are incomplete\n",
    "\n",
    "**When to use:**\n",
    "- Production support systems where helpfulness matters most\n",
    "- When your knowledge base has gaps but you still want useful answers\n",
    "- General-purpose IT support scenarios\n",
    "\n",
    "**Example behavior:**\n",
    "- Question: \"How do I reset my VPN password?\"\n",
    "- Uses similar historical tickets as context\n",
    "- Supplements with general VPN troubleshooting knowledge if needed\n",
    "- Always provides a helpful, actionable answer\n",
    "\n",
    "---\n",
    "\n",
    "###  Switching Between Modes\n",
    "\n",
    "To change modes, update the `CONFIG` dictionary in cell-5:\n",
    "\n",
    "```python\n",
    "CONFIG = {\n",
    "    # ... other settings ...\n",
    "    'rag_mode': 'strict',      # Options: 'strict' or 'augmented'\n",
    "}\n",
    "```\n",
    "\n",
    "Then re-run the query cells to see the difference in behavior!\n",
    "\n",
    "---\n",
    "\n",
    "###  Mode Comparison\n",
    "\n",
    "| Aspect | Strict RAG | Augmented RAG |\n",
    "|--------|-----------|---------------|\n",
    "| **Sources** | Historical tickets only | Historical tickets + LLM knowledge |\n",
    "| **Coverage** | Limited to knowledge base | Can answer beyond knowledge base |\n",
    "| **Traceability** | 100% traceable | Mixed sources |\n",
    "| **Helpfulness** | May say \"I don't know\" | Always attempts helpful answer |\n",
    "| **Best for** | Compliance, QA testing | Production support |\n",
    "\n",
    "---\n",
    "\n",
    "###  Value\n",
    "\n",
    "By implementing both modes, you can:\n",
    "1. **Test retrieval quality**: Strict mode shows if your vector search is finding relevant tickets\n",
    "2. **Compare approaches**: See how LLM knowledge augmentation changes answers\n",
    "3. **Understand trade-offs**: Coverage vs. traceability, helpfulness vs. compliance\n",
    "4. **Learn RAG patterns**: Both are valid approaches used in real-world systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query functions ready\n"
     ]
    }
   ],
   "source": [
    "def search_similar_tickets(query_text: str, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for similar tickets in vector database using pure vector similarity.\n",
    "    \n",
    "    Args:\n",
    "        query_text: Text to search for\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of similar tickets with metadata and scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedder.encode([query_text])[0].tolist()\n",
    "    \n",
    "    # Execute search (pure vector similarity, no filters)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    similar_tickets = []\n",
    "    for i in range(len(results['ids'][0])):\n",
    "        similar_tickets.append({\n",
    "            'id': results['ids'][0][i],\n",
    "            'document': results['documents'][0][i],\n",
    "            'metadata': results['metadatas'][0][i],\n",
    "            'distance': results['distances'][0][i],\n",
    "            'similarity': 1 - results['distances'][0][i]\n",
    "        })\n",
    "    \n",
    "    return similar_tickets\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    question: str,\n",
    "    context_tickets: List[Dict],\n",
    "    temperature: float = 0.2\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using LM Studio LLM with configurable RAG mode.\n",
    "    \n",
    "    RAG Modes:\n",
    "    - 'strict': Context-only mode - LLM must ONLY use information from historical tickets\n",
    "    - 'augmented': Context + knowledge mode - LLM can supplement with general knowledge\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        context_tickets: List of similar tickets for context\n",
    "        temperature: LLM temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer text\n",
    "    \"\"\"\n",
    "    # Build context from similar tickets\n",
    "    context_parts = []\n",
    "    for i, ticket in enumerate(context_tickets, 1):\n",
    "        context_parts.append(f\"\\n--- Similar Ticket {i} (similarity: {ticket['similarity']:.2f}) ---\\n{ticket['document']}\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Choose prompt based on RAG mode\n",
    "    if CONFIG.get('rag_mode', 'augmented') == 'strict':\n",
    "        # STRICT RAG: Context-only mode\n",
    "        prompt = f\"\"\"You are a helpful IT support assistant. You must ONLY use information from the historical tickets provided below to answer the question. Do not use any external knowledge.\n",
    "\n",
    "SIMILAR HISTORICAL TICKETS:\n",
    "{context}\n",
    "\n",
    "NEW SUPPORT QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer ONLY using information from the historical tickets above\n",
    "- If the historical tickets don't contain enough information to answer the question, say \"I don't have enough information in the historical tickets to answer this question fully.\"\n",
    "- Do NOT use any general knowledge or information not present in the historical tickets\n",
    "- Provide a clear, actionable solution based solely on the provided context\n",
    "- Be concise but thorough\n",
    "- Reference which historical ticket(s) your answer comes from\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    else:\n",
    "        # AUGMENTED RAG: Context + LLM knowledge mode\n",
    "        prompt = f\"\"\"You are a helpful IT support assistant. Based on the similar historical tickets provided below, generate a clear and helpful answer to the new support question.\n",
    "\n",
    "SIMILAR HISTORICAL TICKETS:\n",
    "{context}\n",
    "\n",
    "NEW SUPPORT QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use the historical tickets as context to inform your answer\n",
    "- You may supplement with your general IT support knowledge when appropriate\n",
    "- Provide a clear, actionable solution\n",
    "- Be concise but thorough\n",
    "- If the question is unclear or lacks context, ask clarifying questions\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=CONFIG['llm_model'],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful IT support assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=CONFIG['max_tokens'],\n",
    "            timeout=60\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "\n",
    "def calculate_confidence(similar_tickets: List[Dict]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate confidence score based on similarity of retrieved tickets.\n",
    "    \n",
    "    Args:\n",
    "        similar_tickets: List of similar tickets with similarity scores\n",
    "    \n",
    "    Returns:\n",
    "        Confidence score between 0 and 1\n",
    "    \"\"\"\n",
    "    if not similar_tickets:\n",
    "        return 0.0\n",
    "    \n",
    "    # Use weighted average of top similarities\n",
    "    # Top result weighted more heavily\n",
    "    weights = [1.0, 0.8, 0.6, 0.4, 0.2][:len(similar_tickets)]\n",
    "    weighted_sim = sum(t['similarity'] * w for t, w in zip(similar_tickets, weights))\n",
    "    total_weight = sum(weights)\n",
    "    \n",
    "    confidence = weighted_sim / total_weight\n",
    "    return round(confidence, 3)\n",
    "\n",
    "\n",
    "def answer_ticket(ticket_json: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve context and generate answer.\n",
    "    \n",
    "    Args:\n",
    "        ticket_json: Dictionary with ticket data\n",
    "            Required: 'subject', 'body'\n",
    "            Optional fields are stored but not used for filtering\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer, confidence, and references\n",
    "    \"\"\"\n",
    "    # Extract fields\n",
    "    subject = ticket_json.get('subject', '')\n",
    "    body = ticket_json.get('body', '')\n",
    "    \n",
    "    # Create query text\n",
    "    query_text = f\"Subject: {subject}\\nBody: {body}\"\n",
    "    \n",
    "    # Search for similar tickets (pure vector similarity)\n",
    "    print(f\" Searching for similar tickets using vector similarity...\")\n",
    "    \n",
    "    similar_tickets = search_similar_tickets(query_text, CONFIG['top_k'])\n",
    "    \n",
    "    print(f\"   Found {len(similar_tickets)} similar tickets\")\n",
    "    \n",
    "    # Generate answer\n",
    "    rag_mode = CONFIG.get('rag_mode', 'augmented')\n",
    "    print(f\" Generating answer using {rag_mode.upper()} RAG mode...\")\n",
    "    answer = generate_answer(query_text, similar_tickets, CONFIG['temperature'])\n",
    "    \n",
    "    # Calculate confidence\n",
    "    confidence = calculate_confidence(similar_tickets)\n",
    "    \n",
    "    # Format references\n",
    "    references = []\n",
    "    for ticket in similar_tickets:\n",
    "        references.append({\n",
    "            'ticket_id': ticket['id'],\n",
    "            'similarity': round(ticket['similarity'], 3),\n",
    "            'metadata': ticket['metadata'],\n",
    "            'preview': ticket['document'][:200] + '...'\n",
    "        })\n",
    "    \n",
    "    # Build response\n",
    "    response = {\n",
    "        'answer': answer,\n",
    "        'confidence': confidence,\n",
    "        'num_references': len(references),\n",
    "        'references': references,\n",
    "        'rag_mode': rag_mode,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Answer generated (confidence: {confidence:.2%}, mode: {rag_mode})\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"✅ Query functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Sample Query\n",
    "\n",
    "**Note**: This system uses pure vector similarity search based on semantic meaning of the ticket content (subject + body). Metadata (type, queue, priority, etc.) is stored but not used for filtering or ranking - this keeps the system simple and can be enhanced in future lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG['rag_mode'] = 'strict'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SAMPLE TICKET QUERY\n",
      "====================================================================================================\n",
      "\n",
      "SUBJECT:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cannot connect to VPN from home\n",
      "\n",
      "BODY:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I'm trying to connect to the company VPN from my home network but keep getting a connection timeout\n",
      "error.         I've tried restarting my router and computer but the issue persists. This is blocking\n",
      "my work.\n",
      "\n",
      "====================================================================================================\n",
      " Searching for similar tickets using vector similarity...\n",
      "   Found 5 similar tickets\n",
      " Generating answer using AUGMENTED RAG mode...\n",
      "✅ Answer generated (confidence: 41.70%, mode: augmented)\n",
      "\n",
      " GENERATED ANSWER\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Subject:** Re: Cannot connect to VPN from home – troubleshooting steps\n",
       "\n",
       "Hi [Name],\n",
       "\n",
       "Thanks for reaching out. A connection‑timeout usually points to a networking or configuration issue on the client side (or sometimes an IP restriction on the VPN server). Below are some quick checks and actions that often resolve this type of problem.\n",
       "\n",
       "| Step | What to do | Why it helps |\n",
       "|------|------------|--------------|\n",
       "| **1. Verify the VPN address** | Make sure you’re entering the exact hostname/IP the IT team gave you (e.g., `vpn.company.com`). A typo or old address will cause a timeout. | |\n",
       "| **2. Check your local firewall / antivirus** | Temporarily disable any third‑party firewall/antivirus software and try again. Some programs block VPN ports (UDP 1194, TCP 443). | |\n",
       "| **3. Confirm the required port is open** | Run `telnet vpn.company.com 443` or `nc -vz vpn.company.com 443`. If it fails, your home router may be blocking that port. | |\n",
       "| **4. Try a different device / network** | Connect from another computer on the same Wi‑Fi, or use a mobile hotspot. This tells us whether the issue is with your machine or the home network. | |\n",
       "| **5. Reinstall/Update the VPN client** | Uninstall the current VPN client, download the latest installer from the IT portal, and install it again. Older clients can have bugs that cause timeouts. | |\n",
       "| **6. Clear any proxy settings** | In your browser or system network settings make sure no HTTP/HTTPS proxy is configured – VPNs usually don’t work with a proxy in place. | |\n",
       "| **7. Check for IP restrictions** | Some VPN servers only allow connections from whitelisted IP ranges. If you’re on a dynamic home IP, the IT team may need to add it. Ask them if your current public IP (you can find it at `whatismyip.com`) is allowed. | |\n",
       "| **8. Review client logs** | Most VPN clients write a log file (`vpn.log` or similar). If you can share that (or just the first 20‑30 lines), we can spot specific errors. | |\n",
       "\n",
       "### Quick test\n",
       "\n",
       "1. Open a terminal/command prompt.\n",
       "2. Run:  \n",
       "   ```bash\n",
       "   ping vpn.company.com\n",
       "   ```\n",
       "3. If you get replies, the DNS and basic connectivity are fine.  \n",
       "4. Then run:  \n",
       "   ```bash\n",
       "   telnet vpn.company.com 443\n",
       "   ```\n",
       "   or for UDP: `nc -vz -u vpn.company.com 1194` (Linux/macOS).  \n",
       "\n",
       "If either command times out, the issue is likely on your local network side.\n",
       "\n",
       "### If you’re still stuck\n",
       "\n",
       "- Let me know which VPN client you’re using (Cisco AnyConnect, OpenVPN, etc.).\n",
       "- Share any error code or log snippet.\n",
       "- Tell me if you can connect from another device or network.\n",
       "\n",
       "Once we have that info, I’ll be able to narrow it down further. In the meantime, try the steps above and let me know what happens.\n",
       "\n",
       "Best regards,\n",
       "\n",
       "[Your Name]  \n",
       "IT Support Team\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      " Confidence: 41.7%\n",
      " References: 5 similar tickets\n",
      "\n",
      " TOP 3 SIMILAR TICKETS:\n",
      "====================================================================================================\n",
      "\n",
      "1. Similarity: 44.5% | Problem | Priority: medium\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "   Subject_english: Intermittent Connection Issues Reported Body_english: You might need to\n",
      "   update the firmware or reboot the device. Answer_english: Try rebooting the device first. If\n",
      "   the issue persists...\n",
      "\n",
      "2. Similarity: 41.2% | Incident | Priority: medium\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "   Subject_english: Help Needed with Connectivity Issues Body_english: Hi Customer Support,  I'm\n",
      "   <name> from <company_name>. We're having connectivity problems that are slowing down our\n",
      "   remote team's pro...\n",
      "\n",
      "3. Similarity: 39.7% | Problem | Priority: low\n",
      "   ------------------------------------------------------------------------------------------------\n",
      "   Subject_english: Intermittent Connectivity Issues on the ISR4331 Body_english: There are\n",
      "   conflicting network configurations. Answer_english: Please review and unify your network\n",
      "   settings to ensure rel...\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def render_answer(answer_text: str, title: str = \"GENERATED ANSWER\"):\n",
    "    \"\"\"\n",
    "    Render markdown-formatted answer beautifully in Jupyter.\n",
    "    \n",
    "    Args:\n",
    "        answer_text: The LLM-generated answer (may contain markdown)\n",
    "        title: Title to display above the answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n {title}\")\n",
    "    print(\"=\" * 100)\n",
    "    display(Markdown(answer_text))\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "# Create a sample ticket query\n",
    "sample_ticket = {\n",
    "    \"subject\": \"Cannot connect to VPN from home\",\n",
    "    \"body\": \"I'm trying to connect to the company VPN from my home network but keep getting a connection timeout error. \\\n",
    "        I've tried restarting my router and computer but the issue persists. This is blocking my work.\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"SAMPLE TICKET QUERY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nSUBJECT:\")\n",
    "print(\"-\" * 100)\n",
    "print(sample_ticket['subject'])\n",
    "\n",
    "print(f\"\\nBODY:\")\n",
    "print(\"-\" * 100)\n",
    "wrapped_body = textwrap.fill(sample_ticket['body'], width=100)\n",
    "print(wrapped_body)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Get answer\n",
    "response = answer_ticket(sample_ticket)\n",
    "\n",
    "# Display results with beautiful markdown rendering\n",
    "render_answer(response['answer'])\n",
    "\n",
    "print(f\"\\n Confidence: {response['confidence']:.1%}\")\n",
    "print(f\" References: {response['num_references']} similar tickets\")\n",
    "\n",
    "print(\"\\n TOP 3 SIMILAR TICKETS:\")\n",
    "print(\"=\" * 100)\n",
    "for i, ref in enumerate(response['references'][:3], 1):\n",
    "    print(f\"\\n{i}. Similarity: {ref['similarity']:.1%} | {ref['metadata'].get('type', 'Unknown')} | Priority: {ref['metadata'].get('priority', 'N/A')}\")\n",
    "    print(\"   \" + \"-\" * 96)\n",
    "    preview_wrapped = textwrap.fill(ref['preview'], width=96, initial_indent='   ', subsequent_indent='   ')\n",
    "    print(preview_wrapped)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW - set rag mode to augmented and rerun:\n",
    "\n",
    "CONFIG['rag_mode'] = 'augmented'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LLM Evaluation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation function ready\n"
     ]
    }
   ],
   "source": [
    "def evaluate_answer(\n",
    "    question: str,\n",
    "    original_answer: str,\n",
    "    generated_answer: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Use LLM to evaluate generated answer against original.\n",
    "    \n",
    "    Args:\n",
    "        question: Original ticket question\n",
    "        original_answer: Original answer from dataset\n",
    "        generated_answer: RAG-generated answer\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results with scores and feedback\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an expert evaluator for IT support responses. Compare the generated answer against the original answer and rate it on the following criteria:\n",
    "\n",
    "ORIGINAL QUESTION:\n",
    "{question}\n",
    "\n",
    "ORIGINAL ANSWER:\n",
    "{original_answer}\n",
    "\n",
    "GENERATED ANSWER:\n",
    "{generated_answer}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. Accuracy (1-5): Does the generated answer provide correct information?\n",
    "2. Completeness (1-5): Does it cover all important points from the original?\n",
    "3. Clarity (1-5): Is it clear and easy to understand?\n",
    "4. Helpfulness (1-5): Would this answer help resolve the user's issue?\n",
    "\n",
    "Please provide:\n",
    "- A score for each criterion (1-5, where 5 is best)\n",
    "- Brief feedback explaining the scores\n",
    "- An overall score (1-5)\n",
    "\n",
    "Format your response as JSON:\n",
    "{{\n",
    "  \"accuracy\": <score>,\n",
    "  \"completeness\": <score>,\n",
    "  \"clarity\": <score>,\n",
    "  \"helpfulness\": <score>,\n",
    "  \"overall\": <score>,\n",
    "  \"feedback\": \"<explanation>\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=CONFIG['llm_model'],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator. Respond only with valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        # Extract JSON if wrapped in markdown code blocks\n",
    "        if \"```json\" in result_text:\n",
    "            result_text = result_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in result_text:\n",
    "            result_text = result_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        evaluation = json.loads(result_text)\n",
    "        return evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"raw_response\": response.choices[0].message.content if 'response' in locals() else None\n",
    "        }\n",
    "\n",
    "print(\"✅ Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strict\n"
     ]
    }
   ],
   "source": [
    "print(CONFIG['rag_mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " TEST TICKET EVALUATION\n",
      "====================================================================================================\n",
      "\n",
      " TICKET SUBJECT (English):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recurring Excel Crash Issue After the Update\n",
      "\n",
      " TICKET BODY (English):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dear Customer Support,  I’m writing to report a persistent problem that’s surfaced after the latest\n",
      "software update installed on my computer. My name is <name> and I’m experiencing frequent crashes\n",
      "specifically with the Microsoft Office 365 Excel application. Every time I try to open or work\n",
      "within Excel, the app unexpectedly closes, preventing me from accessing important data and documents\n",
      "I need for daily tasks. This issue seems to persist regardless of the spreadsheet file I’m trying to\n",
      "open or create. It started immediately after the update was installed, so we suspect it may be\n",
      "linked to recent changes. Could you provide guidance or a possible solution to resolve this matter\n",
      "quickly? I heavily rely on Microsoft Office 365 for my work and hope this issue is resolved as soon\n",
      "as possible. Understanding the urgency of maintaining productivity, I look forward to your prompt\n",
      "response. Feel free to contact me at <tel_num>.  Thank you for your support.  Sincerely,  <name>\n",
      "\n",
      " Ticket Metadata:\n",
      "   Type: Incident | Priority: high | Language: es\n",
      "\n",
      "====================================================================================================\n",
      " Searching for similar tickets using vector similarity...\n",
      "   Found 5 similar tickets\n",
      " Generating answer using AUGMENTED RAG mode...\n",
      "✅ Answer generated (confidence: 90.30%, mode: augmented)\n",
      "\n",
      " ANSWER COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      " ORIGINAL ANSWER (from dataset):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dear <name>,  Thank you for reaching out about the Excel crash issue after your recent software\n",
      "update. I understand how disruptive this can be, and I’m here to help you get it sorted out quickly.\n",
      "First, try repairing your Office installation with these steps: 1. Open Control Panel > Programs >\n",
      "Programs and Features. 2. Select Microsoft Office 365 and click Change. 3. Choose the Quick Repair\n",
      "option first. If that doesn’t fix the problem, try the Online Repair option.  If the issue persists,\n",
      "make sure both your Excel and Windows are updated with the latest patches and updates.  As a\n",
      "temporary fix, try running Excel in Safe Mode to see if the problem is related to add‑ins. You can\n",
      "do this by holding Ctrl while launching Excel.  For further assistance, I recommend checking whether\n",
      "the update conflict stems from a specific Windows service pack or recent hardware changes.  Feel\n",
      "free to contact us via email or your preferred direct contact method if these steps don’t resolve\n",
      "the issue. We’re committed to helping you get back to normalcy quickly.  Thank you for your patience\n",
      "and understanding.  Sincerely,  Customer Service Agent\n",
      "\n",
      " GENERATED ANSWER (RAG system with markdown rendering):\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Subject:** Re: Recurring Excel Crash Issue After the Update  \n",
       "\n",
       "Dear <name>,\n",
       "\n",
       "Thank you for reaching out. I understand how disruptive these crashes are and will walk you through a few steps that usually resolve the issue quickly.\n",
       "\n",
       "| Step | What to do | Why it helps |\n",
       "|------|------------|--------------|\n",
       "| **1 – Repair Office** | 1️⃣ Open Control Panel → Programs & Features. <br>2️⃣ Select *Microsoft Office 365* and click **Change**. <br>3️⃣ Choose **Quick Repair** first; if that doesn’t help, run **Online Repair** (requires internet). | Fixes corrupted files or missing components introduced by the update. |\n",
       "| **2 – Verify Updates** | 1️⃣ Open Windows Settings → Update & Security → Check for updates. <br>2️⃣ Install any pending Office updates (sometimes a patch follows the initial release). | Keeps both Windows and Office at the same supported version, reducing incompatibilities. |\n",
       "| **3 – Safe Mode / Add‑ins** | 1️⃣ Hold **Ctrl** while launching Excel (or run `excel.exe /safe`). <br>2️⃣ If Excel stays stable, disable all add‑ins: File → Options → Add‑Ins → Manage → Excel Add‑Ins → Go. Uncheck everything and restart normally. | Identifies whether a third‑party add‑in is causing the crash. |\n",
       "| **4 – Check for Conflicting Software** | • Disable or uninstall recently added antivirus/backup tools temporarily. <br>• Ensure no other Office apps (Word, PowerPoint) are crashing; if they do, the problem is broader than Excel alone. | Some security or backup utilities can interfere with Office’s runtime after an update. |\n",
       "| **5 – Test on a Clean Profile** | Create a new Windows user account and try opening Excel there. If it works, your original profile may have corrupted settings. | Confirms whether the issue is system‑wide or tied to your user profile. |\n",
       "\n",
       "### Quick Checklist\n",
       "\n",
       "- [ ] Office 365 fully updated  \n",
       "- [ ] Quick Repair (then Online if needed) completed  \n",
       "- [ ] Excel runs in Safe Mode without crashing  \n",
       "- [ ] All add‑ins disabled → normal mode works  \n",
       "- [ ] No recent antivirus/backup changes  \n",
       "\n",
       "If after completing these steps Excel still crashes, please let me know the exact error message or any crash logs you can find (Event Viewer → Windows Logs → Application). Those details will help us dig deeper.\n",
       "\n",
       "Feel free to reply here or call me directly at <tel_num>. I’ll follow up within 2 hours if I don’t hear back from you.\n",
       "\n",
       "Thank you for your patience—let’s get Excel running smoothly again.\n",
       "\n",
       "Best regards,\n",
       "\n",
       "<Your Name>  \n",
       "IT Support Specialist  \n",
       "Customer Support Team"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      " Evaluating answer quality...\n",
      "\n",
      " EVALUATION RESULTS:\n",
      "====================================================================================================\n",
      "\n",
      "Metric               Score      Rating\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Overall              5/5      ⭐⭐⭐⭐⭐\n",
      "Accuracy             5/5      ⭐⭐⭐⭐⭐\n",
      "Completeness         5/5      ⭐⭐⭐⭐⭐\n",
      "Clarity              5/5      ⭐⭐⭐⭐⭐\n",
      "Helpfulness          5/5      ⭐⭐⭐⭐⭐\n",
      "\n",
      " Confidence Score: 90.3%\n",
      " Similar Tickets Used: 5\n",
      "\n",
      " Evaluator Feedback:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The generated answer accurately reflects the troubleshooting steps from the original response and\n",
      "expands on them with additional useful details (e.g., add‑in disabling, clean profile test). It\n",
      "presents information in a clear table format, making it easy to follow. The extra guidance on\n",
      "checking logs and offering further contact also enhances its helpfulness.\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test evaluation with a ticket from test set\n",
    "test_ticket_idx = 380\n",
    "test_row = test_df.iloc[test_ticket_idx]\n",
    "\n",
    "# Prepare ticket for RAG (only subject and body needed)\n",
    "test_ticket_input = {\n",
    "    \"subject\": test_row['subject_english'],\n",
    "    \"body\": test_row['body_english']\n",
    "}\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\" TEST TICKET EVALUATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Display ticket details\n",
    "print(f\"\\n TICKET SUBJECT (English):\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{test_row['subject_english']}\")\n",
    "\n",
    "print(f\"\\n TICKET BODY (English):\")\n",
    "print(\"-\" * 100)\n",
    "# Wrap body text to 100 characters\n",
    "import textwrap\n",
    "wrapped_body = textwrap.fill(test_row['body_english'], width=100)\n",
    "print(wrapped_body)\n",
    "\n",
    "print(f\"\\n Ticket Metadata:\")\n",
    "print(f\"   Type: {test_row.get('type')} | Priority: {test_row.get('priority')} | Language: {test_row.get('original_language')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Generate answer\n",
    "rag_response = answer_ticket(test_ticket_input)\n",
    "\n",
    "# Display answers with markdown rendering for generated answer\n",
    "print(\"\\n ANSWER COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n ORIGINAL ANSWER (from dataset):\")\n",
    "print(\"-\" * 100)\n",
    "original_wrapped = textwrap.fill(test_row['answer_english'], width=100)\n",
    "print(original_wrapped)\n",
    "\n",
    "# Render generated answer with beautiful markdown formatting\n",
    "from IPython.display import display, Markdown\n",
    "print(\"\\n GENERATED ANSWER (RAG system with markdown rendering):\")\n",
    "print(\"-\" * 100)\n",
    "display(Markdown(rag_response['answer']))\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n Evaluating answer quality...\")\n",
    "question_text = f\"Subject: {test_row['subject_english']}\\nBody: {test_row['body_english']}\"\n",
    "evaluation = evaluate_answer(\n",
    "    question_text,\n",
    "    test_row['answer_english'],\n",
    "    rag_response['answer']\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n EVALUATION RESULTS:\")\n",
    "print(\"=\" * 100)\n",
    "if 'error' not in evaluation:\n",
    "    print(f\"\\n{'Metric':<20} {'Score':<10} {'Rating'}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Overall':<20} {evaluation['overall']}/5      {'⭐' * int(evaluation['overall'])}\")\n",
    "    print(f\"{'Accuracy':<20} {evaluation['accuracy']}/5      {'⭐' * int(evaluation['accuracy'])}\")\n",
    "    print(f\"{'Completeness':<20} {evaluation['completeness']}/5      {'⭐' * int(evaluation['completeness'])}\")\n",
    "    print(f\"{'Clarity':<20} {evaluation['clarity']}/5      {'⭐' * int(evaluation['clarity'])}\")\n",
    "    print(f\"{'Helpfulness':<20} {evaluation['helpfulness']}/5      {'⭐' * int(evaluation['helpfulness'])}\")\n",
    "    \n",
    "    print(f\"\\n Confidence Score: {rag_response['confidence']:.1%}\")\n",
    "    print(f\" Similar Tickets Used: {rag_response['num_references']}\")\n",
    "    \n",
    "    print(f\"\\n Evaluator Feedback:\")\n",
    "    print(\"-\" * 100)\n",
    "    feedback_wrapped = textwrap.fill(evaluation['feedback'], width=100)\n",
    "    print(feedback_wrapped)\n",
    "else:\n",
    "    print(f\"❌ Evaluation error: {evaluation['error']}\")\n",
    "    if evaluation.get('raw_response'):\n",
    "        print(f\"\\nRaw response: {evaluation['raw_response']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Testing & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " BATCH EVALUATION - Testing 5 tickets\n",
      "====================================================================================================\n",
      "\n",
      " Processing ticket 622...  Searching for similar tickets using vector similarity...\n",
      "   Found 5 similar tickets\n",
      " Generating answer using AUGMENTED RAG mode...\n",
      "✅ Answer generated (confidence: 87.60%, mode: augmented)\n",
      "✅ Overall Score: 5/5 | Confidence: 87.6%\n",
      "\n",
      " Processing ticket 555...  Searching for similar tickets using vector similarity...\n",
      "   Found 5 similar tickets\n",
      " Generating answer using AUGMENTED RAG mode...\n",
      "✅ Answer generated (confidence: 89.10%, mode: augmented)\n",
      "✅ Overall Score: 5/5 | Confidence: 89.1%\n",
      "\n",
      " Processing ticket 539...  Searching for similar tickets using vector similarity...\n",
      "   Found 5 similar tickets\n",
      " Generating answer using AUGMENTED RAG mode...\n",
      "✅ Answer generated (confidence: 85.80%, mode: augmented)\n",
      "✅ Overall Score: 5/5 | Confidence: 85.8%\n",
      "\n",
      " Processing ticket 434...  Searching for similar tickets using vector similarity...\n",
      "   Found 5 similar tickets\n",
      " Generating answer using AUGMENTED RAG mode...\n",
      "✅ Answer generated (confidence: 82.40%, mode: augmented)\n",
      "✅ Overall Score: 5/5 | Confidence: 82.4%\n",
      "\n",
      " Processing ticket 550...  Searching for similar tickets using vector similarity...\n",
      "   Found 5 similar tickets\n",
      " Generating answer using AUGMENTED RAG mode...\n",
      "✅ Answer generated (confidence: 90.20%, mode: augmented)\n",
      "✅ Overall Score: 5/5 | Confidence: 90.2%\n",
      "\n",
      "====================================================================================================\n",
      " BATCH EVALUATION SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "Metric               Mean Score      Rating\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Overall              5.00/5         ⭐⭐⭐⭐⭐\n",
      "Accuracy             5.00/5         ⭐⭐⭐⭐⭐\n",
      "Completeness         4.80/5         ⭐⭐⭐⭐\n",
      "Clarity              5.00/5         ⭐⭐⭐⭐⭐\n",
      "Helpfulness          5.00/5         ⭐⭐⭐⭐⭐\n",
      "\n",
      " Average Confidence: 87.0%\n",
      " Average References Used: 5.0\n",
      "\n",
      " DETAILED RESULTS:\n",
      "====================================================================================================\n",
      "Idx    Subject                                  Type       Lang   Score    Conf    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "622    Urgent Outage Issue...                   Incident   en     5/5      87.6%\n",
      "555    Issue with Jira Software 8.20...         Incident   en     5/5      89.1%\n",
      "539    Request to Change Office 365 Subscript.. Change     de     5/5      85.8%\n",
      "434    Recurring SQL Errors Affecting Data Re.. Problem    es     5/5      82.4%\n",
      "550    Issue with the Canon PIXMA MG3620's Wi.. Incident   de     5/5      90.2%\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def run_batch_evaluation(test_df: pd.DataFrame, num_samples: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run RAG system on multiple test tickets and evaluate results.\n",
    "    \n",
    "    Args:\n",
    "        test_df: Test dataset\n",
    "        num_samples: Number of tickets to test\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    sample_indices = np.random.choice(len(test_df), min(num_samples, len(test_df)), replace=False)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\" BATCH EVALUATION - Testing {len(sample_indices)} tickets\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        row = test_df.iloc[idx]\n",
    "        \n",
    "        print(f\"\\n Processing ticket {idx}...\", end=\" \")\n",
    "        \n",
    "        # Prepare input (only subject and body needed)\n",
    "        ticket_input = {\n",
    "            \"subject\": row['subject_english'],\n",
    "            \"body\": row['body_english']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Generate answer\n",
    "            rag_response = answer_ticket(ticket_input)\n",
    "            \n",
    "            # Evaluate\n",
    "            question_text = f\"Subject: {row['subject_english']}\\nBody: {row['body_english']}\"\n",
    "            evaluation = evaluate_answer(\n",
    "                question_text,\n",
    "                row['answer_english'],\n",
    "                rag_response['answer']\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            if 'error' not in evaluation:\n",
    "                results.append({\n",
    "                    'ticket_idx': idx,\n",
    "                    'subject': row['subject_english'][:60] + '...',\n",
    "                    'type': row.get('type'),\n",
    "                    'priority': row.get('priority'),\n",
    "                    'language': row.get('original_language'),\n",
    "                    'confidence': rag_response['confidence'],\n",
    "                    'num_refs': rag_response['num_references'],\n",
    "                    'accuracy': evaluation['accuracy'],\n",
    "                    'completeness': evaluation['completeness'],\n",
    "                    'clarity': evaluation['clarity'],\n",
    "                    'helpfulness': evaluation['helpfulness'],\n",
    "                    'overall': evaluation['overall']\n",
    "                })\n",
    "                print(f\"✅ Overall Score: {evaluation['overall']}/5 | Confidence: {rag_response['confidence']:.1%}\")\n",
    "            else:\n",
    "                print(f\"❌ Evaluation failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run batch evaluation using the TEST data frame - these are tickets we haven't seen before; the answers should not be\n",
    "# in the vector DB\n",
    "\n",
    "eval_results = run_batch_evaluation(test_df, num_samples=5)\n",
    "\n",
    "# Display statistics\n",
    "if len(eval_results) > 0:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\" BATCH EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<20} {'Mean Score':<15} {'Rating'}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Overall':<20} {eval_results['overall'].mean():.2f}/5         {'⭐' * int(eval_results['overall'].mean())}\")\n",
    "    print(f\"{'Accuracy':<20} {eval_results['accuracy'].mean():.2f}/5         {'⭐' * int(eval_results['accuracy'].mean())}\")\n",
    "    print(f\"{'Completeness':<20} {eval_results['completeness'].mean():.2f}/5         {'⭐' * int(eval_results['completeness'].mean())}\")\n",
    "    print(f\"{'Clarity':<20} {eval_results['clarity'].mean():.2f}/5         {'⭐' * int(eval_results['clarity'].mean())}\")\n",
    "    print(f\"{'Helpfulness':<20} {eval_results['helpfulness'].mean():.2f}/5         {'⭐' * int(eval_results['helpfulness'].mean())}\")\n",
    "    \n",
    "    print(f\"\\n Average Confidence: {eval_results['confidence'].mean():.1%}\")\n",
    "    print(f\" Average References Used: {eval_results['num_refs'].mean():.1f}\")\n",
    "    \n",
    "    print(\"\\n DETAILED RESULTS:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Idx':<6} {'Subject':<40} {'Type':<10} {'Lang':<6} {'Score':<8} {'Conf':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "    for _, row in eval_results.iterrows():\n",
    "        subject_short = row['subject'][:38] + '..' if len(row['subject']) > 38 else row['subject']\n",
    "        print(f\"{row['ticket_idx']:<6} {subject_short:<40} {row['type']:<10} {row['language']:<6} {row['overall']}/5      {row['confidence']:.1%}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "else:\n",
    "    print(\"\\n⚠️ No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Next Steps\n",
    "\n",
    "###  What We Did This Week\n",
    "\n",
    "**Built on Previous Weeks:**\n",
    "1. **From Week 02/03**: Used pre-translated ticket data, applied LLM for answer generation\n",
    "2. **From Week 04**: Applied RAG pattern (embed → store → search → retrieve → generate)\n",
    "3. **New Additions**: Train/test split, confidence scoring, LLM evaluation, batch testing\n",
    "\n",
    "**Complete RAG Pipeline:**\n",
    "1. Load pre-translated CSV with English ticket data\n",
    "2. Split into 80% training / 20% test sets\n",
    "3. Generate embeddings for training tickets (subject + body + answer)\n",
    "4. Store in ChromaDB vector database with metadata\n",
    "5. For new tickets: embed → search similar → generate context-aware answer\n",
    "6. Evaluate answer quality using LLM-as-judge\n",
    "7. Batch test for systematic quality assessment\n",
    "\n",
    "###  Connections to Previous Work\n",
    "\n",
    "**Week 02 → Week 05:**\n",
    "- Week 02: Translated individual tickets using LLM\n",
    "- Week 05: Uses pre-translated data for efficiency\n",
    "\n",
    "**Week 03 → Week 05:**\n",
    "- Week 03: Consolidated multiple LLM calls, added JSON output\n",
    "- Week 05: Uses same OpenAI SDK pattern, structured responses\n",
    "\n",
    "**Week 04 → Week 05:**\n",
    "- Week 04: RAG with PDF documents (chunk → embed → search)\n",
    "- Week 05: RAG with structured tickets (fields → embed → search)\n",
    "\n",
    "###  Key Concepts \n",
    "\n",
    "1. **Retrieval-Augmented Generation (RAG)**: Combining search with generation\n",
    "2. **Vector Similarity**: Finding semantically similar content\n",
    "3. **Embeddings**: Converting text to numerical representations\n",
    "4. **Train/Test Split**: Proper evaluation methodology\n",
    "5. **Confidence Scoring**: Measuring system certainty\n",
    "6. **LLM Evaluation**: Using AI to judge AI outputs\n",
    "\n",
    "###  Future Enhancements (Later Lessons)\n",
    "\n",
    "**Metadata Filtering**:\n",
    "- Filter by `type`: Only retrieve similar Incidents, Requests, etc.\n",
    "- Filter by `priority`: P1 tickets get P1 solutions\n",
    "- Filter by `queue`: Department-specific knowledge\n",
    "\n",
    "**Re-ranking**:\n",
    "- Semantic re-ranking of retrieved results\n",
    "- Combine vector similarity with other relevance signals\n",
    "\n",
    "**Hybrid Search**:\n",
    "- Combine vector similarity (semantic) with keyword matching (lexical)\n",
    "- Best of both worlds for retrieval\n",
    "\n",
    "**Multilingual Support**:\n",
    "- Return answers in user's preferred language\n",
    "- Cross-lingual retrieval (query in Spanish, find English solutions)\n",
    "\n",
    "**Production Features**:\n",
    "- API wrapper for easy integration\n",
    "- Monitoring and logging\n",
    "- Caching for common queries\n",
    "- Feedback loop for continuous improvement\n",
    "\n",
    "### Key Files\n",
    "\n",
    "- **Translation**: `translate_tickets.py` (run once)\n",
    "- **RAG System**: `ticket_rag_system.ipynb` (this notebook)\n",
    "- **Original CSV**: `dataset-tickets-multi-lang3-4k.csv`\n",
    "- **Translated CSV**: `dataset-tickets-multi-lang3-4k-translated -all.csv`\n",
    "- **Vector DB**: `./chroma_ticket_db/`\n",
    "\n",
    "### Quick Start Guide\n",
    "\n",
    "**First Time Setup:**\n",
    "1. Run `translate_tickets.py` to create translated CSV (one-time)\n",
    "2. Run this notebook from top to bottom\n",
    "3. Experiment with different `CONFIG` parameters\n",
    "4. Complete homework assignments\n",
    "\n",
    "**Adjusting Parameters:**\n",
    "- `top_k`: Number of similar tickets (3-10 recommended)\n",
    "- `temperature`: LLM creativity (0.1-0.3 for factual support)\n",
    "- `embedding_model`: Trade speed vs quality\n",
    "- `test_mode`: Set `False` to use all ~4K tickets\n",
    "\n",
    "**Testing Your Changes:**\n",
    "- Modify `CONFIG` dictionary\n",
    "- Re-run affected sections\n",
    "- Compare evaluation metrics\n",
    "\n",
    "### Performance Notes\n",
    "\n",
    "**Current Configuration (TEST MODE):**\n",
    "- Loading: First 100 tickets only (fast, for learning)\n",
    "- Embedding: ~5 seconds for 100 tickets\n",
    "- Query: <1 second per ticket\n",
    "- Evaluation: ~10 seconds per ticket (includes LLM calls)\n",
    "\n",
    "**Full Dataset (test_mode=False):**\n",
    "- Loading: ~4,000 tickets\n",
    "- Embedding: ~2 minutes for all tickets\n",
    "- Query: <1 second per ticket (same)\n",
    "- Batch evaluation: Scale linearly with sample size\n",
    "\n",
    "###  Success Metrics\n",
    "\n",
    "**Quality Metrics** (from evaluation):\n",
    "- Overall score: 3-5 stars (LLM-judged)\n",
    "- Accuracy: Correctness of information\n",
    "- Completeness: Coverage of important points\n",
    "- Clarity: Ease of understanding\n",
    "- Helpfulness: Actionability of solution\n",
    "\n",
    "**Confidence Metrics**:\n",
    "- Based on similarity of retrieved tickets\n",
    "- Range: 0-1 (higher = more confident)\n",
    "- Weighted by position (top results weighted more)\n",
    "\n",
    "**When to Worry:**\n",
    "- Confidence consistently <0.5: Check retrieval quality\n",
    "- Overall scores <3: Review LLM prompts or `top_k`\n",
    "- High confidence but low scores: Check training data quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Homework & Experiments\n",
    "\n",
    "Following the tradition from Week 02 and Week 03, here are hands-on assignments to deepen your understanding of the RAG system.\n",
    "\n",
    "### Homework 1: Tune Retrieval Parameters\n",
    "\n",
    "The `top_k` parameter controls how many similar tickets we retrieve for context.\n",
    "\n",
    "**Your Task:**\n",
    "1. Test with `top_k = 1, 3, 5, 10`...\n",
    "2. Use the same sample question each time\n",
    "3. Observe how the answer quality and confidence change\n",
    "\n",
    "**Questions:**\n",
    "- At what point does adding more tickets stop helping?\n",
    "- How does confidence score correlate with `top_k`?\n",
    "\n",
    "**How to do it:**\n",
    "```python\n",
    "# Update config\n",
    "CONFIG['top_k'] = 1  # Try 1, 3, 5, 10\n",
    "\n",
    "# Re-run Section 6 (Sample Query) or Section 7 (Evaluation)\n",
    "# Compare the results\n",
    "```\n",
    "\n",
    " - what `top_k` value works best for support tickets?\n",
    "\n",
    "---\n",
    "\n",
    "### Homework 2: Temperature Experimentation\n",
    "\n",
    "Temperature controls LLM creativity (0 = deterministic, 1 = creative).\n",
    "\n",
    "**Your Task:**\n",
    "1. Try `temperature = 0.1, 0.5, 0.9`\n",
    "2. Ask the same question multiple times at each temperature\n",
    "3. Compare answer consistency and quality\n",
    "\n",
    "**Questions:**\n",
    "- Are higher temperatures better for support tickets? Why or why not?\n",
    "- Which temperature gives the most helpful answers?\n",
    "- Does low temperature make answers too robotic?\n",
    "\n",
    "**How to do it:**\n",
    "```python\n",
    "# Update config\n",
    "CONFIG['temperature'] = 0.1  # Try 0.1, 0.5, 0.9\n",
    "\n",
    "# Re-run sample queries\n",
    "# Note: You may need to ask the same question multiple times to see variation\n",
    "```\n",
    "\n",
    "**Success Criteria**: Recommend an optimal temperature range for IT support answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus Challenge: End-to-End Evaluation\n",
    "\n",
    "**Task**: Run batch evaluation on 20 test tickets and analyze the results.\n",
    "\n",
    "**Questions:**\n",
    "- What's the average accuracy score?\n",
    "- Which ticket types get the best answers?\n",
    "- Are P1 tickets answered better than P3?\n",
    "\n",
    "**How to do it:**\n",
    "```python\n",
    "# Run batch evaluation\n",
    "eval_results = run_batch_evaluation(test_df, num_samples=20)\n",
    "\n",
    "# Analyze by ticket type\n",
    "eval_results.groupby('type')['overall'].mean()\n",
    "\n",
    "# Analyze by priority\n",
    "eval_results.groupby('priority')['overall'].mean()\n",
    "```\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
