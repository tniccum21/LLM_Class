# Week07 - RAG Support Ticket System Design

## System Overview

A production-ready Streamlit application that provides intelligent support ticket resolution using LangChain, ChromaDB vector database, and two-stage retrieval with re-ranking.

### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Week07 RAG System                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ build_vectordb.py  â”‚          â”‚  ticket_support_app.py   â”‚   â”‚
â”‚  â”‚                    â”‚          â”‚                          â”‚   â”‚
â”‚  â”‚ - Load CSV         â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚ - Create embeddingsâ”‚          â”‚  â”‚  User Interface    â”‚ â”‚   â”‚
â”‚  â”‚ - Build ChromaDB   â”‚          â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚   â”‚
â”‚  â”‚ - Persist to disk  â”‚          â”‚  â”‚  â”‚Left  â”‚ â”‚Right â”‚ â”‚ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚  â”‚Panel â”‚ â”‚Panel â”‚ â”‚ â”‚   â”‚
â”‚           â”‚                      â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚   â”‚
â”‚           â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚           â–¼                      â”‚           â”‚             â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚           â–¼             â”‚   â”‚
â”‚  â”‚   ChromaDB Store   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚
â”‚  â”‚  chroma_ticket_db/ â”‚          â”‚  â”‚  RAG Pipeline      â”‚â”‚   â”‚
â”‚  â”‚                    â”‚          â”‚  â”‚  - Embed query     â”‚â”‚   â”‚
â”‚  â”‚ - Embeddings       â”‚          â”‚  â”‚  - Vector search   â”‚â”‚   â”‚
â”‚  â”‚ - Metadata         â”‚          â”‚  â”‚  - Re-rank results â”‚â”‚   â”‚
â”‚  â”‚ - Indices          â”‚          â”‚  â”‚  - Generate answer â”‚â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚
â”‚                                  â”‚           â”‚             â”‚   â”‚
â”‚                                  â”‚           â–¼             â”‚   â”‚
â”‚                                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚
â”‚                                  â”‚  â”‚   LM Studio LLM    â”‚â”‚   â”‚
â”‚                                  â”‚  â”‚  gpt-oss-20b       â”‚â”‚   â”‚
â”‚                                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Design

### 1. Vector Database Builder (`build_vectordb.py`)

**Purpose**: One-time setup script to create and populate the ChromaDB vector database with all available tickets.

**Responsibilities**:
- Load complete dataset (100% of tickets - no train/test split)
- Generate embeddings using HuggingFace model
- Create ChromaDB collection with metadata
- Persist to disk for production use

**Configuration**:
```python
CONFIG = {
    'csv_path': 'dataset-tickets-multi-lang3-4k-translated-all.csv',
    'chroma_db_path': './chroma_ticket_db',
    'collection_name': 'support_tickets',
    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',
    'embedding_dimension': 384,
    'chunk_size': 1000,  # For batch processing
}
```

**Key Functions**:
```python
def load_tickets(csv_path: str) -> pd.DataFrame
def create_embeddings(tickets: pd.DataFrame, model: str) -> List[Document]
def build_vectordb(documents: List[Document], config: dict) -> Chroma
def validate_vectordb(vectordb: Chroma) -> dict
```

**Output**:
- ChromaDB database at `chroma_ticket_db/`
- Validation report with statistics
- Performance metrics (time, memory usage)

---

### 2. Streamlit Support App (`ticket_support_app.py`)

**Purpose**: Interactive web application for real-time ticket resolution using RAG pipeline.

#### UI Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸŽ« Intelligent Support Ticket Assistant                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  ðŸ“ Ticket Input     â”‚  â”‚  ðŸ’¡ AI-Generated Response        â”‚â”‚
â”‚  â”‚                      â”‚  â”‚                                  â”‚â”‚
â”‚  â”‚  Subject:            â”‚  â”‚  [Markdown rendered answer]      â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚                                  â”‚â”‚
â”‚  â”‚  â”‚                â”‚  â”‚  â”‚  ### Solution                    â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  Based on similar tickets...     â”‚â”‚
â”‚  â”‚                      â”‚  â”‚                                  â”‚â”‚
â”‚  â”‚  Body:               â”‚  â”‚  ### Steps                       â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  1. ...                          â”‚â”‚
â”‚  â”‚  â”‚                â”‚  â”‚  â”‚  2. ...                          â”‚â”‚
â”‚  â”‚  â”‚                â”‚  â”‚  â”‚                                  â”‚â”‚
â”‚  â”‚  â”‚                â”‚  â”‚  â”‚  ### References                  â”‚â”‚
â”‚  â”‚  â”‚                â”‚  â”‚  â”‚  - Ticket #1234 (95% match)      â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  - Ticket #5678 (87% match)      â”‚â”‚
â”‚  â”‚                      â”‚  â”‚                                  â”‚â”‚
â”‚  â”‚  âš™ï¸ Options:         â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”‚
â”‚  â”‚  â˜ Strict Mode      â”‚  â”‚  â±ï¸ Response Time: 2.3s          â”‚â”‚
â”‚  â”‚  â˜ Show References  â”‚  â”‚  ðŸ“Š Confidence: 92%              â”‚â”‚
â”‚  â”‚                      â”‚  â”‚  ðŸ” Retrieved: 5 tickets         â”‚â”‚
â”‚  â”‚  [Submit] [Clear]   â”‚  â”‚                                  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Configuration Management

**Flexible Config System**:
```python
class AppConfig:
    """Centralized configuration with extensibility for future options"""

    # Vector Database
    chroma_db_path: str = './chroma_ticket_db'
    collection_name: str = 'support_tickets'

    # Embedding
    embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'

    # Retrieval
    top_k_initial: int = 20
    top_k_reranked: int = 5
    reranker_model: str = 'mixedbread-ai/mxbai-rerank-base-v1'

    # LLM
    lm_studio_url: str = 'http://192.168.7.171:1234'
    llm_model: str = 'gpt-oss-20b'
    max_tokens: int = 6000
    temperature: float = 0.1

    # RAG Modes
    rag_mode: str = 'strict'  # 'strict' or 'augmented'

    # UI Options
    show_references: bool = True
    show_metrics: bool = True

    # Future extensibility
    extra_options: dict = field(default_factory=dict)
```

#### State Management

**Session State Structure**:
```python
# Initialize session state
if 'vectordb' not in st.session_state:
    st.session_state.vectordb = load_vectordb()

if 'reranker' not in st.session_state:
    st.session_state.reranker = load_reranker()

if 'llm' not in st.session_state:
    st.session_state.llm = init_llm()

if 'config' not in st.session_state:
    st.session_state.config = AppConfig()
```

#### RAG Pipeline Implementation

**Core Pipeline Functions**:
```python
def embed_query(query: str, embedder: HuggingFaceEmbeddings) -> List[float]:
    """Generate embedding for user query"""
    pass

def vector_search(
    query_embedding: List[float],
    vectordb: Chroma,
    top_k: int
) -> List[Document]:
    """Initial vector similarity search"""
    pass

def rerank_results(
    query: str,
    documents: List[Document],
    reranker: CrossEncoder,
    top_k: int
) -> List[Tuple[Document, float]]:
    """Re-rank results using cross-encoder"""
    pass

def build_context(
    documents: List[Tuple[Document, float]],
    mode: str
) -> str:
    """Build LLM context from retrieved tickets"""
    pass

def generate_answer(
    query: str,
    context: str,
    llm: ChatOpenAI,
    mode: str
) -> dict:
    """Generate answer using LLM with context"""
    # Returns: {
    #   'answer': str,
    #   'confidence': float,
    #   'references': List[dict],
    #   'metadata': dict
    # }
    pass
```

**LangChain Integration**:
```python
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableLambda
from sentence_transformers import CrossEncoder

def create_rag_chain(config: AppConfig):
    """Create LangChain RAG pipeline"""

    # Build prompt template based on mode
    if config.rag_mode == 'strict':
        system_prompt = """You are a technical support assistant.
        Answer ONLY based on the provided historical ticket context.
        If the context doesn't contain relevant information, say so.

        Historical Tickets:
        {context}
        """
    else:  # augmented
        system_prompt = """You are a technical support assistant.
        Use the provided historical tickets as primary reference,
        but supplement with your IT knowledge when helpful.

        Historical Tickets:
        {context}
        """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "Subject: {subject}\n\nBody: {body}\n\nProvide a detailed solution.")
    ])

    # Build chain
    chain = (
        {
            "context": RunnableLambda(lambda x: x["context"]),
            "subject": RunnableLambda(lambda x: x["subject"]),
            "body": RunnableLambda(lambda x: x["body"])
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain
```

#### Performance Metrics

**Tracked Metrics**:
```python
@dataclass
class PerformanceMetrics:
    """Response performance tracking"""

    # Timing
    total_time: float
    embedding_time: float
    search_time: float
    rerank_time: float
    llm_time: float

    # Quality
    confidence_score: float
    num_retrieved: int
    num_reranked: int

    # Metadata
    timestamp: datetime
    rag_mode: str

    def to_dict(self) -> dict:
        """Convert to dictionary for display"""
        return {
            'Total Response Time': f"{self.total_time:.2f}s",
            'Vector Search': f"{self.search_time:.2f}s",
            'Re-ranking': f"{self.rerank_time:.2f}s",
            'LLM Generation': f"{self.llm_time:.2f}s",
            'Confidence': f"{self.confidence_score:.1%}",
            'Tickets Retrieved': self.num_retrieved,
            'Mode': self.rag_mode.capitalize()
        }
```

#### UI Component Functions

**Left Panel - Input**:
```python
def render_input_panel():
    """Render ticket input section"""
    st.header("ðŸ“ Ticket Input")

    subject = st.text_input(
        "Subject",
        placeholder="Brief description of the issue",
        help="Enter a concise subject line for your support request"
    )

    body = st.text_area(
        "Body",
        height=200,
        placeholder="Detailed description of your issue...",
        help="Provide as much detail as possible to get accurate assistance"
    )

    st.subheader("âš™ï¸ Options")

    rag_mode = st.radio(
        "RAG Mode",
        options=['strict', 'augmented'],
        format_func=lambda x: {
            'strict': 'ðŸ”’ Strict (Context Only)',
            'augmented': 'ðŸ”“ Augmented (+ LLM Knowledge)'
        }[x],
        help="Strict: Answer only from historical tickets | Augmented: Supplement with LLM knowledge"
    )

    show_references = st.checkbox(
        "Show Reference Tickets",
        value=True,
        help="Display the historical tickets used to generate the answer"
    )

    col1, col2 = st.columns(2)
    with col1:
        submit = st.button("Submit", type="primary", use_container_width=True)
    with col2:
        clear = st.button("Clear", use_container_width=True)

    return {
        'subject': subject,
        'body': body,
        'rag_mode': rag_mode,
        'show_references': show_references,
        'submit': submit,
        'clear': clear
    }
```

**Right Panel - Response**:
```python
def render_response_panel(response: dict, metrics: PerformanceMetrics, show_references: bool):
    """Render AI-generated response section"""
    st.header("ðŸ’¡ AI-Generated Response")

    # Main answer
    st.markdown(response['answer'])

    # Performance metrics
    st.divider()

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("â±ï¸ Response Time", f"{metrics.total_time:.2f}s")
    with col2:
        st.metric("ðŸ“Š Confidence", f"{metrics.confidence_score:.1%}")
    with col3:
        st.metric("ðŸ” Retrieved", metrics.num_retrieved)

    # Reference tickets (expandable)
    if show_references and response.get('references'):
        with st.expander("ðŸ“š Reference Tickets", expanded=False):
            for i, ref in enumerate(response['references'], 1):
                st.markdown(f"**Ticket #{i}** (Match: {ref['score']:.1%})")
                st.text(f"Subject: {ref['subject']}")
                st.text_area(
                    f"Content {i}",
                    value=ref['body'],
                    height=100,
                    disabled=True,
                    label_visibility="collapsed"
                )
                st.divider()
```

## File Structure

```
Week07/
â”œâ”€â”€ DESIGN.md                           # This document
â”œâ”€â”€ build_vectordb.py                   # Vector DB builder script
â”œâ”€â”€ ticket_support_app.py               # Main Streamlit application
â”œâ”€â”€ rag_pipeline.py                     # RAG pipeline components
â”œâ”€â”€ config.py                           # Configuration management
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ secrets.env                         # API keys (gitignored)
â”œâ”€â”€ chroma_ticket_db/                   # Vector database (gitignored)
â”œâ”€â”€ dataset-tickets-*.csv               # Training data
â””â”€â”€ README.md                           # User documentation
```

## Dependencies

```txt
# Core
streamlit>=1.30.0
pandas>=2.0.0
python-dotenv>=1.0.0

# LangChain Ecosystem
langchain>=0.1.0
langchain-openai>=0.0.5
langchain-community>=0.0.20
langchain-chroma>=0.1.0

# Vector DB & Embeddings
chromadb>=0.4.22
sentence-transformers>=2.3.0

# LLM Integration
openai>=1.10.0  # OpenAI-compatible API
requests>=2.31.0
```

## Implementation Phases

### Phase 1: Vector Database Builder
- âœ… Load complete CSV dataset
- âœ… Create embeddings for all tickets
- âœ… Build ChromaDB with metadata
- âœ… Validation and statistics

### Phase 2: Core RAG Pipeline
- âœ… Query embedding
- âœ… Vector search implementation
- âœ… Re-ranking with cross-encoder
- âœ… Context building
- âœ… LLM answer generation

### Phase 3: Streamlit UI
- âœ… Two-column layout
- âœ… Input forms (subject/body)
- âœ… Response rendering (markdown)
- âœ… Performance metrics display
- âœ… Reference ticket display

### Phase 4: Configuration & Extensibility
- âœ… Flexible config system
- âœ… RAG mode toggle (strict/augmented)
- âœ… Session state management
- âœ… Error handling

### Phase 5: Polish & Documentation
- âœ… Loading indicators
- âœ… Error messages
- âœ… User help text
- âœ… README documentation

## Configuration Options (Extensible)

**Current Options**:
- `rag_mode`: strict | augmented
- `show_references`: boolean
- `show_metrics`: boolean

**Future Options** (easily added via `extra_options` dict):
- `enable_evaluation`: boolean - Show LLM-as-judge quality scores
- `search_filters`: dict - Metadata filtering (category, priority, language)
- `temperature`: float - LLM temperature control
- `top_k_slider`: boolean - Allow user to adjust retrieval count
- `export_format`: str - Export answer (PDF, markdown, HTML)
- `feedback_enabled`: boolean - Thumbs up/down rating system
- `model_selection`: str - Choose different LLM models
- `embedding_model`: str - Switch embedding models

## Performance Targets

- **Initial Load**: < 5 seconds (one-time ChromaDB + model loading)
- **Query Response**: 2-5 seconds total
  - Embedding: < 0.1s
  - Vector Search: < 0.5s
  - Re-ranking: < 0.5s
  - LLM Generation: 1-4s (depends on LM Studio)
- **Memory Usage**: < 2GB (embeddings + reranker + ChromaDB)

## Error Handling

**Graceful Degradation**:
```python
try:
    # Attempt re-ranking
    reranked = rerank_results(query, initial_results, reranker)
except Exception as e:
    st.warning("Re-ranking unavailable, using vector search results only")
    reranked = initial_results[:config.top_k_reranked]

try:
    # Attempt LLM generation
    answer = generate_answer(query, context, llm)
except Exception as e:
    st.error(f"LLM generation failed: {str(e)}")
    answer = {
        'answer': "Unable to generate answer. Please try again.",
        'error': str(e)
    }
```

## Security Considerations

- âœ… API keys in `.env` file (gitignored)
- âœ… Input validation and sanitization
- âœ… No raw SQL or code execution
- âœ… LM Studio on local network (no external API calls)
- âš ï¸ Rate limiting (future: implement if needed)
- âš ï¸ User authentication (future: if multi-user deployment)

## Testing Strategy

**Unit Tests** (`tests/test_rag_pipeline.py`):
- Embedding generation
- Vector search accuracy
- Re-ranking correctness
- Context building

**Integration Tests** (`tests/test_app.py`):
- End-to-end pipeline
- Config validation
- Error handling

**Manual Testing Checklist**:
- [ ] Submit valid ticket â†’ receives answer
- [ ] Submit empty ticket â†’ validation error
- [ ] Toggle strict/augmented â†’ different responses
- [ ] Show/hide references â†’ UI updates
- [ ] ChromaDB unavailable â†’ error message
- [ ] LM Studio down â†’ graceful degradation

## Deployment Notes

**Local Development**:
```bash
# 1. Build vector database
python build_vectordb.py

# 2. Run Streamlit app
streamlit run ticket_support_app.py
```

**Production Considerations**:
- Use persistent ChromaDB storage
- Configure LM Studio for high availability
- Implement logging and monitoring
- Add caching for frequent queries
- Consider horizontal scaling (multiple workers)

---

## Design Decisions & Rationale

### Why separate `build_vectordb.py`?
- One-time setup vs. runtime query performance
- Allows updating vector DB independently
- Clearer separation of concerns
- Easier to test and validate DB creation

### Why LangChain?
- Standardized RAG pipeline patterns
- Seamless LLM integration
- Built-in ChromaDB support
- Extensible for future enhancements

### Why two-column layout?
- Natural left-to-right workflow (input â†’ output)
- Side-by-side comparison of ticket and answer
- Efficient use of screen space
- Industry-standard pattern (ChatGPT, Claude)

### Why dataclass for config?
- Type safety and validation
- IDE autocomplete support
- Easy serialization/deserialization
- Clear documentation of options

### Why session state for models?
- Avoid reloading models on every interaction
- Significant performance improvement
- Maintains stateful connection to ChromaDB
- Streamlit best practice for expensive resources

---

## Next Steps

1. **Implement `config.py`** - Configuration management system
2. **Implement `rag_pipeline.py`** - Extract pipeline from notebook
3. **Implement `build_vectordb.py`** - Vector database builder
4. **Implement `ticket_support_app.py`** - Streamlit application
5. **Create `requirements.txt`** - Dependency specification
6. **Write `README.md`** - User documentation
7. **Test end-to-end** - Validate complete workflow
